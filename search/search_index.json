{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Project \"Piper\" - extended library documentation \u00b6 On the following pages, you can find documentation for all steps and scripts contained in the library of project \"Piper\". We have also compiled a list of plugins that are required for the library to work. For information on the project and installation of the library, please have a look at our README.md .","title":"Home"},{"location":"#project-piper-extended-library-documentation","text":"On the following pages, you can find documentation for all steps and scripts contained in the library of project \"Piper\". We have also compiled a list of plugins that are required for the library to work. For information on the project and installation of the library, please have a look at our README.md .","title":"Project \"Piper\" - extended library documentation"},{"location":"configuration/","text":"Configuration \u00b6 Configuration is done via a yml-file, located at .pipeline/config.yml in the master branch of your source code repository. Your configuration inherits from the default configuration located at https://github.com/SAP/jenkins-library/blob/master/resources/default_pipeline_environment.yml . Adding custom parameters Please note that adding custom parameters to the configuration is at your own risk. We may introduce new parameters at any time which may clash with your custom parameters. Configuration of the Piper steps as well the Piper templates can be done in a hierarchical manner. Directly passed step parameters will always take precedence over other configuration values and defaults Stage configuration parameters define a Jenkins pipeline stage dependent set of parameters (e.g. deployment options for the Acceptance stage) Step configuration defines how steps behave in general (e.g. step cloudFoundryDeploy ) General configuration parameters define parameters which are available across step boundaries Default configuration comes with the Piper library and is always available Collecting telemetry data \u00b6 In order to improve this Jenkins library we are collecting telemetry data. Data is send using com.sap.piper.pushToSWA Following data (non-personal) is collected for example: Hashed job url, e.g. 4944f745e03f5f79daf0001eec9276ce351d3035 hash calculation is done in your Jenkins server and no original values are transmitted Name of library step which has been executed, like e.g. artifactSetVersion Certain parameters of the executed steps, e.g. buildTool=maven We store the telemetry data for not longer than 6 months on premises of SAP SE. Disable collection of telemetry data If you do not want to send telemetry data you can easily deactivate this. This is done with either of the following two ways: General deactivation in your .pipeline/config.yml file by setting the configuration parameter general -> collectTelemetryData: false (default setting can be found in the library defaults ). Please note: this will only take effect in all steps if you run setupCommonPipelineEnvironment at the beginning of your pipeline Individual deactivation per step by passing the parameter collectTelemetryData: false , like e.g. setVersion script:this, collectTelemetryData: false Example configuration \u00b6 general : gitSshKeyCredentialsId : GitHub_Test_SSH steps : cloudFoundryDeploy : deployTool : 'cf_native' cloudFoundry : org : 'testOrg' space : 'testSpace' credentialsId : 'MY_CF_CREDENTIALSID_IN_JENKINS' newmanExecute : newmanCollection : 'myNewmanCollection.file' newmanEnvironment : 'myNewmanEnvironment' newmanGlobals : 'myNewmanGlobals' Access to configuration from custom scripts \u00b6 Configuration is loaded into commonPipelineEnvironment during step setupCommonPipelineEnvironment . You can access the configuration values via commonPipelineEnvironment.configuration which will return you the complete configuration map. Thus following access is for example possible (accessing gitSshKeyCredentialsId from general section): commonPipelineEnvironment . configuration . general . gitSshKeyCredentialsId Access to configuration in custom library steps \u00b6 Within library steps the ConfigurationHelper object is used. You can see its usage in all the Piper steps, for example newmanExecute .","title":"Configuration"},{"location":"configuration/#configuration","text":"Configuration is done via a yml-file, located at .pipeline/config.yml in the master branch of your source code repository. Your configuration inherits from the default configuration located at https://github.com/SAP/jenkins-library/blob/master/resources/default_pipeline_environment.yml . Adding custom parameters Please note that adding custom parameters to the configuration is at your own risk. We may introduce new parameters at any time which may clash with your custom parameters. Configuration of the Piper steps as well the Piper templates can be done in a hierarchical manner. Directly passed step parameters will always take precedence over other configuration values and defaults Stage configuration parameters define a Jenkins pipeline stage dependent set of parameters (e.g. deployment options for the Acceptance stage) Step configuration defines how steps behave in general (e.g. step cloudFoundryDeploy ) General configuration parameters define parameters which are available across step boundaries Default configuration comes with the Piper library and is always available","title":"Configuration"},{"location":"configuration/#collecting-telemetry-data","text":"In order to improve this Jenkins library we are collecting telemetry data. Data is send using com.sap.piper.pushToSWA Following data (non-personal) is collected for example: Hashed job url, e.g. 4944f745e03f5f79daf0001eec9276ce351d3035 hash calculation is done in your Jenkins server and no original values are transmitted Name of library step which has been executed, like e.g. artifactSetVersion Certain parameters of the executed steps, e.g. buildTool=maven We store the telemetry data for not longer than 6 months on premises of SAP SE. Disable collection of telemetry data If you do not want to send telemetry data you can easily deactivate this. This is done with either of the following two ways: General deactivation in your .pipeline/config.yml file by setting the configuration parameter general -> collectTelemetryData: false (default setting can be found in the library defaults ). Please note: this will only take effect in all steps if you run setupCommonPipelineEnvironment at the beginning of your pipeline Individual deactivation per step by passing the parameter collectTelemetryData: false , like e.g. setVersion script:this, collectTelemetryData: false","title":"Collecting telemetry data"},{"location":"configuration/#example-configuration","text":"general : gitSshKeyCredentialsId : GitHub_Test_SSH steps : cloudFoundryDeploy : deployTool : 'cf_native' cloudFoundry : org : 'testOrg' space : 'testSpace' credentialsId : 'MY_CF_CREDENTIALSID_IN_JENKINS' newmanExecute : newmanCollection : 'myNewmanCollection.file' newmanEnvironment : 'myNewmanEnvironment' newmanGlobals : 'myNewmanGlobals'","title":"Example configuration"},{"location":"configuration/#access-to-configuration-from-custom-scripts","text":"Configuration is loaded into commonPipelineEnvironment during step setupCommonPipelineEnvironment . You can access the configuration values via commonPipelineEnvironment.configuration which will return you the complete configuration map. Thus following access is for example possible (accessing gitSshKeyCredentialsId from general section): commonPipelineEnvironment . configuration . general . gitSshKeyCredentialsId","title":"Access to configuration from custom scripts"},{"location":"configuration/#access-to-configuration-in-custom-library-steps","text":"Within library steps the ConfigurationHelper object is used. You can see its usage in all the Piper steps, for example newmanExecute .","title":"Access to configuration in custom library steps"},{"location":"jenkins/requiredPlugins/","text":"Required Plugins \u00b6 The following Jenkins plugins are needed in order to use the Piper Library. The list below contains the plugin Id and version of the plugin. Plugins ace-editor 1.1 authentication-tokens 1.3 bouncycastle-api 2.16.2 branch-api 2.0.14 cloudbees-folder 6.2.1 credentials 2.1.16 credentials-binding 1.13 display-url-api 2.1.0 docker-commons 1.9 docker-workflow 1.10 durable-task 1.15 git 3.6.2 git-client 2.5.0 git-server 1.7 handlebars 1.1.1 icon-shim 2.0.3 jquery-detached 1.2.1 junit 1.21 mailer 1.20 matrix-project 1.12 momentjs 1.1.1 pipeline-build-step 2.5.1 pipeline-graph-analysis 1.3 pipeline-input-step 2.8 pipeline-milestone-step 1.3.1 pipeline-model-api 1.2.2 pipeline-model-definition 1.1.1 pipeline-model-extensions 1.1.1 pipeline-rest-api 2.6 pipeline-stage-step 2.2 pipeline-stage-tags-metadata 1.2.2 pipeline-stage-view 2.6 pipeline-utility-steps 1.3.0 plain-credentials 1.4 scm-api 2.2.3 script-security 1.34 ssh-credentials 1.13 structs 1.10 workflow-aggregator 2.5 workflow-api 2.23.1 workflow-basic-steps 2.6 workflow-cps 2.41 workflow-cps-global-lib 2.7 workflow-durable-task-step 2.17 workflow-job 2.12.2 workflow-multibranch 2.14 workflow-scm-step 2.6 workflow-step-api 2.13 workflow-support 2.16","title":"Required Plugins"},{"location":"jenkins/requiredPlugins/#required-plugins","text":"The following Jenkins plugins are needed in order to use the Piper Library. The list below contains the plugin Id and version of the plugin. Plugins ace-editor 1.1 authentication-tokens 1.3 bouncycastle-api 2.16.2 branch-api 2.0.14 cloudbees-folder 6.2.1 credentials 2.1.16 credentials-binding 1.13 display-url-api 2.1.0 docker-commons 1.9 docker-workflow 1.10 durable-task 1.15 git 3.6.2 git-client 2.5.0 git-server 1.7 handlebars 1.1.1 icon-shim 2.0.3 jquery-detached 1.2.1 junit 1.21 mailer 1.20 matrix-project 1.12 momentjs 1.1.1 pipeline-build-step 2.5.1 pipeline-graph-analysis 1.3 pipeline-input-step 2.8 pipeline-milestone-step 1.3.1 pipeline-model-api 1.2.2 pipeline-model-definition 1.1.1 pipeline-model-extensions 1.1.1 pipeline-rest-api 2.6 pipeline-stage-step 2.2 pipeline-stage-tags-metadata 1.2.2 pipeline-stage-view 2.6 pipeline-utility-steps 1.3.0 plain-credentials 1.4 scm-api 2.2.3 script-security 1.34 ssh-credentials 1.13 structs 1.10 workflow-aggregator 2.5 workflow-api 2.23.1 workflow-basic-steps 2.6 workflow-cps 2.41 workflow-cps-global-lib 2.7 workflow-durable-task-step 2.17 workflow-job 2.12.2 workflow-multibranch 2.14 workflow-scm-step 2.6 workflow-step-api 2.13 workflow-support 2.16","title":"Required Plugins"},{"location":"steps/artifactSetVersion/","text":"artifactSetVersion \u00b6 Description \u00b6 The continuous delivery process requires that each build is done with a unique version number. The version generated using this step will contain: Version (major.minor.patch) from descriptor file in master repository is preserved. Developers should be able to autonomously decide on increasing either part of this version number. Timestamp CommitId (by default the long version of the hash) Optionally, but enabled by default, the new version is pushed as a new tag into the source code repository (e.g. GitHub). If this option is chosen, git credentials and the repository URL needs to be provided. Since you might not want to configure the git credentials in Jenkins, committing and pushing can be disabled using the commitVersion parameter as described below. If you require strict reproducibility of your builds, this should be used. Prerequsites \u00b6 none Parameters \u00b6 parameter mandatory default possible values script yes artifactType no 'appContainer' buildTool no maven docker, dlang, golang, maven, mta, npm, pip, sbt commitVersion no true true , false dockerVersionSource no '' FROM, (ENV name),appVersion filePath no buildTool= docker : Dockerfile buildTool= dlang : dub.json buildTool= golang : VERSION buildTool= maven : pom.xml buildTool= mta : mta.yaml buildTool= npm : package.json buildTool= pip : version.txt buildTool= sbt : sbtDescriptor.json gitCommitId no GitUtils.getGitCommitId() gitSshCredentialsId If commitVersion is true as defined in custom configuration gitUserEMail no gitUserName no gitSshUrl If commitVersion is true tagPrefix no 'build_' timestamp no current time in format according to timestampTemplate timestampTemplate no %Y%m%d%H%M%S versioningTemplate no buildTool= docker : ${version}-${timestamp}${commitId?\"_\"+commitId:\"\"} />buildTool= dlang : ${version}-${timestamp}${commitId?\"+\"+commitId:\"\"} buildTool= golang : ${version}-${timestamp}${commitId?\"+\"+commitId:\"\"} buildTool= maven : ${version}-${timestamp}${commitId?\"_\"+commitId:\"\"} buildTool= mta : ${version}-${timestamp}${commitId?\"+\"+commitId:\"\"} buildTool= npm : ${version}-${timestamp}${commitId?\"+\"+commitId:\"\"} buildTool= pip : ${version}.${timestamp}${commitId?\".\"+commitId:\"\"} buildTool= sbt : ${version}-${timestamp}${commitId?\"+\"+commitId:\"\"} script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for retrieving e.g. configuration parameters. artifactType defines the type of the artifact. buildTool defines the tool which is used for building the artifact. commitVersion controls if the changed version is committed and pushed to the git repository. If this is enabled (which is the default), you need to provide gitCredentialsId and gitSshUrl . dockerVersionSource specifies the source to be used for the main version which is used for generating the automatic version. This can either be the version of the base image - as retrieved from the FROM statement within the Dockerfile, e.g. FROM jenkins:2.46.2 Alternatively the name of an environment variable defined in the Docker image can be used which contains the version number, e.g. ENV MY_VERSION 1.2.3 The third option appVersion applies only to the artifactType appContainer . Here the version of the app which is packaged into the container will be used as version for the container itself. Using filePath you could define a custom path to the descriptor file. gitCommitId defines the version prefix of the automatically generated version. By default it will take the long commitId hash. You could pass any other string (e.g. the short commitId hash) to be used. In case you don't want to have the gitCommitId added to the automatic versioning string you could set the value to an empty string: '' . gitSshCredentialsId defines the ssh git credentials to be used for writing the tag. The parameters gitUserName and gitUserEMail allow to overwrite the global git settings available on your Jenkins server gitSshUrl defines the git ssh url to the source code repository. tagPrefix defines the prefix wich is used for the git tag which is written during the versioning run. timestamp defines the timestamp to be used in the automatic version string. You could overwrite the default behavior by explicitly setting this string. Step configuration \u00b6 The following parameters can also be specified as step parameters using the global configuration file: artifactType buildTool commitVersion dockerVersionSource filePath gitCredentialsId gitUserEMail gitUserName gitSshUrl tagPrefix timestamp timestampTemplate versioningTemplate Example \u00b6 artifactSetVersion script: this , buildTool: 'maven'","title":"artifactSetVersion"},{"location":"steps/artifactSetVersion/#artifactsetversion","text":"","title":"artifactSetVersion"},{"location":"steps/artifactSetVersion/#description","text":"The continuous delivery process requires that each build is done with a unique version number. The version generated using this step will contain: Version (major.minor.patch) from descriptor file in master repository is preserved. Developers should be able to autonomously decide on increasing either part of this version number. Timestamp CommitId (by default the long version of the hash) Optionally, but enabled by default, the new version is pushed as a new tag into the source code repository (e.g. GitHub). If this option is chosen, git credentials and the repository URL needs to be provided. Since you might not want to configure the git credentials in Jenkins, committing and pushing can be disabled using the commitVersion parameter as described below. If you require strict reproducibility of your builds, this should be used.","title":"Description"},{"location":"steps/artifactSetVersion/#prerequsites","text":"none","title":"Prerequsites"},{"location":"steps/artifactSetVersion/#parameters","text":"parameter mandatory default possible values script yes artifactType no 'appContainer' buildTool no maven docker, dlang, golang, maven, mta, npm, pip, sbt commitVersion no true true , false dockerVersionSource no '' FROM, (ENV name),appVersion filePath no buildTool= docker : Dockerfile buildTool= dlang : dub.json buildTool= golang : VERSION buildTool= maven : pom.xml buildTool= mta : mta.yaml buildTool= npm : package.json buildTool= pip : version.txt buildTool= sbt : sbtDescriptor.json gitCommitId no GitUtils.getGitCommitId() gitSshCredentialsId If commitVersion is true as defined in custom configuration gitUserEMail no gitUserName no gitSshUrl If commitVersion is true tagPrefix no 'build_' timestamp no current time in format according to timestampTemplate timestampTemplate no %Y%m%d%H%M%S versioningTemplate no buildTool= docker : ${version}-${timestamp}${commitId?\"_\"+commitId:\"\"} />buildTool= dlang : ${version}-${timestamp}${commitId?\"+\"+commitId:\"\"} buildTool= golang : ${version}-${timestamp}${commitId?\"+\"+commitId:\"\"} buildTool= maven : ${version}-${timestamp}${commitId?\"_\"+commitId:\"\"} buildTool= mta : ${version}-${timestamp}${commitId?\"+\"+commitId:\"\"} buildTool= npm : ${version}-${timestamp}${commitId?\"+\"+commitId:\"\"} buildTool= pip : ${version}.${timestamp}${commitId?\".\"+commitId:\"\"} buildTool= sbt : ${version}-${timestamp}${commitId?\"+\"+commitId:\"\"} script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for retrieving e.g. configuration parameters. artifactType defines the type of the artifact. buildTool defines the tool which is used for building the artifact. commitVersion controls if the changed version is committed and pushed to the git repository. If this is enabled (which is the default), you need to provide gitCredentialsId and gitSshUrl . dockerVersionSource specifies the source to be used for the main version which is used for generating the automatic version. This can either be the version of the base image - as retrieved from the FROM statement within the Dockerfile, e.g. FROM jenkins:2.46.2 Alternatively the name of an environment variable defined in the Docker image can be used which contains the version number, e.g. ENV MY_VERSION 1.2.3 The third option appVersion applies only to the artifactType appContainer . Here the version of the app which is packaged into the container will be used as version for the container itself. Using filePath you could define a custom path to the descriptor file. gitCommitId defines the version prefix of the automatically generated version. By default it will take the long commitId hash. You could pass any other string (e.g. the short commitId hash) to be used. In case you don't want to have the gitCommitId added to the automatic versioning string you could set the value to an empty string: '' . gitSshCredentialsId defines the ssh git credentials to be used for writing the tag. The parameters gitUserName and gitUserEMail allow to overwrite the global git settings available on your Jenkins server gitSshUrl defines the git ssh url to the source code repository. tagPrefix defines the prefix wich is used for the git tag which is written during the versioning run. timestamp defines the timestamp to be used in the automatic version string. You could overwrite the default behavior by explicitly setting this string.","title":"Parameters"},{"location":"steps/artifactSetVersion/#step-configuration","text":"The following parameters can also be specified as step parameters using the global configuration file: artifactType buildTool commitVersion dockerVersionSource filePath gitCredentialsId gitUserEMail gitUserName gitSshUrl tagPrefix timestamp timestampTemplate versioningTemplate","title":"Step configuration"},{"location":"steps/artifactSetVersion/#example","text":"artifactSetVersion script: this , buildTool: 'maven'","title":"Example"},{"location":"steps/batsExecuteTests/","text":"batsExecuteTests \u00b6 Description \u00b6 This step executes tests using the Bash Automated Testing System - bats-core Prerequsites \u00b6 You need to have a Bats test file. By default you would put this into directory src/test within your source code repository. Parameters \u00b6 parameter mandatory default possible values script yes dockerImage no node:8-stretch dockerWorkspace no /home/node envVars no [:] failOnError no false gitBranch no gitSshKeyCredentialsId no outputFormat no junit tap repository no https://github.com/bats-core/bats-core.git stashContent no ['tests'] testPackage no piper-bats testPath no src/test testRepository no Details: outputFormat defines the format of the test result output. junit would be the standard for automated build environments but you could use also the option tap . For the transformation of the test result to xUnit format the node module tap-xunit is used. dockerImage and dockerWorkspace define the Docker image used for the transformation and testPackage defines the name of the test package used in the xUnit result file. testPath defines either the directory which contains the test files ( *.bats ) or a single file. You can find further details in the Bats-core documentation With failOnError you can define the behavior, in case tests fail. For example, in case of outputFormat: 'junit' you should set it to false . Otherwise test results cannot be recorded using the testsPublishhResults step afterwards. You can pass environment variables to the test execution by defining parameter envVars . With envVars it is possible to pass either fixed values but also templates using commonPipelineEnvironment . Example: batsExecuteTests script : this, envVars = [ FIX_VALUE : 'my fixed value' , CONTAINER_NAME : '${commonPipelineEnvironment.configuration.steps.executeBatsTests.dockerContainerName}' , IMAGE_NAME : '${return commonPipelineEnvironment.getDockerImageNameAndTag()}' ] This means within the test one could refer to environment variables by calling e.g. run docker run --rm -i --name $CONTAINER_NAME --entrypoint /bin/bash $IMAGE_NAME echo \"Test\" Using parameters testRepository the tests can be loaded from another reposirory. In case the tests are not located in the master branch the branch can be specified with gitBranch . For protected repositories you can also define the access credentials via gitSshKeyCredentialsId . Note: In case of using a protected repository, testRepository should include the ssh link to the repository. The parameter repository defines the version of bats-core to be used. By default we use the version from the master branch. Step configuration \u00b6 The following parameters can also be specified as step/stage/general parameters using the global configuration : dockerImage dockerWorkspace envVars failOnError gitBranch gitSshKeyCredentialsId outputFormat repository stashContent testPackage testPath testRepository Example \u00b6 batsExecuteTests script: this testsPublishResults junit: [ pattern: '**/Test-*.xml' , archive: true ]","title":"batsExecuteTests"},{"location":"steps/batsExecuteTests/#batsexecutetests","text":"","title":"batsExecuteTests"},{"location":"steps/batsExecuteTests/#description","text":"This step executes tests using the Bash Automated Testing System - bats-core","title":"Description"},{"location":"steps/batsExecuteTests/#prerequsites","text":"You need to have a Bats test file. By default you would put this into directory src/test within your source code repository.","title":"Prerequsites"},{"location":"steps/batsExecuteTests/#parameters","text":"parameter mandatory default possible values script yes dockerImage no node:8-stretch dockerWorkspace no /home/node envVars no [:] failOnError no false gitBranch no gitSshKeyCredentialsId no outputFormat no junit tap repository no https://github.com/bats-core/bats-core.git stashContent no ['tests'] testPackage no piper-bats testPath no src/test testRepository no Details: outputFormat defines the format of the test result output. junit would be the standard for automated build environments but you could use also the option tap . For the transformation of the test result to xUnit format the node module tap-xunit is used. dockerImage and dockerWorkspace define the Docker image used for the transformation and testPackage defines the name of the test package used in the xUnit result file. testPath defines either the directory which contains the test files ( *.bats ) or a single file. You can find further details in the Bats-core documentation With failOnError you can define the behavior, in case tests fail. For example, in case of outputFormat: 'junit' you should set it to false . Otherwise test results cannot be recorded using the testsPublishhResults step afterwards. You can pass environment variables to the test execution by defining parameter envVars . With envVars it is possible to pass either fixed values but also templates using commonPipelineEnvironment . Example: batsExecuteTests script : this, envVars = [ FIX_VALUE : 'my fixed value' , CONTAINER_NAME : '${commonPipelineEnvironment.configuration.steps.executeBatsTests.dockerContainerName}' , IMAGE_NAME : '${return commonPipelineEnvironment.getDockerImageNameAndTag()}' ] This means within the test one could refer to environment variables by calling e.g. run docker run --rm -i --name $CONTAINER_NAME --entrypoint /bin/bash $IMAGE_NAME echo \"Test\" Using parameters testRepository the tests can be loaded from another reposirory. In case the tests are not located in the master branch the branch can be specified with gitBranch . For protected repositories you can also define the access credentials via gitSshKeyCredentialsId . Note: In case of using a protected repository, testRepository should include the ssh link to the repository. The parameter repository defines the version of bats-core to be used. By default we use the version from the master branch.","title":"Parameters"},{"location":"steps/batsExecuteTests/#step-configuration","text":"The following parameters can also be specified as step/stage/general parameters using the global configuration : dockerImage dockerWorkspace envVars failOnError gitBranch gitSshKeyCredentialsId outputFormat repository stashContent testPackage testPath testRepository","title":"Step configuration"},{"location":"steps/batsExecuteTests/#example","text":"batsExecuteTests script: this testsPublishResults junit: [ pattern: '**/Test-*.xml' , archive: true ]","title":"Example"},{"location":"steps/checkChangeInDevelopment/","text":"checkChangeInDevelopment \u00b6 Description \u00b6 Checks if a Change Document in SAP Solution Manager is in status 'in development'. The change document id is retrieved from the git commit history. The change document id can also be provided via parameter changeDocumentId . Any value provided as parameter has a higher precedence than a value from the commit history. By default the git commit messages between origin/master and HEAD are scanned for a line like ChangeDocument : <changeDocumentId> . The commit range and the pattern can be configured. For details see 'parameters' table. Prerequisites \u00b6 Change Management Client 2.0.0 or compatible version - available for download on Maven Central. Parameters \u00b6 parameter mandatory default possible values script yes changeDocumentId yes changeManagement/changeDocumentLabel no ChangeDocument\\s?: regex pattern changeManagement/credentialsId yes changeManagement/endpoint yes changeManagement/git/from no origin/master changeManagement/git/to no HEAD changeManagement/git/format no %b see git log --help failIfStatusIsNotInDevelopment no true true , false script - The common script environment of the Jenkinsfile running. Typically the reference to the script calling the pipeline step is provided with the this parameter, as in script: this . This allows the function to access the commonPipelineEnvironment for retrieving, for example, configuration parameters. changeDocumentId - The id of the change document to transport. If not provided, it is retrieved from the git commit history. changeManagement/changeDocumentLabel - A pattern used for identifying lines holding the change document id. changeManagement/credentialsId - The id of the credentials to connect to the Solution Manager. The credentials needs to be maintained on Jenkins. changeManagement/endpoint - The address of the Solution Manager. changeManagement/git/from - The starting point for retrieving the change document id changeManagement/git/to - The end point for retrieving the change document id changeManagement/git/format - Specifies what part of the commit is scanned. By default the body of the commit message is scanned. failIfStatusIsNotInDevelopment - when set to false the step will not fail in case the step is not in status 'in development'. Step configuration \u00b6 The step is configured using a customer configuration file provided as resource in an custom shared library. @Library ( 'piper-library-os@master' ) _ // the shared lib containing the additional configuration // needs to be configured in Jenkins @Library ( 'foo@master' ) __ // inside the shared lib denoted by 'foo' the additional configuration file // needs to be located under 'resources' ('resoures/myConfig.yml') prepareDefaultValues script: this , customDefaults: 'myConfig.yml' Example content of 'resources/myConfig.yml' in branch 'master' of the repository denoted by 'foo' : general : changeManagement : changeDocumentLabel : 'ChangeDocument\\s?:' cmClientOpts : '-Djavax.net.ssl.trustStore=<path to truststore>' credentialsId : 'CM' endpoint : 'https://example.org/cm' git : from : 'HEAD~1' to : 'HEAD' format : '%b' The properties configured in section 'general/changeManagement' are shared between all change managment related steps. The properties can also be configured on a per-step basis: [ ... ] steps : checkChangeInDevelopment : changeManagement : endpoint : 'https://example.org/cm' [ ... ] failIfStatusIsNotInDevelopment : true The parameters can also be provided when the step is invoked. For examples see below. Return value \u00b6 true in case the change document is in status 'in development'. Otherwise an hudson.AbortException is thrown. In case failIfStatusIsNotInDevelopment is set to false , false is returned in case the change document is not in status 'in development' Exceptions \u00b6 AbortException : If the change id is not provided via parameter and if the change document id cannot be retrieved from the commit history. If the change is not in status in development . In this case no exception will be thrown when failIfStatusIsNotInDevelopment is set to false . IllegalArgumentException : If a mandatory property is not provided. Examples \u00b6 // simple case. All mandatory parameters provided via // configuration, changeDocumentId provided via commit // history checkChangeInDevelopment script: this // explict endpoint provided, we search for changeDocumentId // starting at the previous commit (HEAD~1) rather than on // 'origin/master' (the default). checkChangeInDevelopment script: this changeManagement: [ endpoint: 'https:example.org/cm' git: [ from: 'HEAD~1' ] ]","title":"checkChangeInDevelopment"},{"location":"steps/checkChangeInDevelopment/#checkchangeindevelopment","text":"","title":"checkChangeInDevelopment"},{"location":"steps/checkChangeInDevelopment/#description","text":"Checks if a Change Document in SAP Solution Manager is in status 'in development'. The change document id is retrieved from the git commit history. The change document id can also be provided via parameter changeDocumentId . Any value provided as parameter has a higher precedence than a value from the commit history. By default the git commit messages between origin/master and HEAD are scanned for a line like ChangeDocument : <changeDocumentId> . The commit range and the pattern can be configured. For details see 'parameters' table.","title":"Description"},{"location":"steps/checkChangeInDevelopment/#prerequisites","text":"Change Management Client 2.0.0 or compatible version - available for download on Maven Central.","title":"Prerequisites"},{"location":"steps/checkChangeInDevelopment/#parameters","text":"parameter mandatory default possible values script yes changeDocumentId yes changeManagement/changeDocumentLabel no ChangeDocument\\s?: regex pattern changeManagement/credentialsId yes changeManagement/endpoint yes changeManagement/git/from no origin/master changeManagement/git/to no HEAD changeManagement/git/format no %b see git log --help failIfStatusIsNotInDevelopment no true true , false script - The common script environment of the Jenkinsfile running. Typically the reference to the script calling the pipeline step is provided with the this parameter, as in script: this . This allows the function to access the commonPipelineEnvironment for retrieving, for example, configuration parameters. changeDocumentId - The id of the change document to transport. If not provided, it is retrieved from the git commit history. changeManagement/changeDocumentLabel - A pattern used for identifying lines holding the change document id. changeManagement/credentialsId - The id of the credentials to connect to the Solution Manager. The credentials needs to be maintained on Jenkins. changeManagement/endpoint - The address of the Solution Manager. changeManagement/git/from - The starting point for retrieving the change document id changeManagement/git/to - The end point for retrieving the change document id changeManagement/git/format - Specifies what part of the commit is scanned. By default the body of the commit message is scanned. failIfStatusIsNotInDevelopment - when set to false the step will not fail in case the step is not in status 'in development'.","title":"Parameters"},{"location":"steps/checkChangeInDevelopment/#step-configuration","text":"The step is configured using a customer configuration file provided as resource in an custom shared library. @Library ( 'piper-library-os@master' ) _ // the shared lib containing the additional configuration // needs to be configured in Jenkins @Library ( 'foo@master' ) __ // inside the shared lib denoted by 'foo' the additional configuration file // needs to be located under 'resources' ('resoures/myConfig.yml') prepareDefaultValues script: this , customDefaults: 'myConfig.yml' Example content of 'resources/myConfig.yml' in branch 'master' of the repository denoted by 'foo' : general : changeManagement : changeDocumentLabel : 'ChangeDocument\\s?:' cmClientOpts : '-Djavax.net.ssl.trustStore=<path to truststore>' credentialsId : 'CM' endpoint : 'https://example.org/cm' git : from : 'HEAD~1' to : 'HEAD' format : '%b' The properties configured in section 'general/changeManagement' are shared between all change managment related steps. The properties can also be configured on a per-step basis: [ ... ] steps : checkChangeInDevelopment : changeManagement : endpoint : 'https://example.org/cm' [ ... ] failIfStatusIsNotInDevelopment : true The parameters can also be provided when the step is invoked. For examples see below.","title":"Step configuration"},{"location":"steps/checkChangeInDevelopment/#return-value","text":"true in case the change document is in status 'in development'. Otherwise an hudson.AbortException is thrown. In case failIfStatusIsNotInDevelopment is set to false , false is returned in case the change document is not in status 'in development'","title":"Return value"},{"location":"steps/checkChangeInDevelopment/#exceptions","text":"AbortException : If the change id is not provided via parameter and if the change document id cannot be retrieved from the commit history. If the change is not in status in development . In this case no exception will be thrown when failIfStatusIsNotInDevelopment is set to false . IllegalArgumentException : If a mandatory property is not provided.","title":"Exceptions"},{"location":"steps/checkChangeInDevelopment/#examples","text":"// simple case. All mandatory parameters provided via // configuration, changeDocumentId provided via commit // history checkChangeInDevelopment script: this // explict endpoint provided, we search for changeDocumentId // starting at the previous commit (HEAD~1) rather than on // 'origin/master' (the default). checkChangeInDevelopment script: this changeManagement: [ endpoint: 'https:example.org/cm' git: [ from: 'HEAD~1' ] ]","title":"Examples"},{"location":"steps/checksPublishResults/","text":"checksPublishResults \u00b6 Description \u00b6 This step can publish static check results from various sources. Prerequisites \u00b6 static check result files - To use this step, there must be static check result files available. installed plugins: pmd dry findbugs checkstyle warnings core Parameters \u00b6 parameter mandatory default possible values script yes aggregation no true see below tasks no false see below pmd no false see below cpd no false see below findbugs no false see below checkstyle no false see below eslint no false see below pylint no false see below archive no false true , false aggregation - Publishes . tasks - Searches and publishes TODOs in files with the Task Scanner Plugin . pmd - Publishes PMD findings with the PMD plugin . cpd - Publishes CPD findings with the DRY plugin . findbugs - Publishes Findbugs findings with the Findbugs plugin . checkstyle - Publishes Checkstyle findings with the Checkstyle plugin . eslint - Publishes ESLint findings (in JSLint format ) with the Warnings plugin . pylint - Publishes PyLint findings with the Warnings plugin , pylint needs to run with --output-format=parseable option. Each of the parameters aggregation , tasks , pmd , cpd , findbugs , checkstyle , eslint and pylint can be set to true or false but also to a map of parameters to hand in different settings for the tools. aggregation \u00b6 parameter mandatory default possible values thresholds no none see thresholds tasks \u00b6 parameter mandatory default possible values pattern no '**/*.java' archive no true true , false high no 'FIXME' normal no 'TODO,REVISE,XXX' low no thresholds no none see thresholds pmd \u00b6 parameter mandatory default possible values pattern no '**/target/pmd.xml' archive no true true , false thresholds no none see thresholds cpd \u00b6 parameter mandatory default possible values pattern no '**/target/cpd.xml' archive no true true , false thresholds no none see thresholds findbugs \u00b6 parameter mandatory default possible values pattern no '**/target/findbugsXml.xml, **/target/findbugs.xml' archive no true true, false thresholds no none see thresholds checkstyle \u00b6 parameter mandatory default possible values pattern no '**/target/checkstyle-result.xml' archive no true true , false thresholds no none see thresholds eslint \u00b6 parameter mandatory default possible values pattern no '**/eslint.jslint.xml' archive no true true , false thresholds no none see thresholds pylint \u00b6 parameter mandatory default possible values pattern no '**/pylint.log' archive no true true , false thresholds no none see thresholds Step configuration \u00b6 Following parameters can also be specified as step parameters using the global configuration file: aggregation tasks pmd cpd findbugs checkstyle eslint pylint archive Thresholds \u00b6 It is possible to define thresholds to fail the build on a certain count of findings. To achive this, just define your thresholds a followed for the specific check tool: thresholds: [ fail: [ all: 999 , low: 99 , normal: 9 , high: 0 ]] This way, the jenkins will fail the build on 1 high issue, 10 normal issues, 100 low issues or a total issue count of 1000. The thresholds parameter can be set for aggregation , tasks , pmd , cpd , findbugs , checkstyle , eslint and pylint . checksPublishResults ( tasks: true , pmd: [ pattern: '**/target/pmd-results.xml' , thresholds: [ fail: [ low: 100 ]]], cpd: [ archive: false ], aggregation: [ thresholds: [ fail: [ high: 0 ]]], archive: true ) Return value \u00b6 none Side effects \u00b6 If both ESLint and PyLint results are published, they are not correctly aggregated in the aggregator plugin. Exceptions \u00b6 none Example \u00b6 // publish java results from pmd, cpd, checkstyle & findbugs checksPublishResults archive: true , pmd: true , cpd: true , findbugs: true , checkstyle: true , aggregation: [ thresholds: [ fail: [ high: 0 ]]] // publish javascript results from ESLint checksPublishResults archive: true , eslint: [ pattern: '**/result-file-with-fancy-name.xml' ], aggregation: [ thresholds: [ fail: [ high: 0 , normal: 10 ]]] // publish scala results from scalastyle checksPublishResults archive: true , checkstyle: [ pattern: '**/target/scalastyle-result.xml' ] // publish python results from pylint checksPublishResults archive: true , pylint: [ pattern: '**/target/pylint.log' ]","title":"checksPublishResults"},{"location":"steps/checksPublishResults/#checkspublishresults","text":"","title":"checksPublishResults"},{"location":"steps/checksPublishResults/#description","text":"This step can publish static check results from various sources.","title":"Description"},{"location":"steps/checksPublishResults/#prerequisites","text":"static check result files - To use this step, there must be static check result files available. installed plugins: pmd dry findbugs checkstyle warnings core","title":"Prerequisites"},{"location":"steps/checksPublishResults/#parameters","text":"parameter mandatory default possible values script yes aggregation no true see below tasks no false see below pmd no false see below cpd no false see below findbugs no false see below checkstyle no false see below eslint no false see below pylint no false see below archive no false true , false aggregation - Publishes . tasks - Searches and publishes TODOs in files with the Task Scanner Plugin . pmd - Publishes PMD findings with the PMD plugin . cpd - Publishes CPD findings with the DRY plugin . findbugs - Publishes Findbugs findings with the Findbugs plugin . checkstyle - Publishes Checkstyle findings with the Checkstyle plugin . eslint - Publishes ESLint findings (in JSLint format ) with the Warnings plugin . pylint - Publishes PyLint findings with the Warnings plugin , pylint needs to run with --output-format=parseable option. Each of the parameters aggregation , tasks , pmd , cpd , findbugs , checkstyle , eslint and pylint can be set to true or false but also to a map of parameters to hand in different settings for the tools.","title":"Parameters"},{"location":"steps/checksPublishResults/#aggregation","text":"parameter mandatory default possible values thresholds no none see thresholds","title":"aggregation"},{"location":"steps/checksPublishResults/#tasks","text":"parameter mandatory default possible values pattern no '**/*.java' archive no true true , false high no 'FIXME' normal no 'TODO,REVISE,XXX' low no thresholds no none see thresholds","title":"tasks"},{"location":"steps/checksPublishResults/#pmd","text":"parameter mandatory default possible values pattern no '**/target/pmd.xml' archive no true true , false thresholds no none see thresholds","title":"pmd"},{"location":"steps/checksPublishResults/#cpd","text":"parameter mandatory default possible values pattern no '**/target/cpd.xml' archive no true true , false thresholds no none see thresholds","title":"cpd"},{"location":"steps/checksPublishResults/#findbugs","text":"parameter mandatory default possible values pattern no '**/target/findbugsXml.xml, **/target/findbugs.xml' archive no true true, false thresholds no none see thresholds","title":"findbugs"},{"location":"steps/checksPublishResults/#checkstyle","text":"parameter mandatory default possible values pattern no '**/target/checkstyle-result.xml' archive no true true , false thresholds no none see thresholds","title":"checkstyle"},{"location":"steps/checksPublishResults/#eslint","text":"parameter mandatory default possible values pattern no '**/eslint.jslint.xml' archive no true true , false thresholds no none see thresholds","title":"eslint"},{"location":"steps/checksPublishResults/#pylint","text":"parameter mandatory default possible values pattern no '**/pylint.log' archive no true true , false thresholds no none see thresholds","title":"pylint"},{"location":"steps/checksPublishResults/#step-configuration","text":"Following parameters can also be specified as step parameters using the global configuration file: aggregation tasks pmd cpd findbugs checkstyle eslint pylint archive","title":"Step configuration"},{"location":"steps/checksPublishResults/#thresholds","text":"It is possible to define thresholds to fail the build on a certain count of findings. To achive this, just define your thresholds a followed for the specific check tool: thresholds: [ fail: [ all: 999 , low: 99 , normal: 9 , high: 0 ]] This way, the jenkins will fail the build on 1 high issue, 10 normal issues, 100 low issues or a total issue count of 1000. The thresholds parameter can be set for aggregation , tasks , pmd , cpd , findbugs , checkstyle , eslint and pylint . checksPublishResults ( tasks: true , pmd: [ pattern: '**/target/pmd-results.xml' , thresholds: [ fail: [ low: 100 ]]], cpd: [ archive: false ], aggregation: [ thresholds: [ fail: [ high: 0 ]]], archive: true )","title":"Thresholds"},{"location":"steps/checksPublishResults/#return-value","text":"none","title":"Return value"},{"location":"steps/checksPublishResults/#side-effects","text":"If both ESLint and PyLint results are published, they are not correctly aggregated in the aggregator plugin.","title":"Side effects"},{"location":"steps/checksPublishResults/#exceptions","text":"none","title":"Exceptions"},{"location":"steps/checksPublishResults/#example","text":"// publish java results from pmd, cpd, checkstyle & findbugs checksPublishResults archive: true , pmd: true , cpd: true , findbugs: true , checkstyle: true , aggregation: [ thresholds: [ fail: [ high: 0 ]]] // publish javascript results from ESLint checksPublishResults archive: true , eslint: [ pattern: '**/result-file-with-fancy-name.xml' ], aggregation: [ thresholds: [ fail: [ high: 0 , normal: 10 ]]] // publish scala results from scalastyle checksPublishResults archive: true , checkstyle: [ pattern: '**/target/scalastyle-result.xml' ] // publish python results from pylint checksPublishResults archive: true , pylint: [ pattern: '**/target/pylint.log' ]","title":"Example"},{"location":"steps/cloudFoundryDeploy/","text":"cloudFoundryDeploy \u00b6 Description \u00b6 Application will be deployed to a test or production space within Cloud Foundry. Deployment can be done in a standard way in a zero downtime manner (using a blue-green deployment approach ) Deployment supports multiple deployment tools Currently the following are supported: Standard cf push and Bluemix blue-green plugin MTA CF CLI Plugin Prerequsites \u00b6 Cloud Foundry organization, space and deployment user are available Credentials for deployment have been configured in Jenkins with a dedicated Id Parameters \u00b6 parameter mandatory default possible values script yes cloudFoundry yes deployTool no cf_native cf_native, mtaDeployPlugin deployType no standard standard, blue-green keepOldInstance no false true, false dockerImage no s4sdk/docker-cf-cli dockerWorkspace no /home/piper mtaDeployParameters -f mtaExtensionDescriptor no '' mtaPath no '' smokeTestScript no blueGreenCheckScript.sh (provided by library). Can be overwritten using config property 'smokeTestScript' smokeTestStatusCode no 200 stashContent no [] script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for retrieving e.g. configuration parameters. cloudFoundry defines a map containing following properties: apiEndpoint : Cloud Foundry API endpoint (default: https://api.cf.eu10.hana.ondemand.com ) appName : App name of application to be deployed (optional) credentialsId : Credentials to be used for deployment (mandatory) manifest : Manifest to be used for deployment org : Cloud Foundry target organization (mandatory) space : Cloud Foundry target space (mandatory) Example: cloudFoundry: [apiEndpoint: 'https://test.server.com', appName:'cfAppName', credentialsId: 'cfCredentialsId', manifest: 'cfManifest', org: 'cfOrg', space: 'cfSpace'] Note It is also possible to use following configuration parameters instead of cloudFoundry map: cfApiEndpoint cfAppName cfCredentialsId cfManifest cfOrg cfSpace Note Due to an incompatible change in the Cloud Foundry CLI, multiple buildpacks are not supported by this step. If your application contains a list of buildpacks instead a single buildpack , this will be automatically re-written by the step when blue-green deployment is used. deployTool defines the tool which should be used for deployment. deployType defines the type of deployment, either standard deployment which results in a system downtime or a zero-downtime blue-green deployment. keepOldInstance in case of a blue-green deployment the old instance will be deleted by default. If this option is set to true the old instance will remain stopped in the Cloud Foundry space. dockerImage defines the Docker image containing the deployment tools (like cf cli, ...) and dockerWorkspace defines the home directory of the default user of the dockerImage smokeTestScript allows to specify a script which performs a check during blue-green deployment. The script gets the FQDN as parameter and returns exit code 0 in case check returned smokeTestStatusCode . More details can be found here Currently this option is only considered for deployTool cf_native . stashContent defines the stash names which should be unstashed at the beginning of the step. This makes the files available in case the step is started on an empty node. Deployment with cf_native \u00b6 appName in cloudFoundry map (or cfAppName ) defines the name of the application which will be deployed to the Cloud Foundry space. manifest in cloudFoundry maps (or cfManifest ) defines the manifest to be used for Cloud Foundry deployment. Note Cloud Foundry supports the deployment of multiple applications using a single manifest file. This option is supported with Piper. In this case define appName: '' since the app name for the individual applications have to be defined via the manifest. You can find details in the Cloud Foundry Documentation Deployment with mtaDeployPlugin \u00b6 mtaPath define path to *.mtar for deployment. mtaExtensionDescriptor defines additional extension descriptor file for deployment. mtaDeployParameters defines additional parameters passed to mta deployment. Step configuration \u00b6 The following parameters can also be specified as step/stage/general parameters using the global configuration : cloudFoundry deployUser deployTool deployType dockerImage dockerWorkspace mtaDeployParameters mtaExtensionDescriptor mtaPath smokeTestScript smokeTestStatusCode stashContent Example \u00b6 cloudFoundryDeploy ( script: script , deployType: 'blue-green' , cloudFoundry: [ apiEndpoint: 'https://test.server.com' , appName: 'cfAppName' , credentialsId: 'cfCredentialsId' , manifest: 'cfManifest' , org: 'cfOrg' , space: 'cfSpace' ], deployTool: 'cf_native' )","title":"cloudFoundryDeploy"},{"location":"steps/cloudFoundryDeploy/#cloudfoundrydeploy","text":"","title":"cloudFoundryDeploy"},{"location":"steps/cloudFoundryDeploy/#description","text":"Application will be deployed to a test or production space within Cloud Foundry. Deployment can be done in a standard way in a zero downtime manner (using a blue-green deployment approach ) Deployment supports multiple deployment tools Currently the following are supported: Standard cf push and Bluemix blue-green plugin MTA CF CLI Plugin","title":"Description"},{"location":"steps/cloudFoundryDeploy/#prerequsites","text":"Cloud Foundry organization, space and deployment user are available Credentials for deployment have been configured in Jenkins with a dedicated Id","title":"Prerequsites"},{"location":"steps/cloudFoundryDeploy/#parameters","text":"parameter mandatory default possible values script yes cloudFoundry yes deployTool no cf_native cf_native, mtaDeployPlugin deployType no standard standard, blue-green keepOldInstance no false true, false dockerImage no s4sdk/docker-cf-cli dockerWorkspace no /home/piper mtaDeployParameters -f mtaExtensionDescriptor no '' mtaPath no '' smokeTestScript no blueGreenCheckScript.sh (provided by library). Can be overwritten using config property 'smokeTestScript' smokeTestStatusCode no 200 stashContent no [] script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for retrieving e.g. configuration parameters. cloudFoundry defines a map containing following properties: apiEndpoint : Cloud Foundry API endpoint (default: https://api.cf.eu10.hana.ondemand.com ) appName : App name of application to be deployed (optional) credentialsId : Credentials to be used for deployment (mandatory) manifest : Manifest to be used for deployment org : Cloud Foundry target organization (mandatory) space : Cloud Foundry target space (mandatory) Example: cloudFoundry: [apiEndpoint: 'https://test.server.com', appName:'cfAppName', credentialsId: 'cfCredentialsId', manifest: 'cfManifest', org: 'cfOrg', space: 'cfSpace'] Note It is also possible to use following configuration parameters instead of cloudFoundry map: cfApiEndpoint cfAppName cfCredentialsId cfManifest cfOrg cfSpace Note Due to an incompatible change in the Cloud Foundry CLI, multiple buildpacks are not supported by this step. If your application contains a list of buildpacks instead a single buildpack , this will be automatically re-written by the step when blue-green deployment is used. deployTool defines the tool which should be used for deployment. deployType defines the type of deployment, either standard deployment which results in a system downtime or a zero-downtime blue-green deployment. keepOldInstance in case of a blue-green deployment the old instance will be deleted by default. If this option is set to true the old instance will remain stopped in the Cloud Foundry space. dockerImage defines the Docker image containing the deployment tools (like cf cli, ...) and dockerWorkspace defines the home directory of the default user of the dockerImage smokeTestScript allows to specify a script which performs a check during blue-green deployment. The script gets the FQDN as parameter and returns exit code 0 in case check returned smokeTestStatusCode . More details can be found here Currently this option is only considered for deployTool cf_native . stashContent defines the stash names which should be unstashed at the beginning of the step. This makes the files available in case the step is started on an empty node.","title":"Parameters"},{"location":"steps/cloudFoundryDeploy/#deployment-with-cf_native","text":"appName in cloudFoundry map (or cfAppName ) defines the name of the application which will be deployed to the Cloud Foundry space. manifest in cloudFoundry maps (or cfManifest ) defines the manifest to be used for Cloud Foundry deployment. Note Cloud Foundry supports the deployment of multiple applications using a single manifest file. This option is supported with Piper. In this case define appName: '' since the app name for the individual applications have to be defined via the manifest. You can find details in the Cloud Foundry Documentation","title":"Deployment with cf_native"},{"location":"steps/cloudFoundryDeploy/#deployment-with-mtadeployplugin","text":"mtaPath define path to *.mtar for deployment. mtaExtensionDescriptor defines additional extension descriptor file for deployment. mtaDeployParameters defines additional parameters passed to mta deployment.","title":"Deployment with mtaDeployPlugin"},{"location":"steps/cloudFoundryDeploy/#step-configuration","text":"The following parameters can also be specified as step/stage/general parameters using the global configuration : cloudFoundry deployUser deployTool deployType dockerImage dockerWorkspace mtaDeployParameters mtaExtensionDescriptor mtaPath smokeTestScript smokeTestStatusCode stashContent","title":"Step configuration"},{"location":"steps/cloudFoundryDeploy/#example","text":"cloudFoundryDeploy ( script: script , deployType: 'blue-green' , cloudFoundry: [ apiEndpoint: 'https://test.server.com' , appName: 'cfAppName' , credentialsId: 'cfCredentialsId' , manifest: 'cfManifest' , org: 'cfOrg' , space: 'cfSpace' ], deployTool: 'cf_native' )","title":"Example"},{"location":"steps/commonPipelineEnvironment/","text":"commonPipelineEnvironment \u00b6 Description \u00b6 Provides project specific settings. Prerequisites \u00b6 none Method details \u00b6 getArtifactVersion() \u00b6 Description \u00b6 Returns the version of the artifact which is build in the pipeline. Parameters \u00b6 none Return value \u00b6 A String containing the version. Side effects \u00b6 none Exceptions \u00b6 none Example \u00b6 def myVersion = commonPipelineEnvironment . getArtifactVersion () setArtifactVersion(version) \u00b6 Description \u00b6 Sets the version of the artifact which is build in the pipeline. Parameters \u00b6 none Return value \u00b6 none Side effects \u00b6 none Exceptions \u00b6 none Example \u00b6 commonPipelineEnvironment . setArtifactVersion ( '1.2.3' ) getConfigProperties() \u00b6 Description \u00b6 Returns the map of project specific configuration properties. No defensive copy is created. Write operations to the map are visible further down in the pipeline. Parameters \u00b6 none Return value \u00b6 A map containing project specific configuration properties. Side effects \u00b6 none Exceptions \u00b6 none Example \u00b6 commonPipelineEnvironment . getConfigProperties () setConfigProperties(configuration) \u00b6 Description \u00b6 Sets the map of configuration properties. Any existing map is overwritten. Parameters \u00b6 configuration - A map containing the new configuration Return value \u00b6 none Side effects \u00b6 none Exceptions \u00b6 none Example \u00b6 commonPipelineEnvironment . setConfigProperties ([ DEPLOY_HOST: 'deploy-host.com' , DEPLOY_ACCOUNT: 'deploy-account' ]) getConfigProperty(property) \u00b6 Description \u00b6 Gets a specific value from the configuration property. Parameters \u00b6 property - The key of the property. Return value \u00b6 The value associated with key property . null is returned in case the property does not exist. Side effects \u00b6 none Exceptions \u00b6 none Example \u00b6 commonPipelineEnvironment . getConfigProperty ( 'DEPLOY_HOST' ) setConfigProperty(property, value) \u00b6 Description \u00b6 Sets property property with value value . Any existing property with key property is overwritten. Parameters \u00b6 property - The key of the property. value - The value of the property. Return value \u00b6 none Side effects \u00b6 none Exceptions \u00b6 none Example \u00b6 commonPipelineEnvironment . setConfigProperty ( 'DEPLOY_HOST' , 'my-deploy-host.com' ) getInfluxCustomData() \u00b6 Description \u00b6 Returns the Influx custom data which can be collected during pipeline run. Parameters \u00b6 none Return value \u00b6 A Map containing the data collected. Side effects \u00b6 none Exceptions \u00b6 none Example \u00b6 def myInfluxData = commonPipelineEnvironment . getInfluxCustomData () getInfluxCustomDataMap() \u00b6 Description \u00b6 Returns the Influx custom data map which can be collected during pipeline run. It is used for example by step influxWriteData . The data map is a map of maps, like [pipeline_data: [:], my_measurement: [:]] Each map inside the map represents a dedicated measurement in the InfluxDB. Parameters \u00b6 none Return value \u00b6 A Map containing a Map s with data collected. Side effects \u00b6 none Exceptions \u00b6 none Example \u00b6 def myInfluxDataMap = commonPipelineEnvironment . getInfluxCustomDataMap () getMtarFileName() \u00b6 Description \u00b6 Returns the path of the mtar archive file. Parameters \u00b6 none Return value \u00b6 The path of the mtar archive file. Side effects \u00b6 none Exceptions \u00b6 none Example \u00b6 commonPipelineEnvironment . getMtarFileName () setMtarFileName(name) \u00b6 Description \u00b6 Sets the path of the mtar archive file. Any old value is discarded. Parameters \u00b6 mtarFilePath - The path of the mtar archive file name. Side effects \u00b6 none Exceptions \u00b6 none Example \u00b6 commonPipelineEnvironment . setMtarFileName ( 'path/to/foo.mtar' ) getPipelineMeasurement(measurementName) \u00b6 Description \u00b6 Returns the value of a specific pipeline measurement. The measurements are collected with step durationMeasure Parameters \u00b6 Name of the measurement Return value \u00b6 Value of the measurement Side effects \u00b6 none Exceptions \u00b6 none Example \u00b6 def myMeasurementValue = commonPipelineEnvironment . getPipelineMeasurement ( 'build_stage_duration' ) setPipelineMeasurement(measurementName, value) \u00b6 Description \u00b6 This is an internal function! Sets the value of a specific pipeline measurement. Please use the step durationMeasure in a pipeline, instead. Parameters \u00b6 Name of the measurement and its value. Return value \u00b6 none Side effects \u00b6 none Exceptions \u00b6 none Example \u00b6 commonPipelineEnvironment . setPipelineMeasurement ( 'build_stage_duration' , 2345 )","title":"commonPipelineEnvironment"},{"location":"steps/commonPipelineEnvironment/#commonpipelineenvironment","text":"","title":"commonPipelineEnvironment"},{"location":"steps/commonPipelineEnvironment/#description","text":"Provides project specific settings.","title":"Description"},{"location":"steps/commonPipelineEnvironment/#prerequisites","text":"none","title":"Prerequisites"},{"location":"steps/commonPipelineEnvironment/#method-details","text":"","title":"Method details"},{"location":"steps/commonPipelineEnvironment/#getartifactversion","text":"","title":"getArtifactVersion()"},{"location":"steps/commonPipelineEnvironment/#description_1","text":"Returns the version of the artifact which is build in the pipeline.","title":"Description"},{"location":"steps/commonPipelineEnvironment/#parameters","text":"none","title":"Parameters"},{"location":"steps/commonPipelineEnvironment/#return-value","text":"A String containing the version.","title":"Return value"},{"location":"steps/commonPipelineEnvironment/#side-effects","text":"none","title":"Side effects"},{"location":"steps/commonPipelineEnvironment/#exceptions","text":"none","title":"Exceptions"},{"location":"steps/commonPipelineEnvironment/#example","text":"def myVersion = commonPipelineEnvironment . getArtifactVersion ()","title":"Example"},{"location":"steps/commonPipelineEnvironment/#setartifactversionversion","text":"","title":"setArtifactVersion(version)"},{"location":"steps/commonPipelineEnvironment/#description_2","text":"Sets the version of the artifact which is build in the pipeline.","title":"Description"},{"location":"steps/commonPipelineEnvironment/#parameters_1","text":"none","title":"Parameters"},{"location":"steps/commonPipelineEnvironment/#return-value_1","text":"none","title":"Return value"},{"location":"steps/commonPipelineEnvironment/#side-effects_1","text":"none","title":"Side effects"},{"location":"steps/commonPipelineEnvironment/#exceptions_1","text":"none","title":"Exceptions"},{"location":"steps/commonPipelineEnvironment/#example_1","text":"commonPipelineEnvironment . setArtifactVersion ( '1.2.3' )","title":"Example"},{"location":"steps/commonPipelineEnvironment/#getconfigproperties","text":"","title":"getConfigProperties()"},{"location":"steps/commonPipelineEnvironment/#description_3","text":"Returns the map of project specific configuration properties. No defensive copy is created. Write operations to the map are visible further down in the pipeline.","title":"Description"},{"location":"steps/commonPipelineEnvironment/#parameters_2","text":"none","title":"Parameters"},{"location":"steps/commonPipelineEnvironment/#return-value_2","text":"A map containing project specific configuration properties.","title":"Return value"},{"location":"steps/commonPipelineEnvironment/#side-effects_2","text":"none","title":"Side effects"},{"location":"steps/commonPipelineEnvironment/#exceptions_2","text":"none","title":"Exceptions"},{"location":"steps/commonPipelineEnvironment/#example_2","text":"commonPipelineEnvironment . getConfigProperties ()","title":"Example"},{"location":"steps/commonPipelineEnvironment/#setconfigpropertiesconfiguration","text":"","title":"setConfigProperties(configuration)"},{"location":"steps/commonPipelineEnvironment/#description_4","text":"Sets the map of configuration properties. Any existing map is overwritten.","title":"Description"},{"location":"steps/commonPipelineEnvironment/#parameters_3","text":"configuration - A map containing the new configuration","title":"Parameters"},{"location":"steps/commonPipelineEnvironment/#return-value_3","text":"none","title":"Return value"},{"location":"steps/commonPipelineEnvironment/#side-effects_3","text":"none","title":"Side effects"},{"location":"steps/commonPipelineEnvironment/#exceptions_3","text":"none","title":"Exceptions"},{"location":"steps/commonPipelineEnvironment/#example_3","text":"commonPipelineEnvironment . setConfigProperties ([ DEPLOY_HOST: 'deploy-host.com' , DEPLOY_ACCOUNT: 'deploy-account' ])","title":"Example"},{"location":"steps/commonPipelineEnvironment/#getconfigpropertyproperty","text":"","title":"getConfigProperty(property)"},{"location":"steps/commonPipelineEnvironment/#description_5","text":"Gets a specific value from the configuration property.","title":"Description"},{"location":"steps/commonPipelineEnvironment/#parameters_4","text":"property - The key of the property.","title":"Parameters"},{"location":"steps/commonPipelineEnvironment/#return-value_4","text":"The value associated with key property . null is returned in case the property does not exist.","title":"Return value"},{"location":"steps/commonPipelineEnvironment/#side-effects_4","text":"none","title":"Side effects"},{"location":"steps/commonPipelineEnvironment/#exceptions_4","text":"none","title":"Exceptions"},{"location":"steps/commonPipelineEnvironment/#example_4","text":"commonPipelineEnvironment . getConfigProperty ( 'DEPLOY_HOST' )","title":"Example"},{"location":"steps/commonPipelineEnvironment/#setconfigpropertyproperty-value","text":"","title":"setConfigProperty(property, value)"},{"location":"steps/commonPipelineEnvironment/#description_6","text":"Sets property property with value value . Any existing property with key property is overwritten.","title":"Description"},{"location":"steps/commonPipelineEnvironment/#parameters_5","text":"property - The key of the property. value - The value of the property.","title":"Parameters"},{"location":"steps/commonPipelineEnvironment/#return-value_5","text":"none","title":"Return value"},{"location":"steps/commonPipelineEnvironment/#side-effects_5","text":"none","title":"Side effects"},{"location":"steps/commonPipelineEnvironment/#exceptions_5","text":"none","title":"Exceptions"},{"location":"steps/commonPipelineEnvironment/#example_5","text":"commonPipelineEnvironment . setConfigProperty ( 'DEPLOY_HOST' , 'my-deploy-host.com' )","title":"Example"},{"location":"steps/commonPipelineEnvironment/#getinfluxcustomdata","text":"","title":"getInfluxCustomData()"},{"location":"steps/commonPipelineEnvironment/#description_7","text":"Returns the Influx custom data which can be collected during pipeline run.","title":"Description"},{"location":"steps/commonPipelineEnvironment/#parameters_6","text":"none","title":"Parameters"},{"location":"steps/commonPipelineEnvironment/#return-value_6","text":"A Map containing the data collected.","title":"Return value"},{"location":"steps/commonPipelineEnvironment/#side-effects_6","text":"none","title":"Side effects"},{"location":"steps/commonPipelineEnvironment/#exceptions_6","text":"none","title":"Exceptions"},{"location":"steps/commonPipelineEnvironment/#example_6","text":"def myInfluxData = commonPipelineEnvironment . getInfluxCustomData ()","title":"Example"},{"location":"steps/commonPipelineEnvironment/#getinfluxcustomdatamap","text":"","title":"getInfluxCustomDataMap()"},{"location":"steps/commonPipelineEnvironment/#description_8","text":"Returns the Influx custom data map which can be collected during pipeline run. It is used for example by step influxWriteData . The data map is a map of maps, like [pipeline_data: [:], my_measurement: [:]] Each map inside the map represents a dedicated measurement in the InfluxDB.","title":"Description"},{"location":"steps/commonPipelineEnvironment/#parameters_7","text":"none","title":"Parameters"},{"location":"steps/commonPipelineEnvironment/#return-value_7","text":"A Map containing a Map s with data collected.","title":"Return value"},{"location":"steps/commonPipelineEnvironment/#side-effects_7","text":"none","title":"Side effects"},{"location":"steps/commonPipelineEnvironment/#exceptions_7","text":"none","title":"Exceptions"},{"location":"steps/commonPipelineEnvironment/#example_7","text":"def myInfluxDataMap = commonPipelineEnvironment . getInfluxCustomDataMap ()","title":"Example"},{"location":"steps/commonPipelineEnvironment/#getmtarfilename","text":"","title":"getMtarFileName()"},{"location":"steps/commonPipelineEnvironment/#description_9","text":"Returns the path of the mtar archive file.","title":"Description"},{"location":"steps/commonPipelineEnvironment/#parameters_8","text":"none","title":"Parameters"},{"location":"steps/commonPipelineEnvironment/#return-value_8","text":"The path of the mtar archive file.","title":"Return value"},{"location":"steps/commonPipelineEnvironment/#side-effects_8","text":"none","title":"Side effects"},{"location":"steps/commonPipelineEnvironment/#exceptions_8","text":"none","title":"Exceptions"},{"location":"steps/commonPipelineEnvironment/#example_8","text":"commonPipelineEnvironment . getMtarFileName ()","title":"Example"},{"location":"steps/commonPipelineEnvironment/#setmtarfilenamename","text":"","title":"setMtarFileName(name)"},{"location":"steps/commonPipelineEnvironment/#description_10","text":"Sets the path of the mtar archive file. Any old value is discarded.","title":"Description"},{"location":"steps/commonPipelineEnvironment/#parameters_9","text":"mtarFilePath - The path of the mtar archive file name.","title":"Parameters"},{"location":"steps/commonPipelineEnvironment/#side-effects_9","text":"none","title":"Side effects"},{"location":"steps/commonPipelineEnvironment/#exceptions_9","text":"none","title":"Exceptions"},{"location":"steps/commonPipelineEnvironment/#example_9","text":"commonPipelineEnvironment . setMtarFileName ( 'path/to/foo.mtar' )","title":"Example"},{"location":"steps/commonPipelineEnvironment/#getpipelinemeasurementmeasurementname","text":"","title":"getPipelineMeasurement(measurementName)"},{"location":"steps/commonPipelineEnvironment/#description_11","text":"Returns the value of a specific pipeline measurement. The measurements are collected with step durationMeasure","title":"Description"},{"location":"steps/commonPipelineEnvironment/#parameters_10","text":"Name of the measurement","title":"Parameters"},{"location":"steps/commonPipelineEnvironment/#return-value_9","text":"Value of the measurement","title":"Return value"},{"location":"steps/commonPipelineEnvironment/#side-effects_10","text":"none","title":"Side effects"},{"location":"steps/commonPipelineEnvironment/#exceptions_10","text":"none","title":"Exceptions"},{"location":"steps/commonPipelineEnvironment/#example_10","text":"def myMeasurementValue = commonPipelineEnvironment . getPipelineMeasurement ( 'build_stage_duration' )","title":"Example"},{"location":"steps/commonPipelineEnvironment/#setpipelinemeasurementmeasurementname-value","text":"","title":"setPipelineMeasurement(measurementName, value)"},{"location":"steps/commonPipelineEnvironment/#description_12","text":"This is an internal function! Sets the value of a specific pipeline measurement. Please use the step durationMeasure in a pipeline, instead.","title":"Description"},{"location":"steps/commonPipelineEnvironment/#parameters_11","text":"Name of the measurement and its value.","title":"Parameters"},{"location":"steps/commonPipelineEnvironment/#return-value_10","text":"none","title":"Return value"},{"location":"steps/commonPipelineEnvironment/#side-effects_11","text":"none","title":"Side effects"},{"location":"steps/commonPipelineEnvironment/#exceptions_11","text":"none","title":"Exceptions"},{"location":"steps/commonPipelineEnvironment/#example_11","text":"commonPipelineEnvironment . setPipelineMeasurement ( 'build_stage_duration' , 2345 )","title":"Example"},{"location":"steps/dockerExecute/","text":"dockerExecute \u00b6 Description \u00b6 Executes a closure inside a docker container with the specified docker image. The workspace is mounted into the docker image. Proxy environment variables defined on the Jenkins machine are also available in the Docker container. Parameters \u00b6 parameter mandatory default possible values script yes containerPortMappings no dockerEnvVars no [:] dockerImage no '' dockerName no dockerOptions no '' dockerVolumeBind no [:] dockerWorkspace no jenkinsKubernetes no [jnlpAgent:s4sdk/jenkins-agent-k8s:latest] sidecarEnvVars no sidecarImage no sidecarName no sidecarOptions no sidecarVolumeBind no sidecarWorkspace no script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. containerPortMappings : Map which defines per docker image the port mappings, like containerPortMappings: ['selenium/standalone-chrome': [[name: 'selPort', containerPort: 4444, hostPort: 4444]]] dockerEnvVars : Environment variables to set in the container, e.g. [http_proxy:'proxy:8080'] dockerImage : Name of the docker image that should be used. If empty, Docker is not used and the command is executed directly on the Jenkins system. dockerName : Kubernetes case: Name of the container launching dockerImage , SideCar: Name of the container in local network dockerOptions Docker options to be set when starting the container. It can be a list or a string. dockerVolumeBind Volumes that should be mounted into the container. dockerWorkspace : only relevant for Kubernetes case: specifies a dedicated user home directory for the container which will be passed as value for environment variable HOME sidecarEnvVars defines environment variables for the sidecar container, similar to dockerEnvVars sidecarImage : Name of the docker image of the sidecar container. Do not provide this value if no sidecar container is required. sidecarName : as dockerName for the sidecar container sidecarOptions : as dockerOptions for the sidecar container sidecarVolumeBind : as dockerVolumeBind for the sidecar container sidecarWorkspace : as dockerWorkspace for the sidecar container Kubernetes support \u00b6 If the Jenkins is setup on a Kubernetes cluster, then you can execute the closure inside a container of a pod by setting an environment variable ON_K8S to true . However, it will ignore containerPortMappings , dockerOptions and dockerVolumeBind values. Step configuration \u00b6 We recommend to define values of step parameters via config.yml file . In following sections the configuration is possible: parameter general step stage script containerPortMappings X X dockerEnvVars X X dockerImage X X dockerName X X dockerOptions X X dockerVolumeBind X X dockerWorkspace X X jenkinsKubernetes X sidecarEnvVars X X sidecarImage X X sidecarName X X sidecarOptions X X sidecarVolumeBind X X sidecarWorkspace X X Return value \u00b6 none Side effects \u00b6 none Exceptions \u00b6 none Example 1: Run closure inside a docker container \u00b6 dockerExecute ( dockerImage: 'maven:3.5-jdk-7' ){ sh \"mvn clean install\" } Example 2: Run closure inside a container in a kubernetes pod \u00b6 # set environment variable export ON_K8S = true \" dockerExecute ( script: this , dockerImage: 'maven:3.5-jdk-7' ){ sh \"mvn clean install\" } In the above example, the dockerEcecute step will internally invoke dockerExecuteOnKubernetes step and execute the closure inside a pod. Example 3: Run closure inside a container which is attached to a sidecar container (as for example used in seleniumExecuteTests \u00b6 dockerExecute ( script: script , containerPortMappings: [ containerPortMappings: 'selenium/standalone-chrome' :[ containerPort: 4444 , hostPort: 4444 ]], dockerImage: 'node:8-stretch' , dockerName: 'node' , dockerWorkspace: '/home/node' , sidecarImage: 'selenium/standalone-chrome' , sidecarName: 'selenium' , ) { git url: 'https://github.wdf.sap.corp/XXXXX/WebDriverIOTest.git' sh '''npm install node index.js ''' }","title":"dockerExecute"},{"location":"steps/dockerExecute/#dockerexecute","text":"","title":"dockerExecute"},{"location":"steps/dockerExecute/#description","text":"Executes a closure inside a docker container with the specified docker image. The workspace is mounted into the docker image. Proxy environment variables defined on the Jenkins machine are also available in the Docker container.","title":"Description"},{"location":"steps/dockerExecute/#parameters","text":"parameter mandatory default possible values script yes containerPortMappings no dockerEnvVars no [:] dockerImage no '' dockerName no dockerOptions no '' dockerVolumeBind no [:] dockerWorkspace no jenkinsKubernetes no [jnlpAgent:s4sdk/jenkins-agent-k8s:latest] sidecarEnvVars no sidecarImage no sidecarName no sidecarOptions no sidecarVolumeBind no sidecarWorkspace no script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. containerPortMappings : Map which defines per docker image the port mappings, like containerPortMappings: ['selenium/standalone-chrome': [[name: 'selPort', containerPort: 4444, hostPort: 4444]]] dockerEnvVars : Environment variables to set in the container, e.g. [http_proxy:'proxy:8080'] dockerImage : Name of the docker image that should be used. If empty, Docker is not used and the command is executed directly on the Jenkins system. dockerName : Kubernetes case: Name of the container launching dockerImage , SideCar: Name of the container in local network dockerOptions Docker options to be set when starting the container. It can be a list or a string. dockerVolumeBind Volumes that should be mounted into the container. dockerWorkspace : only relevant for Kubernetes case: specifies a dedicated user home directory for the container which will be passed as value for environment variable HOME sidecarEnvVars defines environment variables for the sidecar container, similar to dockerEnvVars sidecarImage : Name of the docker image of the sidecar container. Do not provide this value if no sidecar container is required. sidecarName : as dockerName for the sidecar container sidecarOptions : as dockerOptions for the sidecar container sidecarVolumeBind : as dockerVolumeBind for the sidecar container sidecarWorkspace : as dockerWorkspace for the sidecar container","title":"Parameters"},{"location":"steps/dockerExecute/#kubernetes-support","text":"If the Jenkins is setup on a Kubernetes cluster, then you can execute the closure inside a container of a pod by setting an environment variable ON_K8S to true . However, it will ignore containerPortMappings , dockerOptions and dockerVolumeBind values.","title":"Kubernetes support"},{"location":"steps/dockerExecute/#step-configuration","text":"We recommend to define values of step parameters via config.yml file . In following sections the configuration is possible: parameter general step stage script containerPortMappings X X dockerEnvVars X X dockerImage X X dockerName X X dockerOptions X X dockerVolumeBind X X dockerWorkspace X X jenkinsKubernetes X sidecarEnvVars X X sidecarImage X X sidecarName X X sidecarOptions X X sidecarVolumeBind X X sidecarWorkspace X X","title":"Step configuration"},{"location":"steps/dockerExecute/#return-value","text":"none","title":"Return value"},{"location":"steps/dockerExecute/#side-effects","text":"none","title":"Side effects"},{"location":"steps/dockerExecute/#exceptions","text":"none","title":"Exceptions"},{"location":"steps/dockerExecute/#example-1-run-closure-inside-a-docker-container","text":"dockerExecute ( dockerImage: 'maven:3.5-jdk-7' ){ sh \"mvn clean install\" }","title":"Example 1: Run closure inside a docker container"},{"location":"steps/dockerExecute/#example-2-run-closure-inside-a-container-in-a-kubernetes-pod","text":"# set environment variable export ON_K8S = true \" dockerExecute ( script: this , dockerImage: 'maven:3.5-jdk-7' ){ sh \"mvn clean install\" } In the above example, the dockerEcecute step will internally invoke dockerExecuteOnKubernetes step and execute the closure inside a pod.","title":"Example 2: Run closure inside a container in a kubernetes pod"},{"location":"steps/dockerExecute/#example-3-run-closure-inside-a-container-which-is-attached-to-a-sidecar-container-as-for-example-used-in-seleniumexecutetests","text":"dockerExecute ( script: script , containerPortMappings: [ containerPortMappings: 'selenium/standalone-chrome' :[ containerPort: 4444 , hostPort: 4444 ]], dockerImage: 'node:8-stretch' , dockerName: 'node' , dockerWorkspace: '/home/node' , sidecarImage: 'selenium/standalone-chrome' , sidecarName: 'selenium' , ) { git url: 'https://github.wdf.sap.corp/XXXXX/WebDriverIOTest.git' sh '''npm install node index.js ''' }","title":"Example 3: Run closure inside a container which is attached to a sidecar container (as for example used in seleniumExecuteTests"},{"location":"steps/dockerExecuteOnKubernetes/","text":"dockerExecuteOnKubernetes \u00b6 Description \u00b6 Executes a closure inside a container in a kubernetes pod. Proxy environment variables defined on the Jenkins machine are also available in the container. Prerequisites \u00b6 The Jenkins should be running on kubernetes. An environment variable ON_K8S should be created on Jenkins and initialized to true . This could for example be done via Jenkins - Manage Jenkins - Configure System - Global properties - Environment variables Parameters \u00b6 parameter mandatory default possible values script yes containerCommands no containerEnvVars no containerMap no [:] containerName no containerPortMappings no containerWorkspaces no dockerEnvVars no [:] dockerImage yes dockerWorkspace no '' jenkinsKubernetes no [jnlpAgent:s4sdk/jenkins-agent-k8s:latest] stashExcludes no [workspace:nohup.out] stashIncludes no [workspace:**/*.*] script defines the global script environment of the Jenkins file run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. containerCommands specifies start command for containers to overwrite Piper default ( /usr/bin/tail -f /dev/null ). If container's defaultstart command should be used provide empty string like: ['selenium/standalone-chrome': ''] . containerEnvVars specifies environment variables per container. If not provided dockerEnvVars will be used. containerMap A map of docker image to the name of the container. The pod will be created with all the images from this map and they are labled based on the value field of each map entry. Example: ['maven:3.5-jdk-8-alpine': 'mavenExecute', 'selenium/standalone-chrome': 'selenium', 'famiko/jmeter-base': 'checkJMeter', 's4sdk/docker-cf-cli': 'cloudfoundry'] containerName : optional configuration in combination with containerMap to define the container where the commands should be executed in containerPortMappings : Map which defines per docker image the port mappings, like containerPortMappings: ['selenium/standalone-chrome': [[name: 'selPort', containerPort: 4444, hostPort: 4444]]] containerWorkspaces specifies workspace (=home directory of user) per container. If not provided dockerWorkspace will be used. If empty, home directory will not be set. dockerImage Name of the docker image that should be used. If empty, Docker is not used. dockerEnvVars Environment variables to set in the container, e.g. [http_proxy:'proxy:8080'] dockerWorkspace Docker options to be set when starting the container. It can be a list or a string. Step configuration \u00b6 We recommend to define values of step parameters via config.yml file . In following sections the configuration is possible: parameter general step stage script containerCommands X X containerEnvVars X X containerMap X X containerName X X containerPortMappings X X containerWorkspaces X X dockerEnvVars X X dockerImage X X dockerWorkspace X X jenkinsKubernetes X stashExcludes X X stashIncludes X X Return value \u00b6 none Side effects \u00b6 none Exceptions \u00b6 none Example 1: Run a closure in a single container pod \u00b6 # set environment variable export ON_K8S = true \" dockerExecuteOnKubernetes ( script: script , dockerImage: 'maven:3.5-jdk-7' ){ sh \"mvn clean install\" } In the above example, a pod will be created with a docker container of image maven:3.5-jdk-7 . The closure will be then executed inside the container. Example 2: Run a closure in a multi-container pod \u00b6 # set environment variable export ON_K8S = true \" dockerExecuteOnKubernetes ( script: script , containerMap: [ 'maven:3.5-jdk-8-alpine' : 'maven' , 's4sdk/docker-cf-cli' : 'cfcli' ]){ container ( 'maven' ){ sh \"mvn clean install\" } container ( 'cfcli' ){ sh \"cf plugins\" } } In the above example, a pod will be created with multiple Docker containers that are passed as a containerMap . The containers can be chosen for executing by referring their labels as shown in the example. Example 3: Running a closure in a dedicated container of a multi-container pod \u00b6 # set environment variable export ON_K8S = true \" dockerExecuteOnKubernetes ( script: script , containerCommands: [ 'selenium/standalone-chrome' : '' ], containerMap: [ 'maven:3.5-jdk-8-alpine' : 'maven' , 'selenium/standalone-chrome' : 'selenium' ], containerName: 'maven' , containerPortMappings: [ 'selenium/standalone-chrome' : [ containerPort: 4444 , hostPort: 4444 ]] containerWorkspaces: [ 'selenium/standalone-chrome' : '' ] ){ echo \"Executing inside a Kubernetes Pod inside 'maven' container to run Selenium tests\" sh \"mvn clean install\" }","title":"dockerExecuteOnKubernetes"},{"location":"steps/dockerExecuteOnKubernetes/#dockerexecuteonkubernetes","text":"","title":"dockerExecuteOnKubernetes"},{"location":"steps/dockerExecuteOnKubernetes/#description","text":"Executes a closure inside a container in a kubernetes pod. Proxy environment variables defined on the Jenkins machine are also available in the container.","title":"Description"},{"location":"steps/dockerExecuteOnKubernetes/#prerequisites","text":"The Jenkins should be running on kubernetes. An environment variable ON_K8S should be created on Jenkins and initialized to true . This could for example be done via Jenkins - Manage Jenkins - Configure System - Global properties - Environment variables","title":"Prerequisites"},{"location":"steps/dockerExecuteOnKubernetes/#parameters","text":"parameter mandatory default possible values script yes containerCommands no containerEnvVars no containerMap no [:] containerName no containerPortMappings no containerWorkspaces no dockerEnvVars no [:] dockerImage yes dockerWorkspace no '' jenkinsKubernetes no [jnlpAgent:s4sdk/jenkins-agent-k8s:latest] stashExcludes no [workspace:nohup.out] stashIncludes no [workspace:**/*.*] script defines the global script environment of the Jenkins file run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. containerCommands specifies start command for containers to overwrite Piper default ( /usr/bin/tail -f /dev/null ). If container's defaultstart command should be used provide empty string like: ['selenium/standalone-chrome': ''] . containerEnvVars specifies environment variables per container. If not provided dockerEnvVars will be used. containerMap A map of docker image to the name of the container. The pod will be created with all the images from this map and they are labled based on the value field of each map entry. Example: ['maven:3.5-jdk-8-alpine': 'mavenExecute', 'selenium/standalone-chrome': 'selenium', 'famiko/jmeter-base': 'checkJMeter', 's4sdk/docker-cf-cli': 'cloudfoundry'] containerName : optional configuration in combination with containerMap to define the container where the commands should be executed in containerPortMappings : Map which defines per docker image the port mappings, like containerPortMappings: ['selenium/standalone-chrome': [[name: 'selPort', containerPort: 4444, hostPort: 4444]]] containerWorkspaces specifies workspace (=home directory of user) per container. If not provided dockerWorkspace will be used. If empty, home directory will not be set. dockerImage Name of the docker image that should be used. If empty, Docker is not used. dockerEnvVars Environment variables to set in the container, e.g. [http_proxy:'proxy:8080'] dockerWorkspace Docker options to be set when starting the container. It can be a list or a string.","title":"Parameters"},{"location":"steps/dockerExecuteOnKubernetes/#step-configuration","text":"We recommend to define values of step parameters via config.yml file . In following sections the configuration is possible: parameter general step stage script containerCommands X X containerEnvVars X X containerMap X X containerName X X containerPortMappings X X containerWorkspaces X X dockerEnvVars X X dockerImage X X dockerWorkspace X X jenkinsKubernetes X stashExcludes X X stashIncludes X X","title":"Step configuration"},{"location":"steps/dockerExecuteOnKubernetes/#return-value","text":"none","title":"Return value"},{"location":"steps/dockerExecuteOnKubernetes/#side-effects","text":"none","title":"Side effects"},{"location":"steps/dockerExecuteOnKubernetes/#exceptions","text":"none","title":"Exceptions"},{"location":"steps/dockerExecuteOnKubernetes/#example-1-run-a-closure-in-a-single-container-pod","text":"# set environment variable export ON_K8S = true \" dockerExecuteOnKubernetes ( script: script , dockerImage: 'maven:3.5-jdk-7' ){ sh \"mvn clean install\" } In the above example, a pod will be created with a docker container of image maven:3.5-jdk-7 . The closure will be then executed inside the container.","title":"Example 1: Run a closure in a single container pod"},{"location":"steps/dockerExecuteOnKubernetes/#example-2-run-a-closure-in-a-multi-container-pod","text":"# set environment variable export ON_K8S = true \" dockerExecuteOnKubernetes ( script: script , containerMap: [ 'maven:3.5-jdk-8-alpine' : 'maven' , 's4sdk/docker-cf-cli' : 'cfcli' ]){ container ( 'maven' ){ sh \"mvn clean install\" } container ( 'cfcli' ){ sh \"cf plugins\" } } In the above example, a pod will be created with multiple Docker containers that are passed as a containerMap . The containers can be chosen for executing by referring their labels as shown in the example.","title":"Example 2: Run a closure in a multi-container pod"},{"location":"steps/dockerExecuteOnKubernetes/#example-3-running-a-closure-in-a-dedicated-container-of-a-multi-container-pod","text":"# set environment variable export ON_K8S = true \" dockerExecuteOnKubernetes ( script: script , containerCommands: [ 'selenium/standalone-chrome' : '' ], containerMap: [ 'maven:3.5-jdk-8-alpine' : 'maven' , 'selenium/standalone-chrome' : 'selenium' ], containerName: 'maven' , containerPortMappings: [ 'selenium/standalone-chrome' : [ containerPort: 4444 , hostPort: 4444 ]] containerWorkspaces: [ 'selenium/standalone-chrome' : '' ] ){ echo \"Executing inside a Kubernetes Pod inside 'maven' container to run Selenium tests\" sh \"mvn clean install\" }","title":"Example 3: Running a closure in a dedicated container of a multi-container pod"},{"location":"steps/durationMeasure/","text":"durationMeasure \u00b6 Description \u00b6 This step is used to measure the duration of a set of steps, e.g. a certain stage. The duration is stored in a Map. The measurement data can then be written to an Influx database using step influxWriteData . Tip Measuring for example the duration of pipeline stages helps to identify potential bottlenecks within the deployment pipeline. This then helps to counter identified issues with respective optimization measures, e.g parallelization of tests. Prerequisites \u00b6 none Pipeline configuration \u00b6 none Parameters \u00b6 parameter mandatory default possible values script yes measurementName no test_duration Details: script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. measurementName defines the name of the measurement which is written to the Influx database. Step configuration \u00b6 none Example \u00b6 durationMeasure ( script: this , measurementName: 'build_duration' ) { //execute your build }","title":"durationMeasure"},{"location":"steps/durationMeasure/#durationmeasure","text":"","title":"durationMeasure"},{"location":"steps/durationMeasure/#description","text":"This step is used to measure the duration of a set of steps, e.g. a certain stage. The duration is stored in a Map. The measurement data can then be written to an Influx database using step influxWriteData . Tip Measuring for example the duration of pipeline stages helps to identify potential bottlenecks within the deployment pipeline. This then helps to counter identified issues with respective optimization measures, e.g parallelization of tests.","title":"Description"},{"location":"steps/durationMeasure/#prerequisites","text":"none","title":"Prerequisites"},{"location":"steps/durationMeasure/#pipeline-configuration","text":"none","title":"Pipeline configuration"},{"location":"steps/durationMeasure/#parameters","text":"parameter mandatory default possible values script yes measurementName no test_duration Details: script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. measurementName defines the name of the measurement which is written to the Influx database.","title":"Parameters"},{"location":"steps/durationMeasure/#step-configuration","text":"none","title":"Step configuration"},{"location":"steps/durationMeasure/#example","text":"durationMeasure ( script: this , measurementName: 'build_duration' ) { //execute your build }","title":"Example"},{"location":"steps/gaugeExecuteTests/","text":"gaugeExecuteTests \u00b6 Description \u00b6 In this step Gauge ( getgauge.io ) acceptance tests are executed. Using Gauge it will be possible to have a three-tier test layout: Acceptance Criteria Test implemenation layer Application driver layer This layout is propagated by Jez Humble and Dave Farley in their book \"Continuous Delivery\" as a way to create maintainable acceptance test suites (see \"Continuous Delivery\", p. 190ff). Using Gauge it is possible to write test specifications in Markdown syntax and therefore allow e.g. product owners to write the relevant acceptance test specifications. At the same time it allows the developer to implement the steps described in the specification in her development environment. You can use the sample projects of Gauge. Make sure to run against a Selenium Hub configuration In the test example of gauge-archetype-selenium please make sure to allow it to run against a Selenium hub: Please extend DriverFactory.java for example in following way: String hubUrl = System . getenv ( \"HUB_URL\" ); //when running on a Docker deamon (and not using Kubernetes plugin), Docker images will be linked //in this case hubUrl will be http://selenium:4444/wd/hub due to the linking of the containers hubUrl = ( hubUrl == null ) ? \"http://localhost:4444/wd/hub\" : hubUrl ; Capabilities chromeCapabilities = DesiredCapabilities . chrome (); System . out . println ( \"Running on Selenium Hub: \" + hubUrl ); return new RemoteWebDriver ( new URL ( hubUrl ), chromeCapabilities ); Prerequsites \u00b6 none Example \u00b6 Pipeline step: gaugeExecuteTests script: this , testServerUrl: 'http://test.url' Parameters \u00b6 parameter mandatory default possible values script yes buildTool no maven dockerEnvVars no [HUB:TRUE, HUB_URL:http://localhost:4444/wd/hub] dockerImage no buildTool= maven : maven:3.5-jdk-8 buildTool= npm : node:8-stretch dockerName no buildTool= maven : maven buildTool= npm : npm dockerWorkspace no buildTool= maven : buildTool= npm : /home/node failOnError no false gitBranch no gitSshKeyCredentialsId no `` installCommand no curl -SsL https://downloads.gauge.org/stable | sh -s -- --location=$HOME/bin/gauge languageRunner no buildTool= maven : java buildTool= npm : js runCommand no buildTool= maven : mvn test-compile gauge:execute buildTool= npm : gauge run stashContent no buildDescriptor tests testOptions no buildTool= maven : -DspecsDir=specs buildTool= npm : specs testRepository no testServerUrl no Details: script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. buildTool defines the build tool to be used for the test execution. dockerEnvVars , see step dockerExecute dockerImage , see step dockerExecute dockerName , see step dockerExecute dockerWorkspace , see step dockerExecute With failOnError you can define the behavior, in case tests fail. When this is set to true test results cannot be recorded using the publishTestResults step afterwards. installCommand defines the command for installing Gauge. In case the dockerImage already contains Gauge it can be set to empty: ``. languageRunner defines the Gauge language runner to be used. runCommand defines the command which is used for executing Gauge. If specific stashes should be considered for the tests, you can pass this via parameter stashContent testOptions allows to set specific options for the Gauge execution. Details can be found for example in the Gauge Maven plugin documentation In case the test implementation is stored in a different repository than the code itself, you can define the repository containing the tests using parameter testRepository and if required gitBranch (for a different branch than master) and gitSshKeyCredentialsId (for protected repositories). For protected repositories the testRepository needs to contain the ssh git url. testServerUrl is passed as environment variable TARGET_SERVER_URL to the test execution. Tests running against the system should read the host information from this environment variable in order to be infrastructure agnostic. Step configuration \u00b6 We recommend to define values of step parameters via config.yml file . In following sections the configuration is possible: parameter general step stage script buildTool X X dockerEnvVars X X dockerImage X X dockerName X X dockerWorkspace X X failOnError X X gitBranch X X gitSshKeyCredentialsId X X stashContent X X testOptions X X testRepository X X testServerUrl X X","title":"gaugeExecuteTests"},{"location":"steps/gaugeExecuteTests/#gaugeexecutetests","text":"","title":"gaugeExecuteTests"},{"location":"steps/gaugeExecuteTests/#description","text":"In this step Gauge ( getgauge.io ) acceptance tests are executed. Using Gauge it will be possible to have a three-tier test layout: Acceptance Criteria Test implemenation layer Application driver layer This layout is propagated by Jez Humble and Dave Farley in their book \"Continuous Delivery\" as a way to create maintainable acceptance test suites (see \"Continuous Delivery\", p. 190ff). Using Gauge it is possible to write test specifications in Markdown syntax and therefore allow e.g. product owners to write the relevant acceptance test specifications. At the same time it allows the developer to implement the steps described in the specification in her development environment. You can use the sample projects of Gauge. Make sure to run against a Selenium Hub configuration In the test example of gauge-archetype-selenium please make sure to allow it to run against a Selenium hub: Please extend DriverFactory.java for example in following way: String hubUrl = System . getenv ( \"HUB_URL\" ); //when running on a Docker deamon (and not using Kubernetes plugin), Docker images will be linked //in this case hubUrl will be http://selenium:4444/wd/hub due to the linking of the containers hubUrl = ( hubUrl == null ) ? \"http://localhost:4444/wd/hub\" : hubUrl ; Capabilities chromeCapabilities = DesiredCapabilities . chrome (); System . out . println ( \"Running on Selenium Hub: \" + hubUrl ); return new RemoteWebDriver ( new URL ( hubUrl ), chromeCapabilities );","title":"Description"},{"location":"steps/gaugeExecuteTests/#prerequsites","text":"none","title":"Prerequsites"},{"location":"steps/gaugeExecuteTests/#example","text":"Pipeline step: gaugeExecuteTests script: this , testServerUrl: 'http://test.url'","title":"Example"},{"location":"steps/gaugeExecuteTests/#parameters","text":"parameter mandatory default possible values script yes buildTool no maven dockerEnvVars no [HUB:TRUE, HUB_URL:http://localhost:4444/wd/hub] dockerImage no buildTool= maven : maven:3.5-jdk-8 buildTool= npm : node:8-stretch dockerName no buildTool= maven : maven buildTool= npm : npm dockerWorkspace no buildTool= maven : buildTool= npm : /home/node failOnError no false gitBranch no gitSshKeyCredentialsId no `` installCommand no curl -SsL https://downloads.gauge.org/stable | sh -s -- --location=$HOME/bin/gauge languageRunner no buildTool= maven : java buildTool= npm : js runCommand no buildTool= maven : mvn test-compile gauge:execute buildTool= npm : gauge run stashContent no buildDescriptor tests testOptions no buildTool= maven : -DspecsDir=specs buildTool= npm : specs testRepository no testServerUrl no Details: script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. buildTool defines the build tool to be used for the test execution. dockerEnvVars , see step dockerExecute dockerImage , see step dockerExecute dockerName , see step dockerExecute dockerWorkspace , see step dockerExecute With failOnError you can define the behavior, in case tests fail. When this is set to true test results cannot be recorded using the publishTestResults step afterwards. installCommand defines the command for installing Gauge. In case the dockerImage already contains Gauge it can be set to empty: ``. languageRunner defines the Gauge language runner to be used. runCommand defines the command which is used for executing Gauge. If specific stashes should be considered for the tests, you can pass this via parameter stashContent testOptions allows to set specific options for the Gauge execution. Details can be found for example in the Gauge Maven plugin documentation In case the test implementation is stored in a different repository than the code itself, you can define the repository containing the tests using parameter testRepository and if required gitBranch (for a different branch than master) and gitSshKeyCredentialsId (for protected repositories). For protected repositories the testRepository needs to contain the ssh git url. testServerUrl is passed as environment variable TARGET_SERVER_URL to the test execution. Tests running against the system should read the host information from this environment variable in order to be infrastructure agnostic.","title":"Parameters"},{"location":"steps/gaugeExecuteTests/#step-configuration","text":"We recommend to define values of step parameters via config.yml file . In following sections the configuration is possible: parameter general step stage script buildTool X X dockerEnvVars X X dockerImage X X dockerName X X dockerWorkspace X X failOnError X X gitBranch X X gitSshKeyCredentialsId X X stashContent X X testOptions X X testRepository X X testServerUrl X X","title":"Step configuration"},{"location":"steps/githubPublishRelease/","text":"githubPublishRelease \u00b6 Description \u00b6 This step creates a tag in your GitHub repository together with a release. The release can be filled with text plus additional information like: Closed pull request since last release Closed issues since last release link to delta information showing all commits since last release The result looks like Prerequisites \u00b6 You need to create a personal access token within GitHub and add this to the Jenkins credentials store. Please see GitHub documentation for details about creating the personal access token . Example \u00b6 Usage of pipeline step: githubPublishRelease script: this , releaseBodyHeader: \"**This is the latest success!**<br />\" Parameters \u00b6 parameter mandatory default possible values script yes addClosedIssues no false addDeltaToLastRelease no false customFilterExtension no `` excludeLabels no duplicate invalid question wontfix githubApiUrl no //https://api.github.com githubOrg yes script.commonPipelineEnvironment.getGitFolder() githubRepo yes script.commonPipelineEnvironment.getGitRepo() githubServerUrl no https://github.com githubTokenCredentialsId yes releaseBodyHeader no version yes script.commonPipelineEnvironment.getArtifactVersion() Details \u00b6 script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. All GitHub related properties allow you to overwrite the default behavior of identifying e.g. GitHub organization, GitHub repository. version defines the version number which will be written as tag as well as release name By defining the releaseBodyHeader you can specify the content which will appear for the release If you set addClosedIssues to true , a list of all closed issues and merged pull-requests since the last release will added below the releaseBodyHeader If you set addDeltaToLastRelease to true , a link will be added to the relese information that brings up all commits since the last release. By passing the parameter customFilterExtension it is possible to pass additional filter criteria for retrieving closed issues since the last release. Additional criteria could be for example specific label , or filter according to GitHub API documentation . It is possible to exclude issues with dedicated labels using parameter excludeLabels . Usage is like excludeLabels: ['label1', 'label2'] Step configuration \u00b6 We recommend to define values of step parameters via config.yml file . In following sections the configuration is possible: parameter general step stage script addClosedIssues X X addDeltaToLastRelease X X customFilterExtension X X excludeLabels X X githubApiUrl X X X githubOrg X X githubRepo X X githubServerUrl X X X githubTokenCredentialsId X X X releaseBodyHeader X X version X X","title":"githubPublishRelease"},{"location":"steps/githubPublishRelease/#githubpublishrelease","text":"","title":"githubPublishRelease"},{"location":"steps/githubPublishRelease/#description","text":"This step creates a tag in your GitHub repository together with a release. The release can be filled with text plus additional information like: Closed pull request since last release Closed issues since last release link to delta information showing all commits since last release The result looks like","title":"Description"},{"location":"steps/githubPublishRelease/#prerequisites","text":"You need to create a personal access token within GitHub and add this to the Jenkins credentials store. Please see GitHub documentation for details about creating the personal access token .","title":"Prerequisites"},{"location":"steps/githubPublishRelease/#example","text":"Usage of pipeline step: githubPublishRelease script: this , releaseBodyHeader: \"**This is the latest success!**<br />\"","title":"Example"},{"location":"steps/githubPublishRelease/#parameters","text":"parameter mandatory default possible values script yes addClosedIssues no false addDeltaToLastRelease no false customFilterExtension no `` excludeLabels no duplicate invalid question wontfix githubApiUrl no //https://api.github.com githubOrg yes script.commonPipelineEnvironment.getGitFolder() githubRepo yes script.commonPipelineEnvironment.getGitRepo() githubServerUrl no https://github.com githubTokenCredentialsId yes releaseBodyHeader no version yes script.commonPipelineEnvironment.getArtifactVersion()","title":"Parameters"},{"location":"steps/githubPublishRelease/#details","text":"script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. All GitHub related properties allow you to overwrite the default behavior of identifying e.g. GitHub organization, GitHub repository. version defines the version number which will be written as tag as well as release name By defining the releaseBodyHeader you can specify the content which will appear for the release If you set addClosedIssues to true , a list of all closed issues and merged pull-requests since the last release will added below the releaseBodyHeader If you set addDeltaToLastRelease to true , a link will be added to the relese information that brings up all commits since the last release. By passing the parameter customFilterExtension it is possible to pass additional filter criteria for retrieving closed issues since the last release. Additional criteria could be for example specific label , or filter according to GitHub API documentation . It is possible to exclude issues with dedicated labels using parameter excludeLabels . Usage is like excludeLabels: ['label1', 'label2']","title":"Details"},{"location":"steps/githubPublishRelease/#step-configuration","text":"We recommend to define values of step parameters via config.yml file . In following sections the configuration is possible: parameter general step stage script addClosedIssues X X addDeltaToLastRelease X X customFilterExtension X X excludeLabels X X githubApiUrl X X X githubOrg X X githubRepo X X githubServerUrl X X X githubTokenCredentialsId X X X releaseBodyHeader X X version X X","title":"Step configuration"},{"location":"steps/handlePipelineStepErrors/","text":"handlePipelineStepErrors \u00b6 Description \u00b6 Used by other steps to make error analysis easier. Lists parameters and other data available to the step in which the error occurs. Prerequisites \u00b6 none Parameters \u00b6 parameter mandatory default possible values stepParameters yes stepName yes echoDetails yes true true, false stepParameters - The parameters from the step to be executed. The list of parameters is then shown in the console output. stepName - The name of the step executed to be shown in the console output. echoDetails - If set to true the following will be output to the console: Step beginning: --- BEGIN LIBRARY STEP: ${stepName}.groovy --- Step end: --- END LIBRARY STEP: ${stepName}.groovy --- Step errors: ---------------------------------------------------------- --- ERROR OCCURED IN LIBRARY STEP: ${stepName} ---------------------------------------------------------- FOLLOWING PARAMETERS WERE AVAILABLE TO THIS STEP: *** ${stepParameters} *** ERROR WAS: *** ${err} *** FURTHER INFORMATION: * Documentation of step ${stepName}: .../${stepName}/ * Pipeline documentation: https://... * GitHub repository for pipeline steps: https://... ---------------------------------------------------------- Step configuration \u00b6 none Return value \u00b6 none Side effects \u00b6 none Exceptions \u00b6 none Example \u00b6 handlePipelineStepErrors ( stepName: 'executeHealthCheck' , stepParameters: parameters ) { def url = new Utils (). getMandatoryParameter ( parameters , 'url' , null ) def statusCode = curl ( url ) if ( statusCode != '200' ) error \"Health Check failed: ${statusCode}\" }","title":"handlePipelineStepErrors"},{"location":"steps/handlePipelineStepErrors/#handlepipelinesteperrors","text":"","title":"handlePipelineStepErrors"},{"location":"steps/handlePipelineStepErrors/#description","text":"Used by other steps to make error analysis easier. Lists parameters and other data available to the step in which the error occurs.","title":"Description"},{"location":"steps/handlePipelineStepErrors/#prerequisites","text":"none","title":"Prerequisites"},{"location":"steps/handlePipelineStepErrors/#parameters","text":"parameter mandatory default possible values stepParameters yes stepName yes echoDetails yes true true, false stepParameters - The parameters from the step to be executed. The list of parameters is then shown in the console output. stepName - The name of the step executed to be shown in the console output. echoDetails - If set to true the following will be output to the console: Step beginning: --- BEGIN LIBRARY STEP: ${stepName}.groovy --- Step end: --- END LIBRARY STEP: ${stepName}.groovy --- Step errors: ---------------------------------------------------------- --- ERROR OCCURED IN LIBRARY STEP: ${stepName} ---------------------------------------------------------- FOLLOWING PARAMETERS WERE AVAILABLE TO THIS STEP: *** ${stepParameters} *** ERROR WAS: *** ${err} *** FURTHER INFORMATION: * Documentation of step ${stepName}: .../${stepName}/ * Pipeline documentation: https://... * GitHub repository for pipeline steps: https://... ----------------------------------------------------------","title":"Parameters"},{"location":"steps/handlePipelineStepErrors/#step-configuration","text":"none","title":"Step configuration"},{"location":"steps/handlePipelineStepErrors/#return-value","text":"none","title":"Return value"},{"location":"steps/handlePipelineStepErrors/#side-effects","text":"none","title":"Side effects"},{"location":"steps/handlePipelineStepErrors/#exceptions","text":"none","title":"Exceptions"},{"location":"steps/handlePipelineStepErrors/#example","text":"handlePipelineStepErrors ( stepName: 'executeHealthCheck' , stepParameters: parameters ) { def url = new Utils (). getMandatoryParameter ( parameters , 'url' , null ) def statusCode = curl ( url ) if ( statusCode != '200' ) error \"Health Check failed: ${statusCode}\" }","title":"Example"},{"location":"steps/healthExecuteCheck/","text":"healthExecuteCheck \u00b6 Description \u00b6 Calls the health endpoint url of the application. The intention of the check is to verify that a suitable health endpoint is available. Such a health endpoint is required for operation purposes. This check is used as a real-life test for your productive health endpoints. Check Depth Typically, tools performing simple health checks are not too smart. Therefore it is important to choose an endpoint for checking wisely. This check therefore only checks if the application/service url returns HTTP 200 . This is in line with health check capabilities of platforms which are used for example in load balancing scenarios. Here you can find an example for Amazon AWS . Prerequisites \u00b6 Endpoint for health check is configured. Warning The health endpoint needs to be available without authentication! Tip If using Spring Boot framework, ideally the provided /health endpoint is used and extended by development. Further information can be found in the Spring Boot documenation for Endpoints Example \u00b6 Pipeline step: healthExecuteCheck testServerUrl: 'https://testserver.com' Parameters \u00b6 parameter mandatory default possible values script yes healthEndpoint no `` testServerUrl no Details: script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. Health check function is called providing full qualified testServerUrl (and optionally with healthEndpoint if endpoint is not the standard url) to the health check. In case response of the call is different than HTTP 200 OK the health check fails and the pipeline stops . Step configuration \u00b6 We recommend to define values of step parameters via config.yml file . In following sections the configuration is possible: parameter general step stage script healthEndpoint X X X testServerUrl X X X","title":"healthExecuteCheck"},{"location":"steps/healthExecuteCheck/#healthexecutecheck","text":"","title":"healthExecuteCheck"},{"location":"steps/healthExecuteCheck/#description","text":"Calls the health endpoint url of the application. The intention of the check is to verify that a suitable health endpoint is available. Such a health endpoint is required for operation purposes. This check is used as a real-life test for your productive health endpoints. Check Depth Typically, tools performing simple health checks are not too smart. Therefore it is important to choose an endpoint for checking wisely. This check therefore only checks if the application/service url returns HTTP 200 . This is in line with health check capabilities of platforms which are used for example in load balancing scenarios. Here you can find an example for Amazon AWS .","title":"Description"},{"location":"steps/healthExecuteCheck/#prerequisites","text":"Endpoint for health check is configured. Warning The health endpoint needs to be available without authentication! Tip If using Spring Boot framework, ideally the provided /health endpoint is used and extended by development. Further information can be found in the Spring Boot documenation for Endpoints","title":"Prerequisites"},{"location":"steps/healthExecuteCheck/#example","text":"Pipeline step: healthExecuteCheck testServerUrl: 'https://testserver.com'","title":"Example"},{"location":"steps/healthExecuteCheck/#parameters","text":"parameter mandatory default possible values script yes healthEndpoint no `` testServerUrl no Details: script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. Health check function is called providing full qualified testServerUrl (and optionally with healthEndpoint if endpoint is not the standard url) to the health check. In case response of the call is different than HTTP 200 OK the health check fails and the pipeline stops .","title":"Parameters"},{"location":"steps/healthExecuteCheck/#step-configuration","text":"We recommend to define values of step parameters via config.yml file . In following sections the configuration is possible: parameter general step stage script healthEndpoint X X X testServerUrl X X X","title":"Step configuration"},{"location":"steps/influxWriteData/","text":"influxWriteData \u00b6 Description \u00b6 Since your Continuous Delivery Pipeline in Jenkins provides your productive development and delivery infrastructure you should monitor the pipeline to ensure it runs as expected. How to setup this monitoring is described in the following. You basically need three components: The InfluxDB Jenkins plugin which allows you to send build metrics to InfluxDB servers The InfluxDB to store this data (Docker available) A Grafana dashboard to visualize the data stored in InfluxDB (Docker available) no InfluxDB available? If you don't have an InfluxDB available yet this step will still provide you some benefit. It will create following files for you and archive them into your build: jenkins_data.json : This file gives you build-specific information, like e.g. build result, stage where the build failed pipeline_data.json : This file gives you detailed information about your pipeline, e.g. stage durations, steps executed, ... Prerequisites \u00b6 Setting up InfluxDB with Grafana \u00b6 The easiest way to start with is using the available official docker images. You can either run these docker containers on the same host on which you run your Jenkins or each docker on individual VMs (hosts). Very basic setup can be done like that (with user \"admin\" and password \"adminPwd\" for both InfluxDB and Grafana): docker run -d -p 8083:8083 -p 8086:8086 --restart=always --name influxdb -v /var/influx_data:/var/lib/influxdb influxdb docker run -d -p 3000:3000 --name grafana --restart=always --link influxdb:influxdb -e \"GF_SECURITY_ADMIN_PASSWORD=adminPwd\" grafana/grafana For more advanced setup please reach out to the respective documentation: https://hub.docker.com/_/influxdb/ (and https://github.com/docker-library/docs/tree/master/influxdb) https://hub.docker.com/r/grafana/grafana/ (and https://github.com/grafana/grafana-docker) After you have started your InfluxDB docker you need to create a database: in a Webbrowser open the InfluxDB Web-UI using the following URL: <host of your docker>:8083 (port 8083 is used for access via Web-UI, for Jenkins you use port 8086 to access the DB) create new DB (the name of this DB you need to provide later to Jenkins) create Admin user (this user you need to provide later to Jenkins) With InfluxDB version 1.1 the InfluxDB Web-UI is deprecated You can perform the above steps via commandline: The following command will create a database with name <databasename> curl -i -XPOST http://localhost:8086/query --data-urlencode \"q=CREATE DATABASE \\<databasename\\>\" The admin user with the name <adminusername> and the password <adminuserpwd> can be created with curl -i -XPOST http://localhost:8086/query --data-urlencode \"q=CREATE USER \\<adminusername\\> WITH PASSWORD '\\<adminuserpwd\\>' WITH ALL PRIVILEGES\" Once you have started both docker containers and Influx and Grafana are running you need to configure the Jenkins Plugin according to your settings. Pipeline configuration \u00b6 To setup your Jenkins you need to do two configuration steps: Configure Jenkins (via Manage Jenkins) Adapt pipeline configuration Configure Jenkins \u00b6 Once the plugin is available in your Jenkins: go to \"Manage Jenkins\" > \"Configure System\" > scroll down to section \"influxdb target\" maintain Influx data Jenkins as a Service For Jenkins as a Service instances this is already preset to the local InfluxDB with the name jenkins . In this case there is not need to do any additional configuration. Adapt pipeline configuration \u00b6 You need to define the influxDB server in your pipeline as it is defined in the InfluxDb plugin configuration (see above). influxDBServer = jenkins Parameters \u00b6 parameter mandatory default possible values script yes artifactVersion yes commonPipelineEnvironment.getArtifactVersion() influxServer no jenkins influxPrefix no null Step configuration \u00b6 The following parameters can also be specified as step parameters using the global configuration file: influxServer influxPrefix Example \u00b6 influxWriteData script: this Work with InfluxDB and Grafana \u00b6 You can access your Grafana via Web-UI: <host of your grafana(-docker)>:<port3000> (or another port in case you have defined another one when starting your docker) As a first step you need to add your InfluxDB as Data source to your Grafana: Login as user admin (PW as defined when starting your docker) in the navigation go to data sources -> add data source: name type: InfluxDB Url: http://<host of your InfluxDB server>:<port> Access: direct (not via proxy) database: <name of the DB as specified above> User: <name of the admin user as specified in step above> Password: <password of the admin user as specified in step above> Jenkins as a Service For Jenkins as a Service the data source configuration is already available. Therefore no need to go through the data source configuration step unless you want to add addtional data sources. Data collected in InfluxDB \u00b6 The Influx plugin collects following data in the Piper context: All data as per default InfluxDB plugin capabilities Additional data collected via commonPipelineEnvironment.setInfluxCustomDataProperty() and via commonPipelineEnvironment.setPipelineMeasurement() Add custom information to your InfluxDB You can simply add custom data collected during your pipeline runs via available data objects. Example: //add data to measurement jenkins_custom_data - value can be a String or a Number commonPipelineEnvironment . setInfluxCustomDataProperty ( 'myProperty' , 2018 ) Collected InfluxDB measurements \u00b6 Measurements are potentially pre-fixed - see parameter influxPrefix above. Measurement name data column description All measurements build_number project_name All below measurements will have these columns. Details see InfluxDB plugin documentation jenkins_data build_result build_time last_successful_build tests_failed tests_skipped tests_total ... Details see InfluxDB plugin documentation cobertura_data cobertura_branch_coverage_rate cobertura_class_coverage_rate cobertura_line_coverage_rate cobertura_package_coverage_rate ... Details see InfluxDB plugin documentation jacoco_data jacoco_branch_coverage_rate jacoco_class_coverage_rate jacoco_instruction_coverage_rate jacoco_line_coverage_rate jacoco_method_coverage_rate Details see InfluxDB plugin documentation performance_data 90Percentile average max median min error_count error_percent ... Details see InfluxDB plugin documentation sonarqube_data blocker_issues critical_issues info_issues major_issues minor_issues lines_of_code ... Details see InfluxDB plugin documentation jenkins_custom_data Piper fills following colums by default: build_result build_result_key build_step (->step in case of error) build_error (->error message in case of error) filled by commonPipelineEnvironment.setInfluxCustomDataProperty() pipeline_data Examples from the Piper templates: build_duration opa_duration deploy_test_duration deploy_test_duration fortify_duration release_duration ... filled by step measureDuration using parameter measurementName step_data Considered, e.g.: build_quality (Milestone/Release) build_url bats checkmarx fortify gauge nsp opa opensourcedependency ppms jmeter supa snyk sonar sourceclear uiveri5 vulas whitesource traceability ... xmakestage xmakepromote filled by commonPipelineEnvironment.setInfluxStepData() Examples for InfluxDB queries which can be used in Grafana \u00b6 Project Names containing dashes (-) The InfluxDB plugin replaces dashes (-) with underscores (_). Please keep this in mind when specifying your project_name for a InfluxDB query. Example 1: Select last 10 successful builds \u00b6 select top ( build_number , 10 ), build_result from jenkins_data WHERE build_result = 'SUCCESS' Example 2: Select last 10 step names of failed builds \u00b6 select top ( build_number , 10 ), build_result , build_step from jenkins_custom_data WHERE build_result = 'FAILURE' Example 3: Select build duration of step for a specific project \u00b6 select build_duration / 1000 from \"pipeline_data\" WHERE project_name = 'PiperTestOrg_piper_test_master' Example 4: Get transparency about successful/failed steps for a specific project \u00b6 select top ( build_number , 10 ) AS \"Build\" , build_url , build_quality , fortify , gauge , vulas , opa from step_data WHERE project_name = 'PiperTestOrg_piper_test_master' Note With this query you can create transparency about which steps ran successfully / not successfully in your pipeline and which ones were not executed at all. By specifying all the steps you consider relevant in your select statement it is very easy to create this transparency.","title":"influxWriteData"},{"location":"steps/influxWriteData/#influxwritedata","text":"","title":"influxWriteData"},{"location":"steps/influxWriteData/#description","text":"Since your Continuous Delivery Pipeline in Jenkins provides your productive development and delivery infrastructure you should monitor the pipeline to ensure it runs as expected. How to setup this monitoring is described in the following. You basically need three components: The InfluxDB Jenkins plugin which allows you to send build metrics to InfluxDB servers The InfluxDB to store this data (Docker available) A Grafana dashboard to visualize the data stored in InfluxDB (Docker available) no InfluxDB available? If you don't have an InfluxDB available yet this step will still provide you some benefit. It will create following files for you and archive them into your build: jenkins_data.json : This file gives you build-specific information, like e.g. build result, stage where the build failed pipeline_data.json : This file gives you detailed information about your pipeline, e.g. stage durations, steps executed, ...","title":"Description"},{"location":"steps/influxWriteData/#prerequisites","text":"","title":"Prerequisites"},{"location":"steps/influxWriteData/#setting-up-influxdb-with-grafana","text":"The easiest way to start with is using the available official docker images. You can either run these docker containers on the same host on which you run your Jenkins or each docker on individual VMs (hosts). Very basic setup can be done like that (with user \"admin\" and password \"adminPwd\" for both InfluxDB and Grafana): docker run -d -p 8083:8083 -p 8086:8086 --restart=always --name influxdb -v /var/influx_data:/var/lib/influxdb influxdb docker run -d -p 3000:3000 --name grafana --restart=always --link influxdb:influxdb -e \"GF_SECURITY_ADMIN_PASSWORD=adminPwd\" grafana/grafana For more advanced setup please reach out to the respective documentation: https://hub.docker.com/_/influxdb/ (and https://github.com/docker-library/docs/tree/master/influxdb) https://hub.docker.com/r/grafana/grafana/ (and https://github.com/grafana/grafana-docker) After you have started your InfluxDB docker you need to create a database: in a Webbrowser open the InfluxDB Web-UI using the following URL: <host of your docker>:8083 (port 8083 is used for access via Web-UI, for Jenkins you use port 8086 to access the DB) create new DB (the name of this DB you need to provide later to Jenkins) create Admin user (this user you need to provide later to Jenkins) With InfluxDB version 1.1 the InfluxDB Web-UI is deprecated You can perform the above steps via commandline: The following command will create a database with name <databasename> curl -i -XPOST http://localhost:8086/query --data-urlencode \"q=CREATE DATABASE \\<databasename\\>\" The admin user with the name <adminusername> and the password <adminuserpwd> can be created with curl -i -XPOST http://localhost:8086/query --data-urlencode \"q=CREATE USER \\<adminusername\\> WITH PASSWORD '\\<adminuserpwd\\>' WITH ALL PRIVILEGES\" Once you have started both docker containers and Influx and Grafana are running you need to configure the Jenkins Plugin according to your settings.","title":"Setting up InfluxDB with Grafana"},{"location":"steps/influxWriteData/#pipeline-configuration","text":"To setup your Jenkins you need to do two configuration steps: Configure Jenkins (via Manage Jenkins) Adapt pipeline configuration","title":"Pipeline configuration"},{"location":"steps/influxWriteData/#configure-jenkins","text":"Once the plugin is available in your Jenkins: go to \"Manage Jenkins\" > \"Configure System\" > scroll down to section \"influxdb target\" maintain Influx data Jenkins as a Service For Jenkins as a Service instances this is already preset to the local InfluxDB with the name jenkins . In this case there is not need to do any additional configuration.","title":"Configure Jenkins"},{"location":"steps/influxWriteData/#adapt-pipeline-configuration","text":"You need to define the influxDB server in your pipeline as it is defined in the InfluxDb plugin configuration (see above). influxDBServer = jenkins","title":"Adapt pipeline configuration"},{"location":"steps/influxWriteData/#parameters","text":"parameter mandatory default possible values script yes artifactVersion yes commonPipelineEnvironment.getArtifactVersion() influxServer no jenkins influxPrefix no null","title":"Parameters"},{"location":"steps/influxWriteData/#step-configuration","text":"The following parameters can also be specified as step parameters using the global configuration file: influxServer influxPrefix","title":"Step configuration"},{"location":"steps/influxWriteData/#example","text":"influxWriteData script: this","title":"Example"},{"location":"steps/influxWriteData/#work-with-influxdb-and-grafana","text":"You can access your Grafana via Web-UI: <host of your grafana(-docker)>:<port3000> (or another port in case you have defined another one when starting your docker) As a first step you need to add your InfluxDB as Data source to your Grafana: Login as user admin (PW as defined when starting your docker) in the navigation go to data sources -> add data source: name type: InfluxDB Url: http://<host of your InfluxDB server>:<port> Access: direct (not via proxy) database: <name of the DB as specified above> User: <name of the admin user as specified in step above> Password: <password of the admin user as specified in step above> Jenkins as a Service For Jenkins as a Service the data source configuration is already available. Therefore no need to go through the data source configuration step unless you want to add addtional data sources.","title":"Work with InfluxDB and Grafana"},{"location":"steps/influxWriteData/#data-collected-in-influxdb","text":"The Influx plugin collects following data in the Piper context: All data as per default InfluxDB plugin capabilities Additional data collected via commonPipelineEnvironment.setInfluxCustomDataProperty() and via commonPipelineEnvironment.setPipelineMeasurement() Add custom information to your InfluxDB You can simply add custom data collected during your pipeline runs via available data objects. Example: //add data to measurement jenkins_custom_data - value can be a String or a Number commonPipelineEnvironment . setInfluxCustomDataProperty ( 'myProperty' , 2018 )","title":"Data collected in InfluxDB"},{"location":"steps/influxWriteData/#collected-influxdb-measurements","text":"Measurements are potentially pre-fixed - see parameter influxPrefix above. Measurement name data column description All measurements build_number project_name All below measurements will have these columns. Details see InfluxDB plugin documentation jenkins_data build_result build_time last_successful_build tests_failed tests_skipped tests_total ... Details see InfluxDB plugin documentation cobertura_data cobertura_branch_coverage_rate cobertura_class_coverage_rate cobertura_line_coverage_rate cobertura_package_coverage_rate ... Details see InfluxDB plugin documentation jacoco_data jacoco_branch_coverage_rate jacoco_class_coverage_rate jacoco_instruction_coverage_rate jacoco_line_coverage_rate jacoco_method_coverage_rate Details see InfluxDB plugin documentation performance_data 90Percentile average max median min error_count error_percent ... Details see InfluxDB plugin documentation sonarqube_data blocker_issues critical_issues info_issues major_issues minor_issues lines_of_code ... Details see InfluxDB plugin documentation jenkins_custom_data Piper fills following colums by default: build_result build_result_key build_step (->step in case of error) build_error (->error message in case of error) filled by commonPipelineEnvironment.setInfluxCustomDataProperty() pipeline_data Examples from the Piper templates: build_duration opa_duration deploy_test_duration deploy_test_duration fortify_duration release_duration ... filled by step measureDuration using parameter measurementName step_data Considered, e.g.: build_quality (Milestone/Release) build_url bats checkmarx fortify gauge nsp opa opensourcedependency ppms jmeter supa snyk sonar sourceclear uiveri5 vulas whitesource traceability ... xmakestage xmakepromote filled by commonPipelineEnvironment.setInfluxStepData()","title":"Collected InfluxDB measurements"},{"location":"steps/influxWriteData/#examples-for-influxdb-queries-which-can-be-used-in-grafana","text":"Project Names containing dashes (-) The InfluxDB plugin replaces dashes (-) with underscores (_). Please keep this in mind when specifying your project_name for a InfluxDB query.","title":"Examples for InfluxDB queries which can be used in Grafana"},{"location":"steps/influxWriteData/#example-1-select-last-10-successful-builds","text":"select top ( build_number , 10 ), build_result from jenkins_data WHERE build_result = 'SUCCESS'","title":"Example 1: Select last 10 successful builds"},{"location":"steps/influxWriteData/#example-2-select-last-10-step-names-of-failed-builds","text":"select top ( build_number , 10 ), build_result , build_step from jenkins_custom_data WHERE build_result = 'FAILURE'","title":"Example 2: Select last 10 step names of failed builds"},{"location":"steps/influxWriteData/#example-3-select-build-duration-of-step-for-a-specific-project","text":"select build_duration / 1000 from \"pipeline_data\" WHERE project_name = 'PiperTestOrg_piper_test_master'","title":"Example 3: Select build duration of step for a specific project"},{"location":"steps/influxWriteData/#example-4-get-transparency-about-successfulfailed-steps-for-a-specific-project","text":"select top ( build_number , 10 ) AS \"Build\" , build_url , build_quality , fortify , gauge , vulas , opa from step_data WHERE project_name = 'PiperTestOrg_piper_test_master' Note With this query you can create transparency about which steps ran successfully / not successfully in your pipeline and which ones were not executed at all. By specifying all the steps you consider relevant in your select statement it is very easy to create this transparency.","title":"Example 4: Get transparency about successful/failed steps for a specific project"},{"location":"steps/karmaExecuteTests/","text":"karmaExecuteTests \u00b6 Description \u00b6 In this step the ( Karma test runner ) is executed. The step is using the seleniumExecuteTest step to spins up two containers in a Docker network: a Selenium/Chrome container ( selenium/standalone-chrome ) a NodeJS container ( node:8-stretch ) In the Docker network, the containers can be referenced by the values provided in dockerName and sidecarName , the default values are karma and selenium . These values must be used in the hostname properties of the test configuration ( Karma and WebDriver ). Note In a Kubernetes environment, the containers both need to be referenced with localhost . Prerequisites \u00b6 running Karma tests - have a NPM module with running tests executed with Karma configured WebDriver - have the karma-webdriver-launcher package installed and a custom, WebDriver-based browser configured in Karma Parameters \u00b6 parameter mandatory default possible values script yes containerPortMappings no [node:8-stretch: [[containerPort: 9876, hostPort: 9876]]] dockerEnvVars no [ NO_PROXY: 'localhost,karma,$NO_PROXY', no_proxy: 'localhost,karma,$no_proxy'] dockerImage no node:8-stretch dockerName no karma dockerWorkspace no /home/node failOnError no installCommand no npm install --quiet modules no ['.'] runCommand no npm run karma sidecarEnvVars no [ NO_PROXY: 'localhost,selenium,$NO_PROXY', no_proxy: 'localhost,selenium,$no_proxy'] sidecarImage no sidecarName no sidecarVolumeBind no stashContent no ['buildDescriptor', 'tests'] script - defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. containerPortMappings - see step dockerExecute dockerEnvVars - see step dockerExecute dockerImage - see step dockerExecute dockerName - see step dockerExecute dockerWorkspace - see step dockerExecute failOnError - see step seleniumExecuteTests installCommand - the command that is executed to install dependencies modules - define the paths of the modules to execute tests on runCommand - the command that is executed to start the tests sidecarEnvVars - see step dockerExecute sidecarImage - see step dockerExecute sidecarName - see step dockerExecute sidecarVolumeBind - see step dockerExecute stashContent - pass specific stashed that should be considered for the tests Step configuration \u00b6 We recommend to define values of step parameters via config.yml file . In following sections the configuration is possible: parameter general step stage script containerPortMappings X X X dockerEnvVars X X X dockerImage X X X dockerName X X X dockerWorkspace X X X failOnError X X X installCommand X X X modules X X X runCommand X X X sidecarEnvVars X X X sidecarImage X X X sidecarName X X X sidecarVolumeBind X X X stashContent X X X Return value \u00b6 none Side effects \u00b6 Step uses seleniumExecuteTest & dockerExecute inside. Exceptions \u00b6 none Example \u00b6 karmaExecuteTests script: this , modules: [ './shoppinglist' , './catalog' ]","title":"karmaExecuteTests"},{"location":"steps/karmaExecuteTests/#karmaexecutetests","text":"","title":"karmaExecuteTests"},{"location":"steps/karmaExecuteTests/#description","text":"In this step the ( Karma test runner ) is executed. The step is using the seleniumExecuteTest step to spins up two containers in a Docker network: a Selenium/Chrome container ( selenium/standalone-chrome ) a NodeJS container ( node:8-stretch ) In the Docker network, the containers can be referenced by the values provided in dockerName and sidecarName , the default values are karma and selenium . These values must be used in the hostname properties of the test configuration ( Karma and WebDriver ). Note In a Kubernetes environment, the containers both need to be referenced with localhost .","title":"Description"},{"location":"steps/karmaExecuteTests/#prerequisites","text":"running Karma tests - have a NPM module with running tests executed with Karma configured WebDriver - have the karma-webdriver-launcher package installed and a custom, WebDriver-based browser configured in Karma","title":"Prerequisites"},{"location":"steps/karmaExecuteTests/#parameters","text":"parameter mandatory default possible values script yes containerPortMappings no [node:8-stretch: [[containerPort: 9876, hostPort: 9876]]] dockerEnvVars no [ NO_PROXY: 'localhost,karma,$NO_PROXY', no_proxy: 'localhost,karma,$no_proxy'] dockerImage no node:8-stretch dockerName no karma dockerWorkspace no /home/node failOnError no installCommand no npm install --quiet modules no ['.'] runCommand no npm run karma sidecarEnvVars no [ NO_PROXY: 'localhost,selenium,$NO_PROXY', no_proxy: 'localhost,selenium,$no_proxy'] sidecarImage no sidecarName no sidecarVolumeBind no stashContent no ['buildDescriptor', 'tests'] script - defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. containerPortMappings - see step dockerExecute dockerEnvVars - see step dockerExecute dockerImage - see step dockerExecute dockerName - see step dockerExecute dockerWorkspace - see step dockerExecute failOnError - see step seleniumExecuteTests installCommand - the command that is executed to install dependencies modules - define the paths of the modules to execute tests on runCommand - the command that is executed to start the tests sidecarEnvVars - see step dockerExecute sidecarImage - see step dockerExecute sidecarName - see step dockerExecute sidecarVolumeBind - see step dockerExecute stashContent - pass specific stashed that should be considered for the tests","title":"Parameters"},{"location":"steps/karmaExecuteTests/#step-configuration","text":"We recommend to define values of step parameters via config.yml file . In following sections the configuration is possible: parameter general step stage script containerPortMappings X X X dockerEnvVars X X X dockerImage X X X dockerName X X X dockerWorkspace X X X failOnError X X X installCommand X X X modules X X X runCommand X X X sidecarEnvVars X X X sidecarImage X X X sidecarName X X X sidecarVolumeBind X X X stashContent X X X","title":"Step configuration"},{"location":"steps/karmaExecuteTests/#return-value","text":"none","title":"Return value"},{"location":"steps/karmaExecuteTests/#side-effects","text":"Step uses seleniumExecuteTest & dockerExecute inside.","title":"Side effects"},{"location":"steps/karmaExecuteTests/#exceptions","text":"none","title":"Exceptions"},{"location":"steps/karmaExecuteTests/#example","text":"karmaExecuteTests script: this , modules: [ './shoppinglist' , './catalog' ]","title":"Example"},{"location":"steps/mailSendNotification/","text":"mailSendNotification \u00b6 Description \u00b6 Sends notifications to all potential culprits of a current or previous build failure plus to fixed list of recipients. It will attach the current build log to the email. Notifications are sent in following cases: current build failed or is unstable current build is successful and previous build failed or was unstable Prerequsites \u00b6 none Example \u00b6 Usage of pipeline step: mailSendNotification script: this Parameters \u00b6 parameter mandatory default possible values script yes buildResult no gitCommitId no script.commonPipelineEnvironment.getGitCommitId() gitSshKeyCredentialsId no `` gitUrl no notificationAttachment no true notificationRecipients no notifyCulprits no true numLogLinesInBody no 100 projectName no wrapInNode no false Details \u00b6 script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. buildResult may be used to overrule the build result coming from currentBuild.result . This is for example used in the step pipelineRestartSteps gitCommitId defines a dedicated git commitId for culprit retrieval. gitUrl and gitCommitId are used to retrieve culprit information. gitSshKeyCredentialsId only required if your git repository is protected. It defines the credentialsId for the git ssh credentials. notificationAttachment defines if the console log file should be attached to the notification mail. notificationRecipients defines the fixed list of recipient that always get the notification. In case you want to send the notification to the culprits only set it to an empty string '' . Note Multiple recipients need to be separated with the space character. In case you do not want to have any fixed recipients of the notifications leave the property empty. notifyCulprits defines if potential culprits should receive an email. numLogLinesInBody defines the number of log lines (=last lines of the log) which are included into the body of the notification email. projectName may be used to specify a different name in the email subject. wrapInNode needs to be set to true if step is used outside of a node context, e.g. post actions in a declarative pipeline script. Step configuration \u00b6 We recommend to define values of step parameters via config.yml file . In following sections the configuration is possible: parameter general step stage script buildResult X X gitCommitId X X gitSshKeyCredentialsId X X X gitUrl X X notificationAttachment X X notificationRecipients X X notifyCulprits X X numLogLinesInBody X X projectName X X wrapInNode X X Return value \u00b6 none Side effects \u00b6 none Exceptions \u00b6 none","title":"mailSendNotification"},{"location":"steps/mailSendNotification/#mailsendnotification","text":"","title":"mailSendNotification"},{"location":"steps/mailSendNotification/#description","text":"Sends notifications to all potential culprits of a current or previous build failure plus to fixed list of recipients. It will attach the current build log to the email. Notifications are sent in following cases: current build failed or is unstable current build is successful and previous build failed or was unstable","title":"Description"},{"location":"steps/mailSendNotification/#prerequsites","text":"none","title":"Prerequsites"},{"location":"steps/mailSendNotification/#example","text":"Usage of pipeline step: mailSendNotification script: this","title":"Example"},{"location":"steps/mailSendNotification/#parameters","text":"parameter mandatory default possible values script yes buildResult no gitCommitId no script.commonPipelineEnvironment.getGitCommitId() gitSshKeyCredentialsId no `` gitUrl no notificationAttachment no true notificationRecipients no notifyCulprits no true numLogLinesInBody no 100 projectName no wrapInNode no false","title":"Parameters"},{"location":"steps/mailSendNotification/#details","text":"script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. buildResult may be used to overrule the build result coming from currentBuild.result . This is for example used in the step pipelineRestartSteps gitCommitId defines a dedicated git commitId for culprit retrieval. gitUrl and gitCommitId are used to retrieve culprit information. gitSshKeyCredentialsId only required if your git repository is protected. It defines the credentialsId for the git ssh credentials. notificationAttachment defines if the console log file should be attached to the notification mail. notificationRecipients defines the fixed list of recipient that always get the notification. In case you want to send the notification to the culprits only set it to an empty string '' . Note Multiple recipients need to be separated with the space character. In case you do not want to have any fixed recipients of the notifications leave the property empty. notifyCulprits defines if potential culprits should receive an email. numLogLinesInBody defines the number of log lines (=last lines of the log) which are included into the body of the notification email. projectName may be used to specify a different name in the email subject. wrapInNode needs to be set to true if step is used outside of a node context, e.g. post actions in a declarative pipeline script.","title":"Details"},{"location":"steps/mailSendNotification/#step-configuration","text":"We recommend to define values of step parameters via config.yml file . In following sections the configuration is possible: parameter general step stage script buildResult X X gitCommitId X X gitSshKeyCredentialsId X X X gitUrl X X notificationAttachment X X notificationRecipients X X notifyCulprits X X numLogLinesInBody X X projectName X X wrapInNode X X","title":"Step configuration"},{"location":"steps/mailSendNotification/#return-value","text":"none","title":"Return value"},{"location":"steps/mailSendNotification/#side-effects","text":"none","title":"Side effects"},{"location":"steps/mailSendNotification/#exceptions","text":"none","title":"Exceptions"},{"location":"steps/mavenExecute/","text":"mavenExecute \u00b6 Description \u00b6 Executes a maven command inside a Docker container. Parameters \u00b6 parameter mandatory default example values script yes dockerImage no 'maven:3.5-jdk-7' globalSettingsFile no 'local_folder/settings.xml' projectSettingsFile no pomPath no 'local_folder/m2' flags no '-o' goals no 'clean install' m2Path no 'local_folder/m2' defines no '-Dmaven.tests.skip=true' logSuccessfulMavenTransfers no false 'true' script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for retrieving, for example, configuration parameters. dockerImage Name of the docker image that should be used. globalSettingsFile Path or url to the mvn settings file that should be used as global settings file. projectSettingsFile Path or url to the mvn settings file that should be used as project settings file. pomPath Path to the pom file that should be used. flags Flags to provide when running mvn. goals Maven goals that should be executed. m2Path Path to the location of the local repository that should be used. defines Additional properties. logSuccessfulMavenTransfers configures maven to log successful downloads. This is set to false by default to reduce the noise in build logs. Step configuration \u00b6 The following parameters can also be specified as step parameters using the global configuration file: dockerImage globalSettingsFile projectSettingsFile pomPath m2Path Exceptions \u00b6 None Example \u00b6 mavenExecute script: this , goals: 'clean install'","title":"mavenExecute"},{"location":"steps/mavenExecute/#mavenexecute","text":"","title":"mavenExecute"},{"location":"steps/mavenExecute/#description","text":"Executes a maven command inside a Docker container.","title":"Description"},{"location":"steps/mavenExecute/#parameters","text":"parameter mandatory default example values script yes dockerImage no 'maven:3.5-jdk-7' globalSettingsFile no 'local_folder/settings.xml' projectSettingsFile no pomPath no 'local_folder/m2' flags no '-o' goals no 'clean install' m2Path no 'local_folder/m2' defines no '-Dmaven.tests.skip=true' logSuccessfulMavenTransfers no false 'true' script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for retrieving, for example, configuration parameters. dockerImage Name of the docker image that should be used. globalSettingsFile Path or url to the mvn settings file that should be used as global settings file. projectSettingsFile Path or url to the mvn settings file that should be used as project settings file. pomPath Path to the pom file that should be used. flags Flags to provide when running mvn. goals Maven goals that should be executed. m2Path Path to the location of the local repository that should be used. defines Additional properties. logSuccessfulMavenTransfers configures maven to log successful downloads. This is set to false by default to reduce the noise in build logs.","title":"Parameters"},{"location":"steps/mavenExecute/#step-configuration","text":"The following parameters can also be specified as step parameters using the global configuration file: dockerImage globalSettingsFile projectSettingsFile pomPath m2Path","title":"Step configuration"},{"location":"steps/mavenExecute/#exceptions","text":"None","title":"Exceptions"},{"location":"steps/mavenExecute/#example","text":"mavenExecute script: this , goals: 'clean install'","title":"Example"},{"location":"steps/mtaBuild/","text":"mtaBuild \u00b6 Description \u00b6 Executes the SAP Multitarget Application Archive Builder to create an mtar archive of the application. Before doing this, validates that SAP Multitarget Application Archive Builder exists and the version is compatible. Note that a version is formed by major.minor.patch , and a version is compatible to another version if the minor and patch versions are higher, but the major version is not, e.g. if 3.39.10 is the expected version, 3.39.11 and 3.40.1 would be compatible versions, but 4.0.1 would not be a compatible version. Prerequisites \u00b6 A docker image meeting the following requirements SAP MTA Archive Builder 1.0.6 or compatible version - can be downloaded from SAP Development Tools . Java 8 or compatible version - necessary to run the mta.jar file. NodeJS installed - the MTA Builder uses npm to download node module dependencies such as grunt . Parameters \u00b6 parameter mandatory default possible values script yes dockerImage yes dockerOptions no '' buildTarget yes 'NEO' 'CF', 'NEO', 'XSA' extension no mtaJarLocation no 'mta.jar' applicationName no script - The common script environment of the Jenkinsfile running. Typically the reference to the script calling the pipeline step is provided with the this parameter, as in script: this . This allows the function to access the commonPipelineEnvironment for retrieving, for example, configuration parameters. dockerImage - The Docker image to execute the MTA build. A custom built image needs to include Multi-target Application Archive Builder. Refer to SAP Help Portal for information on how to set it up. dockerOptions Docker options to be set when starting the container. It can be a list or a string. buildTarget - The target platform to which the mtar can be deployed. extension - The path to the extension descriptor file. mtaJarLocation - The location of the SAP Multitarget Application Archive Builder jar file, including file name and extension. First, the location is retrieved from the environment variables using the environment variable MTA_JAR_LOCATION . If no environment variable is provided, the location is retrieved from the parameters, or the step configuration using the key mtaJarLocation . If SAP Multitarget Application Archive Builder is not found on one of the previous locations an AbortException is thrown. Note that the environment variable MTA_JAR_LOCATION has priority. In case that the script runs on multiple nodes, SAP Multitarget Application Archive Builder must be located on all the nodes, therefore the environment variable must be also configured on all the nodes. applicationName - The name of the application which is being built. If the parameter has been provided and no mta.yaml exists, the mta.yaml will be automatically generated using this parameter and the information ( name and version ) from package.json before the actual build starts. Step configuration \u00b6 The following parameters can also be specified as step parameters using the global configuration file: dockerImage buildTarget extension mtaJarLocation applicationName Return value \u00b6 none Side effects \u00b6 The file name of the resulting archive is written to the commonPipelineEnvironment with variable name mtarFileName . Exceptions \u00b6 AbortException : If SAP Multitarget Application Archive Builder is not found. If there is an invalid buildTarget . If there is no key ID inside the mta.yaml file. Example \u00b6 def mtarFileName dir ( '/path/to/FioriApp' ){ mtarFileName = mtaBuild script: this , buildTarget: 'NEO' }","title":"mtaBuild"},{"location":"steps/mtaBuild/#mtabuild","text":"","title":"mtaBuild"},{"location":"steps/mtaBuild/#description","text":"Executes the SAP Multitarget Application Archive Builder to create an mtar archive of the application. Before doing this, validates that SAP Multitarget Application Archive Builder exists and the version is compatible. Note that a version is formed by major.minor.patch , and a version is compatible to another version if the minor and patch versions are higher, but the major version is not, e.g. if 3.39.10 is the expected version, 3.39.11 and 3.40.1 would be compatible versions, but 4.0.1 would not be a compatible version.","title":"Description"},{"location":"steps/mtaBuild/#prerequisites","text":"A docker image meeting the following requirements SAP MTA Archive Builder 1.0.6 or compatible version - can be downloaded from SAP Development Tools . Java 8 or compatible version - necessary to run the mta.jar file. NodeJS installed - the MTA Builder uses npm to download node module dependencies such as grunt .","title":"Prerequisites"},{"location":"steps/mtaBuild/#parameters","text":"parameter mandatory default possible values script yes dockerImage yes dockerOptions no '' buildTarget yes 'NEO' 'CF', 'NEO', 'XSA' extension no mtaJarLocation no 'mta.jar' applicationName no script - The common script environment of the Jenkinsfile running. Typically the reference to the script calling the pipeline step is provided with the this parameter, as in script: this . This allows the function to access the commonPipelineEnvironment for retrieving, for example, configuration parameters. dockerImage - The Docker image to execute the MTA build. A custom built image needs to include Multi-target Application Archive Builder. Refer to SAP Help Portal for information on how to set it up. dockerOptions Docker options to be set when starting the container. It can be a list or a string. buildTarget - The target platform to which the mtar can be deployed. extension - The path to the extension descriptor file. mtaJarLocation - The location of the SAP Multitarget Application Archive Builder jar file, including file name and extension. First, the location is retrieved from the environment variables using the environment variable MTA_JAR_LOCATION . If no environment variable is provided, the location is retrieved from the parameters, or the step configuration using the key mtaJarLocation . If SAP Multitarget Application Archive Builder is not found on one of the previous locations an AbortException is thrown. Note that the environment variable MTA_JAR_LOCATION has priority. In case that the script runs on multiple nodes, SAP Multitarget Application Archive Builder must be located on all the nodes, therefore the environment variable must be also configured on all the nodes. applicationName - The name of the application which is being built. If the parameter has been provided and no mta.yaml exists, the mta.yaml will be automatically generated using this parameter and the information ( name and version ) from package.json before the actual build starts.","title":"Parameters"},{"location":"steps/mtaBuild/#step-configuration","text":"The following parameters can also be specified as step parameters using the global configuration file: dockerImage buildTarget extension mtaJarLocation applicationName","title":"Step configuration"},{"location":"steps/mtaBuild/#return-value","text":"none","title":"Return value"},{"location":"steps/mtaBuild/#side-effects","text":"The file name of the resulting archive is written to the commonPipelineEnvironment with variable name mtarFileName .","title":"Side effects"},{"location":"steps/mtaBuild/#exceptions","text":"AbortException : If SAP Multitarget Application Archive Builder is not found. If there is an invalid buildTarget . If there is no key ID inside the mta.yaml file.","title":"Exceptions"},{"location":"steps/mtaBuild/#example","text":"def mtarFileName dir ( '/path/to/FioriApp' ){ mtarFileName = mtaBuild script: this , buildTarget: 'NEO' }","title":"Example"},{"location":"steps/neoDeploy/","text":"neoDeploy \u00b6 Description \u00b6 Deploys an Application to SAP Cloud Platform (SAP CP) using the SAP Cloud Platform Console Client (Neo Java Web SDK). Before doing this, validates that SAP Cloud Platform Console Client is installed and the version is compatible. Note that a version is formed by major.minor.patch , and a version is compatible to another version if the minor and patch versions are higher, but the major version is not, e.g. if 3.39.10 is the expected version, 3.39.11 and 3.40.1 would be compatible versions, but 4.0.1 would not be a compatible version. Prerequisites \u00b6 SAP CP account - the account to where the application is deployed. SAP CP user for deployment - a user with deployment permissions in the given account. Jenkins credentials for deployment - must be configured in Jenkins credentials with a dedicated Id. Neo Java Web SDK 3.39.10 or compatible version - can be downloaded from Maven Central . The Neo Java Web SDK needs to be extracted into the folder provided by neoHome . In case this parameters is not provided and there is no NEO_HOME parameter in the environment <neoRoot>/tools needs to be in the PATH . This step is also capable of triggering the neo deploy tool provided inside a docker image. Java 8 or compatible version - needed by the Neo-Java-Web-SDK Parameters when using MTA deployment method (default - MTA) \u00b6 parameter mandatory default possible values account no archivePath no deployAccount deprecated, use account no deployHost deprecated, use host no deployMode yes 'mta' 'mta' , 'warParams' , 'warPropertiesFile' host no neoCredentialsId no 'CI_CREDENTIALS_ID' neoHome no script yes Parameters when using WAR file deployment method with .properties file (WAR_PROPERTIESFILE) \u00b6 parameter mandatory default possible values archivePath no deployMode yes 'mta' 'mta' , 'warParams' , 'warPropertiesFile' neoCredentialsId no 'CI_CREDENTIALS_ID' neoHome no propertiesFile yes script yes warAction yes 'deploy' 'deploy' , 'rolling-update' Parameters when using WAR file deployment method witout .properties file - with parameters (WAR_PARAMS) \u00b6 parameter mandatory default possible values account no applicationName yes archivePath no deployAccount deprecated, use account no deployHost deprecated, use host no deployMode yes 'mta' 'mta' , 'warParams' , 'warPropertiesFile' host no neoCredentialsId no 'CI_CREDENTIALS_ID' neoHome no runtime yes runtime-version yes script yes vmSize no 'lite' 'lite' , 'pro' , 'prem' , 'prem-plus' warAction yes 'deploy' 'deploy' , 'rolling-update' account - The SAP Cloud Platform account to deploy to. applicationName - Name of the application you want to manage, configure, or deploy archivePath - The path to the archive for deployment to SAP CP. If not provided mtarFilePath from commom pipeline environment is used instead. deployAccount - deprecated, use account . The SAP Cloud Platform account to deploy to. deployHost - deprecated, use host . The SAP Cloud Platform host to deploy to. deployMode - The deployment mode which should be used. Available options are 'mta' (default), 'warParams' (deploying WAR file and passing all the deployment parameters via the function call) and 'warPropertiesFile' (deploying WAR file and putting all the deployment parameters in a .properties file) host - The SAP Cloud Platform host to deploy to. neoCredentialsId - The Jenkins credentials containing user and password used for SAP CP deployment. neoHome - The path to the neo-java-web-sdk tool used for SAP CP deployment. If no parameter is provided, the path is retrieved from the environment variables using the environment variable NEO_HOME . If no parameter and no environment variable is provided, the path is retrieved from the step configuration using the step configuration key neoHome . If the previous configurations are not provided, the tool is expected on the PATH , and if it is not available on the PATH an AbortException is thrown. propertiesFile - The path to the .properties file in which all necessary deployment properties for the application are defined. runtime - Name of SAP Cloud Platform application runtime runtime-version - Version of SAP Cloud Platform application runtime script - The common script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for retrieving e.g. configuration parameters. vmSize - Compute unit (VM) size. Acceptable values: lite, pro, prem, prem-plus. warAction - Action mode when using WAR file mode. Available options are deploy (default) and rolling-update which performs update of an application without downtime in one go. The step is prepared for being executed in docker. The corresponding parameters can be applied. See step dockerExecute for details. Step configuration \u00b6 The following parameters can also be specified as step parameters using the global configuration file: account dockerEnvVars dockerImage dockerOptions host neoCredentialsId neoHome Return value \u00b6 none Side effects \u00b6 none Exceptions \u00b6 Exception : If archivePath is not provided. If propertiesFile is not provided (when using 'WAR_PROPERTIESFILE' deployment mode). If applicationName is not provided (when using 'WAR_PARAMS' deployment mode). If runtime is not provided (when using 'WAR_PARAMS' deployment mode). If runtime-version is not provided (when using 'WAR_PARAMS' deployment mode). AbortException : If neo-java-web-sdk is not installed, or neoHome is wrong. If deployHost is wrong. If deployAccount is wrong. CredentialNotFoundException : If the credentials cannot be resolved. Example \u00b6 neoDeploy script: this , archivePath: 'path/to/archiveFile.mtar' , credentialsId: 'my-credentials-id' Example configuration: steps : <...> neoDeploy : account : <myDeployAccount> host : hana.example.org","title":"neoDeploy"},{"location":"steps/neoDeploy/#neodeploy","text":"","title":"neoDeploy"},{"location":"steps/neoDeploy/#description","text":"Deploys an Application to SAP Cloud Platform (SAP CP) using the SAP Cloud Platform Console Client (Neo Java Web SDK). Before doing this, validates that SAP Cloud Platform Console Client is installed and the version is compatible. Note that a version is formed by major.minor.patch , and a version is compatible to another version if the minor and patch versions are higher, but the major version is not, e.g. if 3.39.10 is the expected version, 3.39.11 and 3.40.1 would be compatible versions, but 4.0.1 would not be a compatible version.","title":"Description"},{"location":"steps/neoDeploy/#prerequisites","text":"SAP CP account - the account to where the application is deployed. SAP CP user for deployment - a user with deployment permissions in the given account. Jenkins credentials for deployment - must be configured in Jenkins credentials with a dedicated Id. Neo Java Web SDK 3.39.10 or compatible version - can be downloaded from Maven Central . The Neo Java Web SDK needs to be extracted into the folder provided by neoHome . In case this parameters is not provided and there is no NEO_HOME parameter in the environment <neoRoot>/tools needs to be in the PATH . This step is also capable of triggering the neo deploy tool provided inside a docker image. Java 8 or compatible version - needed by the Neo-Java-Web-SDK","title":"Prerequisites"},{"location":"steps/neoDeploy/#parameters-when-using-mta-deployment-method-default-mta","text":"parameter mandatory default possible values account no archivePath no deployAccount deprecated, use account no deployHost deprecated, use host no deployMode yes 'mta' 'mta' , 'warParams' , 'warPropertiesFile' host no neoCredentialsId no 'CI_CREDENTIALS_ID' neoHome no script yes","title":"Parameters when using MTA deployment method (default - MTA)"},{"location":"steps/neoDeploy/#parameters-when-using-war-file-deployment-method-with-properties-file-war_propertiesfile","text":"parameter mandatory default possible values archivePath no deployMode yes 'mta' 'mta' , 'warParams' , 'warPropertiesFile' neoCredentialsId no 'CI_CREDENTIALS_ID' neoHome no propertiesFile yes script yes warAction yes 'deploy' 'deploy' , 'rolling-update'","title":"Parameters when using WAR file deployment method with .properties file (WAR_PROPERTIESFILE)"},{"location":"steps/neoDeploy/#parameters-when-using-war-file-deployment-method-witout-properties-file-with-parameters-war_params","text":"parameter mandatory default possible values account no applicationName yes archivePath no deployAccount deprecated, use account no deployHost deprecated, use host no deployMode yes 'mta' 'mta' , 'warParams' , 'warPropertiesFile' host no neoCredentialsId no 'CI_CREDENTIALS_ID' neoHome no runtime yes runtime-version yes script yes vmSize no 'lite' 'lite' , 'pro' , 'prem' , 'prem-plus' warAction yes 'deploy' 'deploy' , 'rolling-update' account - The SAP Cloud Platform account to deploy to. applicationName - Name of the application you want to manage, configure, or deploy archivePath - The path to the archive for deployment to SAP CP. If not provided mtarFilePath from commom pipeline environment is used instead. deployAccount - deprecated, use account . The SAP Cloud Platform account to deploy to. deployHost - deprecated, use host . The SAP Cloud Platform host to deploy to. deployMode - The deployment mode which should be used. Available options are 'mta' (default), 'warParams' (deploying WAR file and passing all the deployment parameters via the function call) and 'warPropertiesFile' (deploying WAR file and putting all the deployment parameters in a .properties file) host - The SAP Cloud Platform host to deploy to. neoCredentialsId - The Jenkins credentials containing user and password used for SAP CP deployment. neoHome - The path to the neo-java-web-sdk tool used for SAP CP deployment. If no parameter is provided, the path is retrieved from the environment variables using the environment variable NEO_HOME . If no parameter and no environment variable is provided, the path is retrieved from the step configuration using the step configuration key neoHome . If the previous configurations are not provided, the tool is expected on the PATH , and if it is not available on the PATH an AbortException is thrown. propertiesFile - The path to the .properties file in which all necessary deployment properties for the application are defined. runtime - Name of SAP Cloud Platform application runtime runtime-version - Version of SAP Cloud Platform application runtime script - The common script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for retrieving e.g. configuration parameters. vmSize - Compute unit (VM) size. Acceptable values: lite, pro, prem, prem-plus. warAction - Action mode when using WAR file mode. Available options are deploy (default) and rolling-update which performs update of an application without downtime in one go. The step is prepared for being executed in docker. The corresponding parameters can be applied. See step dockerExecute for details.","title":"Parameters when using WAR file deployment method witout .properties file - with parameters (WAR_PARAMS)"},{"location":"steps/neoDeploy/#step-configuration","text":"The following parameters can also be specified as step parameters using the global configuration file: account dockerEnvVars dockerImage dockerOptions host neoCredentialsId neoHome","title":"Step configuration"},{"location":"steps/neoDeploy/#return-value","text":"none","title":"Return value"},{"location":"steps/neoDeploy/#side-effects","text":"none","title":"Side effects"},{"location":"steps/neoDeploy/#exceptions","text":"Exception : If archivePath is not provided. If propertiesFile is not provided (when using 'WAR_PROPERTIESFILE' deployment mode). If applicationName is not provided (when using 'WAR_PARAMS' deployment mode). If runtime is not provided (when using 'WAR_PARAMS' deployment mode). If runtime-version is not provided (when using 'WAR_PARAMS' deployment mode). AbortException : If neo-java-web-sdk is not installed, or neoHome is wrong. If deployHost is wrong. If deployAccount is wrong. CredentialNotFoundException : If the credentials cannot be resolved.","title":"Exceptions"},{"location":"steps/neoDeploy/#example","text":"neoDeploy script: this , archivePath: 'path/to/archiveFile.mtar' , credentialsId: 'my-credentials-id' Example configuration: steps : <...> neoDeploy : account : <myDeployAccount> host : hana.example.org","title":"Example"},{"location":"steps/pipelineExecute/","text":"pipelineExecute \u00b6 Description \u00b6 Loads a pipeline from a git repository. The idea is to set up a pipeline job in Jenkins that loads a minimal pipeline, which in turn loads the shared library and then uses this step to load the actual pipeline. A centrally maintained pipeline script (Jenkinsfile) can be re-used by several projects using pipelineExecute as outlined in the example below. Prerequisites \u00b6 none Parameters \u00b6 parameter mandatory default possible values repoUrl yes branch no 'master' path no 'Jenkinsfile' credentialsId no An empty String repoUrl The url to the git repository of the pipeline to be loaded. branch The branch of the git repository from which the pipeline should be checked out. path The path to the Jenkinsfile, inside the repository, to be loaded. credentialsId The Jenkins credentials containing user and password needed to access a private git repository. Step configuration \u00b6 none Return value \u00b6 none Side effects \u00b6 none Exceptions \u00b6 Exception If repoUrl is not provided. Example \u00b6 pipelineExecute repoUrl: \"https://github.com/MyOrg/MyPipelineRepo.git\" , branch: 'feature1' , path: 'path/to/Jenkinsfile' , credentialsId: 'MY_REPO_CREDENTIALS'","title":"pipelineExecute"},{"location":"steps/pipelineExecute/#pipelineexecute","text":"","title":"pipelineExecute"},{"location":"steps/pipelineExecute/#description","text":"Loads a pipeline from a git repository. The idea is to set up a pipeline job in Jenkins that loads a minimal pipeline, which in turn loads the shared library and then uses this step to load the actual pipeline. A centrally maintained pipeline script (Jenkinsfile) can be re-used by several projects using pipelineExecute as outlined in the example below.","title":"Description"},{"location":"steps/pipelineExecute/#prerequisites","text":"none","title":"Prerequisites"},{"location":"steps/pipelineExecute/#parameters","text":"parameter mandatory default possible values repoUrl yes branch no 'master' path no 'Jenkinsfile' credentialsId no An empty String repoUrl The url to the git repository of the pipeline to be loaded. branch The branch of the git repository from which the pipeline should be checked out. path The path to the Jenkinsfile, inside the repository, to be loaded. credentialsId The Jenkins credentials containing user and password needed to access a private git repository.","title":"Parameters"},{"location":"steps/pipelineExecute/#step-configuration","text":"none","title":"Step configuration"},{"location":"steps/pipelineExecute/#return-value","text":"none","title":"Return value"},{"location":"steps/pipelineExecute/#side-effects","text":"none","title":"Side effects"},{"location":"steps/pipelineExecute/#exceptions","text":"Exception If repoUrl is not provided.","title":"Exceptions"},{"location":"steps/pipelineExecute/#example","text":"pipelineExecute repoUrl: \"https://github.com/MyOrg/MyPipelineRepo.git\" , branch: 'feature1' , path: 'path/to/Jenkinsfile' , credentialsId: 'MY_REPO_CREDENTIALS'","title":"Example"},{"location":"steps/pipelineRestartSteps/","text":"pipelineRestartSteps \u00b6 Description \u00b6 Support of restarting failed stages or steps in a pipeline is limited in Jenkins. This has been documented in the Jenkins Jira issue JENKINS-33846 . For declarative pipelines there is a solution available which partially addresses this topic: https://jenkins.io/doc/book/pipeline/running-pipelines/#restart-from-a-stage. Nonetheless, still features are missing, so it can't be used in all cases. The more complex Piper pipelines which share a state via commonPipelineEnvironment will for example not work with the standard restart-from-stage . The step pipelineRestartSteps aims to address this gap and allows individual parts of a pipeline (e.g. a failed deployment) to be restarted. This is done in a way that the pipeline waits for user input to restart the pipeline in case of a failure. In case this user input is not provided the pipeline stops after a timeout which can be configured. Prerequisites \u00b6 none Example \u00b6 Usage of pipeline step: pipelineRestartSteps ( script: this ) { node { //your steps ... } } Caution Use node inside the step. If a node exists outside the step context, the input step which is triggered in the process will block a Jenkins executor. In case you cannot use node inside this step, please choose the parameter timeoutInSeconds carefully! Parameters \u00b6 parameter mandatory default possible values script yes sendMail no true timeoutInSeconds no 900 Details \u00b6 script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. If sendMail: true the step mailSendNotification will be triggered in case of an error timeoutInSeconds defines the time period where the job waits for input. Default is 15 minutes. Once this time is passed the job enters state FAILED. Step configuration \u00b6 We recommend to define values of step parameters via config.yml file . In following sections the configuration is possible: parameter general step stage script sendMail X X X timeoutInSeconds X X X Return value \u00b6 none Side effects \u00b6 none Exceptions \u00b6 none","title":"pipelineRestartSteps"},{"location":"steps/pipelineRestartSteps/#pipelinerestartsteps","text":"","title":"pipelineRestartSteps"},{"location":"steps/pipelineRestartSteps/#description","text":"Support of restarting failed stages or steps in a pipeline is limited in Jenkins. This has been documented in the Jenkins Jira issue JENKINS-33846 . For declarative pipelines there is a solution available which partially addresses this topic: https://jenkins.io/doc/book/pipeline/running-pipelines/#restart-from-a-stage. Nonetheless, still features are missing, so it can't be used in all cases. The more complex Piper pipelines which share a state via commonPipelineEnvironment will for example not work with the standard restart-from-stage . The step pipelineRestartSteps aims to address this gap and allows individual parts of a pipeline (e.g. a failed deployment) to be restarted. This is done in a way that the pipeline waits for user input to restart the pipeline in case of a failure. In case this user input is not provided the pipeline stops after a timeout which can be configured.","title":"Description"},{"location":"steps/pipelineRestartSteps/#prerequisites","text":"none","title":"Prerequisites"},{"location":"steps/pipelineRestartSteps/#example","text":"Usage of pipeline step: pipelineRestartSteps ( script: this ) { node { //your steps ... } } Caution Use node inside the step. If a node exists outside the step context, the input step which is triggered in the process will block a Jenkins executor. In case you cannot use node inside this step, please choose the parameter timeoutInSeconds carefully!","title":"Example"},{"location":"steps/pipelineRestartSteps/#parameters","text":"parameter mandatory default possible values script yes sendMail no true timeoutInSeconds no 900","title":"Parameters"},{"location":"steps/pipelineRestartSteps/#details","text":"script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. If sendMail: true the step mailSendNotification will be triggered in case of an error timeoutInSeconds defines the time period where the job waits for input. Default is 15 minutes. Once this time is passed the job enters state FAILED.","title":"Details"},{"location":"steps/pipelineRestartSteps/#step-configuration","text":"We recommend to define values of step parameters via config.yml file . In following sections the configuration is possible: parameter general step stage script sendMail X X X timeoutInSeconds X X X","title":"Step configuration"},{"location":"steps/pipelineRestartSteps/#return-value","text":"none","title":"Return value"},{"location":"steps/pipelineRestartSteps/#side-effects","text":"none","title":"Side effects"},{"location":"steps/pipelineRestartSteps/#exceptions","text":"none","title":"Exceptions"},{"location":"steps/pipelineStashFiles/","text":"pipelineStashFiles \u00b6 Description \u00b6 This step stashes files that are needed in other build steps (on other nodes). Prerequsites \u00b6 none Parameters \u00b6 parameter mandatory default possible values script yes runCheckmarx no false runOpaTests no false stashIncludes no see details stashExcludes no see details Details: The step is stashing files before and after the build. This is due to the fact, that some of the code that needs to be stashed, is generated during the build (TypeScript for NPM). stash name mandatory prerequisite pattern buildDescriptor no includes: **/pom.xml, **/.mvn/**, **/assembly.xml, **/.swagger-codegen-ignore, **/package.json, **/requirements.txt, **/setup.py, **/whitesource_config.py, **/mta*.y*ml, **/.npmrc, **/whitesource.*.json, **/whitesource-fs-agent.config, Dockerfile, **/VERSION, **/version.txt, **/build.sbt, **/sbtDescriptor.json, **/project/* excludes: **/node_modules/**/package.json checkmarx no Checkmarx is enabled includes: **/*.js, **/*.scala, **/*.go excludes: **/*.mockserver.js, node_modules/**/*.js classFiles no includes: **/target/classes/**/*.class, **/target/test-classes/**/*.class excludes: '' deployDescriptor no includes: **/manifest*.y*ml, **/*.mtaext.y*ml, **/*.mtaext, **/xs-app.json, helm/**, *.y*ml exclude: '' git no includes: **/gitmetadata/** exludes: '' opa5 no OPA5 is enabled includes: **/*.* excludes: '' opensourceConfiguration no includes: **/srcclr.yml, **/vulas-custom.properties, **/.nsprc, **/.retireignore, **/.retireignore.json, **/.snyk excludes: '' pipelineConfigAndTests no includes: .pipeline/*.* excludes: '' securityDescriptor no includes: **/xs-security.json exludes: '' sonar no includes: **/jacoco*.exec, **/sonar-project.properties exludes: '' tests no includes: **/pom.xml, **/*.json, **/*.xml, **/src/**, **/node_modules/**, **/specs/**, **/env/**, **/*.js excludes: '' Overwriting default stashing behavior It is possible to overwrite the default behavior of the stashes using the parameters stashIncludes and stashExcludes , e.g. stashIncludes: [buildDescriptor: '**/mybuild.yml] stashExcludes: [tests: '**/NOTRELEVANT.*] Step configuration \u00b6 The following parameters can also be specified as step parameters using the global configuration file: runOpaTests runCheckmarx stashExcludes stashIncludes Explanation of pipeline step \u00b6 Usage of pipeline step: pipelineStashFiles script: this { mavenExecute script: this , ... }","title":"pipelineStashFiles"},{"location":"steps/pipelineStashFiles/#pipelinestashfiles","text":"","title":"pipelineStashFiles"},{"location":"steps/pipelineStashFiles/#description","text":"This step stashes files that are needed in other build steps (on other nodes).","title":"Description"},{"location":"steps/pipelineStashFiles/#prerequsites","text":"none","title":"Prerequsites"},{"location":"steps/pipelineStashFiles/#parameters","text":"parameter mandatory default possible values script yes runCheckmarx no false runOpaTests no false stashIncludes no see details stashExcludes no see details Details: The step is stashing files before and after the build. This is due to the fact, that some of the code that needs to be stashed, is generated during the build (TypeScript for NPM). stash name mandatory prerequisite pattern buildDescriptor no includes: **/pom.xml, **/.mvn/**, **/assembly.xml, **/.swagger-codegen-ignore, **/package.json, **/requirements.txt, **/setup.py, **/whitesource_config.py, **/mta*.y*ml, **/.npmrc, **/whitesource.*.json, **/whitesource-fs-agent.config, Dockerfile, **/VERSION, **/version.txt, **/build.sbt, **/sbtDescriptor.json, **/project/* excludes: **/node_modules/**/package.json checkmarx no Checkmarx is enabled includes: **/*.js, **/*.scala, **/*.go excludes: **/*.mockserver.js, node_modules/**/*.js classFiles no includes: **/target/classes/**/*.class, **/target/test-classes/**/*.class excludes: '' deployDescriptor no includes: **/manifest*.y*ml, **/*.mtaext.y*ml, **/*.mtaext, **/xs-app.json, helm/**, *.y*ml exclude: '' git no includes: **/gitmetadata/** exludes: '' opa5 no OPA5 is enabled includes: **/*.* excludes: '' opensourceConfiguration no includes: **/srcclr.yml, **/vulas-custom.properties, **/.nsprc, **/.retireignore, **/.retireignore.json, **/.snyk excludes: '' pipelineConfigAndTests no includes: .pipeline/*.* excludes: '' securityDescriptor no includes: **/xs-security.json exludes: '' sonar no includes: **/jacoco*.exec, **/sonar-project.properties exludes: '' tests no includes: **/pom.xml, **/*.json, **/*.xml, **/src/**, **/node_modules/**, **/specs/**, **/env/**, **/*.js excludes: '' Overwriting default stashing behavior It is possible to overwrite the default behavior of the stashes using the parameters stashIncludes and stashExcludes , e.g. stashIncludes: [buildDescriptor: '**/mybuild.yml] stashExcludes: [tests: '**/NOTRELEVANT.*]","title":"Parameters"},{"location":"steps/pipelineStashFiles/#step-configuration","text":"The following parameters can also be specified as step parameters using the global configuration file: runOpaTests runCheckmarx stashExcludes stashIncludes","title":"Step configuration"},{"location":"steps/pipelineStashFiles/#explanation-of-pipeline-step","text":"Usage of pipeline step: pipelineStashFiles script: this { mavenExecute script: this , ... }","title":"Explanation of pipeline step"},{"location":"steps/prepareDefaultValues/","text":"prepareDefaultValues \u00b6 Description \u00b6 Loads the pipeline library default values from the file resources/default_pipeline_environment.yml . Afterwards the values can be loaded by the method: ConfigurationLoader.defaultStepConfiguration Parameters \u00b6 None Step configuration \u00b6 None Exceptions \u00b6 None Example \u00b6 prepareDefaultValues ()","title":"prepareDefaultValues"},{"location":"steps/prepareDefaultValues/#preparedefaultvalues","text":"","title":"prepareDefaultValues"},{"location":"steps/prepareDefaultValues/#description","text":"Loads the pipeline library default values from the file resources/default_pipeline_environment.yml . Afterwards the values can be loaded by the method: ConfigurationLoader.defaultStepConfiguration","title":"Description"},{"location":"steps/prepareDefaultValues/#parameters","text":"None","title":"Parameters"},{"location":"steps/prepareDefaultValues/#step-configuration","text":"None","title":"Step configuration"},{"location":"steps/prepareDefaultValues/#exceptions","text":"None","title":"Exceptions"},{"location":"steps/prepareDefaultValues/#example","text":"prepareDefaultValues ()","title":"Example"},{"location":"steps/seleniumExecuteTests/","text":"seleniumExecuteTests \u00b6 Description \u00b6 Enables UI test execution with Selenium in a sidecar container. The step executes a closure (see example below) connecting to a sidecar container with a Selenium Server. When executing in a local Docker environment, please make sure to set Selenium host to selenium in your tests. Kubernetes environment, plese make sure to set Seleniums host to localhost in your tests. Proxy Environments If work in an environment containing a proxy, please make sure that localhost / selenium is added to your proxy exclusion list, e.g. via environment variable NO_PROXY & no_proxy . You can pass those via parameters dockerEnvVars and sidecarEnvVars directly to the containers if required. Prerequisites \u00b6 none Example \u00b6 seleniumExecuteTests ( script: this ) { git url: 'https://github.wdf.sap.corp/xxxxx/WebDriverIOTest.git' sh '''npm install node index.js''' } Example test using WebdriverIO \u00b6 Example based on http://webdriver.io/guide/getstarted/modes.html and http://webdriver.io/guide.html Configuration for Local Docker Environment \u00b6 var webdriverio = require ( 'webdriverio' ); var options = { host : 'selenium' , port : 4444 , desiredCapabilities : { browserName : 'chrome' } }; Configuration for Kubernetes Environment \u00b6 var webdriverio = require ( 'webdriverio' ); var options = { host : 'localhost' , port : 4444 , desiredCapabilities : { browserName : 'chrome' } }; Test Code (index.js) \u00b6 // ToDo: add configuration from above webdriverio . remote ( options ) . init () . url ( 'http://www.google.com' ) . getTitle (). then ( function ( title ) { console . log ( 'Title was: ' + title ); }) . end () . catch ( function ( err ) { console . log ( err ); }); Parameters \u00b6 parameter mandatory default possible values script yes buildTool no npm maven , npm containerPortMappings no [selenium/standalone-chrome:[[containerPort:4444, hostPort:4444]]] dockerEnvVars no dockerImage no buildTool= maven : maven:3.5-jdk-8 buildTool= npm : node:8-stretch dockerName no buildTool= maven : maven buildTool= npm : npm dockerWorkspace no buildTool= maven : buildTool= npm : /home/node failOnError no true gitBranch no gitSshKeyCredentialsId no `` sidecarEnvVars no sidecarImage no selenium/standalone-chrome sidecarName no selenium sidecarVolumeBind no [/dev/shm:/dev/shm] stashContent no tests testRepository no script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. buildTool defines the build tool to be used for the test execution. containerPortMappings , see step dockerExecute dockerEnvVars , see step dockerExecute dockerImage , see step dockerExecute dockerName , see step dockerExecute dockerWorkspace , see step dockerExecute failOnError specifies if the step should fail in case the execution of the body of this step fails. sidecarEnvVars , see step dockerExecute sidecarImage , see step dockerExecute sidecarName , see step dockerExecute sidecarVolumeBind , see step dockerExecute If specific stashes should be considered for the tests, you can pass this via parameter stashContent In case the test implementation is stored in a different repository than the code itself, you can define the repository containing the tests using parameter testRepository and if required gitBranch (for a different branch than master) and gitSshKeyCredentialsId (for protected repositories). For protected repositories the testRepository needs to contain the ssh git url. Step configuration \u00b6 We recommend to define values of step parameters via config.yml file . In following sections the configuration is possible: parameter general step stage script buildTool X X containerPortMappings X X X dockerEnvVars X X X dockerImage X X X dockerName X X X dockerWorkspace X X X failOnError X X X gitBranch X X X gitSshKeyCredentialsId X X X sidecarEnvVars X X X sidecarImage X X X sidecarName X X X sidecarVolumeBind X X X stashContent X X X testRepository X X X Return value \u00b6 none Side effects \u00b6 none Exceptions \u00b6 none","title":"seleniumExecuteTests"},{"location":"steps/seleniumExecuteTests/#seleniumexecutetests","text":"","title":"seleniumExecuteTests"},{"location":"steps/seleniumExecuteTests/#description","text":"Enables UI test execution with Selenium in a sidecar container. The step executes a closure (see example below) connecting to a sidecar container with a Selenium Server. When executing in a local Docker environment, please make sure to set Selenium host to selenium in your tests. Kubernetes environment, plese make sure to set Seleniums host to localhost in your tests. Proxy Environments If work in an environment containing a proxy, please make sure that localhost / selenium is added to your proxy exclusion list, e.g. via environment variable NO_PROXY & no_proxy . You can pass those via parameters dockerEnvVars and sidecarEnvVars directly to the containers if required.","title":"Description"},{"location":"steps/seleniumExecuteTests/#prerequisites","text":"none","title":"Prerequisites"},{"location":"steps/seleniumExecuteTests/#example","text":"seleniumExecuteTests ( script: this ) { git url: 'https://github.wdf.sap.corp/xxxxx/WebDriverIOTest.git' sh '''npm install node index.js''' }","title":"Example"},{"location":"steps/seleniumExecuteTests/#example-test-using-webdriverio","text":"Example based on http://webdriver.io/guide/getstarted/modes.html and http://webdriver.io/guide.html","title":"Example test using WebdriverIO"},{"location":"steps/seleniumExecuteTests/#configuration-for-local-docker-environment","text":"var webdriverio = require ( 'webdriverio' ); var options = { host : 'selenium' , port : 4444 , desiredCapabilities : { browserName : 'chrome' } };","title":"Configuration for Local Docker Environment"},{"location":"steps/seleniumExecuteTests/#configuration-for-kubernetes-environment","text":"var webdriverio = require ( 'webdriverio' ); var options = { host : 'localhost' , port : 4444 , desiredCapabilities : { browserName : 'chrome' } };","title":"Configuration for Kubernetes Environment"},{"location":"steps/seleniumExecuteTests/#test-code-indexjs","text":"// ToDo: add configuration from above webdriverio . remote ( options ) . init () . url ( 'http://www.google.com' ) . getTitle (). then ( function ( title ) { console . log ( 'Title was: ' + title ); }) . end () . catch ( function ( err ) { console . log ( err ); });","title":"Test Code (index.js)"},{"location":"steps/seleniumExecuteTests/#parameters","text":"parameter mandatory default possible values script yes buildTool no npm maven , npm containerPortMappings no [selenium/standalone-chrome:[[containerPort:4444, hostPort:4444]]] dockerEnvVars no dockerImage no buildTool= maven : maven:3.5-jdk-8 buildTool= npm : node:8-stretch dockerName no buildTool= maven : maven buildTool= npm : npm dockerWorkspace no buildTool= maven : buildTool= npm : /home/node failOnError no true gitBranch no gitSshKeyCredentialsId no `` sidecarEnvVars no sidecarImage no selenium/standalone-chrome sidecarName no selenium sidecarVolumeBind no [/dev/shm:/dev/shm] stashContent no tests testRepository no script defines the global script environment of the Jenkinsfile run. Typically this is passed to this parameter. This allows the function to access the commonPipelineEnvironment for storing the measured duration. buildTool defines the build tool to be used for the test execution. containerPortMappings , see step dockerExecute dockerEnvVars , see step dockerExecute dockerImage , see step dockerExecute dockerName , see step dockerExecute dockerWorkspace , see step dockerExecute failOnError specifies if the step should fail in case the execution of the body of this step fails. sidecarEnvVars , see step dockerExecute sidecarImage , see step dockerExecute sidecarName , see step dockerExecute sidecarVolumeBind , see step dockerExecute If specific stashes should be considered for the tests, you can pass this via parameter stashContent In case the test implementation is stored in a different repository than the code itself, you can define the repository containing the tests using parameter testRepository and if required gitBranch (for a different branch than master) and gitSshKeyCredentialsId (for protected repositories). For protected repositories the testRepository needs to contain the ssh git url.","title":"Parameters"},{"location":"steps/seleniumExecuteTests/#step-configuration","text":"We recommend to define values of step parameters via config.yml file . In following sections the configuration is possible: parameter general step stage script buildTool X X containerPortMappings X X X dockerEnvVars X X X dockerImage X X X dockerName X X X dockerWorkspace X X X failOnError X X X gitBranch X X X gitSshKeyCredentialsId X X X sidecarEnvVars X X X sidecarImage X X X sidecarName X X X sidecarVolumeBind X X X stashContent X X X testRepository X X X","title":"Step configuration"},{"location":"steps/seleniumExecuteTests/#return-value","text":"none","title":"Return value"},{"location":"steps/seleniumExecuteTests/#side-effects","text":"none","title":"Side effects"},{"location":"steps/seleniumExecuteTests/#exceptions","text":"none","title":"Exceptions"},{"location":"steps/setupCommonPipelineEnvironment/","text":"setupCommonPipelineEnvironment \u00b6 Description \u00b6 Initializes the commonPipelineEnvironment , which is used throughout the complete pipeline. Tip This step needs to run at the beginning of a pipeline right after the SCM checkout. Then subsequent pipeline steps consume the information from commonPipelineEnvironment ; it does not need to be passed to pipeline steps explicitly. Prerequisites \u00b6 A configuration file with properties (default location: .pipeline/config.properties ). The property values are used as default values in many pipeline steps. Parameters \u00b6 parameter mandatory default possible values script yes - configFile no .pipeline/config.properties script - The reference to the pipeline script (Jenkinsfile). Normally this needs to be provided. configFile - Property file defining project specific settings. Step configuration \u00b6 none Return value \u00b6 none Side effects \u00b6 none Exceptions \u00b6 none Example \u00b6 setupCommonPipelineEnvironment script: this","title":"setupCommonPipelineEnvironment"},{"location":"steps/setupCommonPipelineEnvironment/#setupcommonpipelineenvironment","text":"","title":"setupCommonPipelineEnvironment"},{"location":"steps/setupCommonPipelineEnvironment/#description","text":"Initializes the commonPipelineEnvironment , which is used throughout the complete pipeline. Tip This step needs to run at the beginning of a pipeline right after the SCM checkout. Then subsequent pipeline steps consume the information from commonPipelineEnvironment ; it does not need to be passed to pipeline steps explicitly.","title":"Description"},{"location":"steps/setupCommonPipelineEnvironment/#prerequisites","text":"A configuration file with properties (default location: .pipeline/config.properties ). The property values are used as default values in many pipeline steps.","title":"Prerequisites"},{"location":"steps/setupCommonPipelineEnvironment/#parameters","text":"parameter mandatory default possible values script yes - configFile no .pipeline/config.properties script - The reference to the pipeline script (Jenkinsfile). Normally this needs to be provided. configFile - Property file defining project specific settings.","title":"Parameters"},{"location":"steps/setupCommonPipelineEnvironment/#step-configuration","text":"none","title":"Step configuration"},{"location":"steps/setupCommonPipelineEnvironment/#return-value","text":"none","title":"Return value"},{"location":"steps/setupCommonPipelineEnvironment/#side-effects","text":"none","title":"Side effects"},{"location":"steps/setupCommonPipelineEnvironment/#exceptions","text":"none","title":"Exceptions"},{"location":"steps/setupCommonPipelineEnvironment/#example","text":"setupCommonPipelineEnvironment script: this","title":"Example"},{"location":"steps/testsPublishResults/","text":"testsPublishResults \u00b6 Description \u00b6 This step can publish test results from various sources. Prerequsites \u00b6 test result files - To use this step, there must be test result files available. installed plugins: junit jacoco cobertura performance Pipeline configuration \u00b6 none Explanation of pipeline step \u00b6 Usage of pipeline step: testsPublishResults ( junit: [ updateResults: true , archive: true ], jacoco: [ archive: true ] ) Available parameters: parameter mandatory default possible values script yes junit no false true, false jacoco no false true, false cobertura no false true, false jmeter no false true, false script - The common script environment of the Jenkinsfile running. Typically the reference to the script calling the pipeline step is provided with the this parameter, as in script: this . This allows the function to access the commonPipelineEnvironment for retrieving, for example, configuration parameters. junit - Publishes test results files in JUnit format with the JUnit Plugin . jacoco - Publishes code coverage with the JaCoCo plugin . cobertura - Publishes code coverage with the Cobertura plugin . jmeter - Publishes performance test results with the Performance plugin . Each of the parameters junit , jacoco , cobertura and jmeter can be set to true or false but also to a map of parameters to hand in different settings for the tools. junit \u00b6 parameter mandatory default possible values pattern no '**/TEST-*.xml' archive no false true, false updateResults no false true, false allowEmptyResults no true true, false jacoco \u00b6 parameter mandatory default possible values pattern no '**/target/*.exec' include no '' '**/*.class' exclude no '' '**/Test*' archive no false true, false allowEmptyResults no true true, false cobertura \u00b6 parameter mandatory default possible values pattern no '**/target/coverage/cobertura-coverage.xml' archive no false true, false allowEmptyResults no true true, false onlyStableBuilds no true true, false jmeter \u00b6 parameter mandatory default possible values pattern no '**/*.jtl' errorFailedThreshold no 20 errorUnstableThreshold no 10 errorUnstableResponseTimeThreshold no `` relativeFailedThresholdPositive no 0 relativeFailedThresholdNegative no 0 relativeUnstableThresholdPositive no 0 relativeUnstableThresholdNegative no 0 modeOfThreshold no false true, false modeThroughput no false true, false nthBuildNumber no 0 configType no PRT failBuildIfNoResultFile no false true, false compareBuildPrevious no true true, false archive no false true, false allowEmptyResults no true true, false Step configuration \u00b6 Following parameters can also be specified as step parameters using the global configuration file: junit jacoco cobertura jmeter Return value \u00b6 none Side effects \u00b6 none Exceptions \u00b6 none Example \u00b6 // publish test results with coverage testsPublishResults ( junit: [ updateResults: true , archive: true ], jacoco: [ archive: true ] ) // publish test results with coverage testsPublishResults ( junit: [ pattern: '**/target/TEST*.xml' , archive: true ], cobertura: [ pattern: '**/target/coverage/cobertura-coverage.xml' ] )","title":"testsPublishResults"},{"location":"steps/testsPublishResults/#testspublishresults","text":"","title":"testsPublishResults"},{"location":"steps/testsPublishResults/#description","text":"This step can publish test results from various sources.","title":"Description"},{"location":"steps/testsPublishResults/#prerequsites","text":"test result files - To use this step, there must be test result files available. installed plugins: junit jacoco cobertura performance","title":"Prerequsites"},{"location":"steps/testsPublishResults/#pipeline-configuration","text":"none","title":"Pipeline configuration"},{"location":"steps/testsPublishResults/#explanation-of-pipeline-step","text":"Usage of pipeline step: testsPublishResults ( junit: [ updateResults: true , archive: true ], jacoco: [ archive: true ] ) Available parameters: parameter mandatory default possible values script yes junit no false true, false jacoco no false true, false cobertura no false true, false jmeter no false true, false script - The common script environment of the Jenkinsfile running. Typically the reference to the script calling the pipeline step is provided with the this parameter, as in script: this . This allows the function to access the commonPipelineEnvironment for retrieving, for example, configuration parameters. junit - Publishes test results files in JUnit format with the JUnit Plugin . jacoco - Publishes code coverage with the JaCoCo plugin . cobertura - Publishes code coverage with the Cobertura plugin . jmeter - Publishes performance test results with the Performance plugin . Each of the parameters junit , jacoco , cobertura and jmeter can be set to true or false but also to a map of parameters to hand in different settings for the tools.","title":"Explanation of pipeline step"},{"location":"steps/testsPublishResults/#junit","text":"parameter mandatory default possible values pattern no '**/TEST-*.xml' archive no false true, false updateResults no false true, false allowEmptyResults no true true, false","title":"junit"},{"location":"steps/testsPublishResults/#jacoco","text":"parameter mandatory default possible values pattern no '**/target/*.exec' include no '' '**/*.class' exclude no '' '**/Test*' archive no false true, false allowEmptyResults no true true, false","title":"jacoco"},{"location":"steps/testsPublishResults/#cobertura","text":"parameter mandatory default possible values pattern no '**/target/coverage/cobertura-coverage.xml' archive no false true, false allowEmptyResults no true true, false onlyStableBuilds no true true, false","title":"cobertura"},{"location":"steps/testsPublishResults/#jmeter","text":"parameter mandatory default possible values pattern no '**/*.jtl' errorFailedThreshold no 20 errorUnstableThreshold no 10 errorUnstableResponseTimeThreshold no `` relativeFailedThresholdPositive no 0 relativeFailedThresholdNegative no 0 relativeUnstableThresholdPositive no 0 relativeUnstableThresholdNegative no 0 modeOfThreshold no false true, false modeThroughput no false true, false nthBuildNumber no 0 configType no PRT failBuildIfNoResultFile no false true, false compareBuildPrevious no true true, false archive no false true, false allowEmptyResults no true true, false","title":"jmeter"},{"location":"steps/testsPublishResults/#step-configuration","text":"Following parameters can also be specified as step parameters using the global configuration file: junit jacoco cobertura jmeter","title":"Step configuration"},{"location":"steps/testsPublishResults/#return-value","text":"none","title":"Return value"},{"location":"steps/testsPublishResults/#side-effects","text":"none","title":"Side effects"},{"location":"steps/testsPublishResults/#exceptions","text":"none","title":"Exceptions"},{"location":"steps/testsPublishResults/#example","text":"// publish test results with coverage testsPublishResults ( junit: [ updateResults: true , archive: true ], jacoco: [ archive: true ] ) // publish test results with coverage testsPublishResults ( junit: [ pattern: '**/target/TEST*.xml' , archive: true ], cobertura: [ pattern: '**/target/coverage/cobertura-coverage.xml' ] )","title":"Example"},{"location":"steps/toolValidate/","text":"toolValidate \u00b6 Description \u00b6 Checks the existence and compatibility of a tool, necessary for a successful pipeline execution. In case a violation is found, an exception is raised. Prerequisites \u00b6 none Parameters \u00b6 parameter mandatory default possible values tool yes 'java', 'mta', 'neo' home yes tool The tool that is checked for existence and compatible version. home The location in the file system where Jenkins can access the tool. Step configuration \u00b6 none Return value \u00b6 none Side effects \u00b6 none Exceptions \u00b6 IllegalArgumentException : If at least one of the parameters tool , home is not provided. AbortException : If tool is not supported. Example \u00b6 toolValidate tool: 'neo' , home: '/path/to/neo-java-web-sdk'","title":"toolValidate"},{"location":"steps/toolValidate/#toolvalidate","text":"","title":"toolValidate"},{"location":"steps/toolValidate/#description","text":"Checks the existence and compatibility of a tool, necessary for a successful pipeline execution. In case a violation is found, an exception is raised.","title":"Description"},{"location":"steps/toolValidate/#prerequisites","text":"none","title":"Prerequisites"},{"location":"steps/toolValidate/#parameters","text":"parameter mandatory default possible values tool yes 'java', 'mta', 'neo' home yes tool The tool that is checked for existence and compatible version. home The location in the file system where Jenkins can access the tool.","title":"Parameters"},{"location":"steps/toolValidate/#step-configuration","text":"none","title":"Step configuration"},{"location":"steps/toolValidate/#return-value","text":"none","title":"Return value"},{"location":"steps/toolValidate/#side-effects","text":"none","title":"Side effects"},{"location":"steps/toolValidate/#exceptions","text":"IllegalArgumentException : If at least one of the parameters tool , home is not provided. AbortException : If tool is not supported.","title":"Exceptions"},{"location":"steps/toolValidate/#example","text":"toolValidate tool: 'neo' , home: '/path/to/neo-java-web-sdk'","title":"Example"},{"location":"steps/transportRequestCreate/","text":"transportRequestCreate \u00b6 Description \u00b6 Creates a Transport Request for a Change Document on the Solution Manager (type SOLMAN ) or a Transport Request inside an ABAP system (type CTS ) The id of the transport request is availabe via commonPipelineEnvironment.getTransportRequestId() Prerequisites \u00b6 Change Management Client 2.0.0 or compatible version - available for download on Maven Central. Parameters \u00b6 parameter mandatory default possible values script yes changeDocumentId for SOLMAN transportType for CTS no targetSystem for CTS no description for CTS no changeManagement/credentialsId yes changeManagement/endpoint yes changeManagement/clientOpts no changeManagement/git/from no origin/master changeManagement/git/to no HEAD changeManagement/changeDocumentLabel no ChangeDocument\\s?: regex pattern changeManagement/git/format no %b see git log --help changeManagement/type no SOLMAN SOLMAN , CTS script - The common script environment of the Jenkinsfile running. Typically the reference to the script calling the pipeline step is provided with the this parameter, as in script: this . This allows the function to access the commonPipelineEnvironment for retrieving, for example, configuration parameters. changeDocumentId - for SOLMAN only. The id of the change document to that the transport request is bound to. Typically this value is provided via commit message in the commit history. changeManagement/type Where/how the transport request is created (via SAP Solution Manager, ABAP). changeManagement/credentialsId - The credentials to connect to the service endpoint (Solution Manager, ABAP System). changeManagement/endpoint - The service endpoint (Solution Manager, ABAP System). changeManagement/clientOpts - Options forwarded to JVM used by the CM client, like JAVA_OPTS changeManagement/git/from - The starting point for retrieving the change document id changeManagement/git/to - The end point for retrieving the change document id changeManagement/changeDocumentLabel - For type SOLMAN only. A pattern used for identifying lines holding the change document id. changeManagement/git/format - Specifies what part of the commit is scanned. By default the body of the commit message is scanned. description - for CTS only. The description of the transport request. targetSystem - for CTS only. The system receiving the transport request. transportType - for type CTS only. Typically W (workbench) or C customizing. Step configuration \u00b6 The step is configured using a customer configuration file provided as resource in an custom shared library. @Library ( 'piper-library-os@master' ) _ // the shared lib containing the additional configuration // needs to be configured in Jenkins @Library ( 'foo@master' ) __ // inside the shared lib denoted by 'foo' the additional configuration file // needs to be located under 'resources' ('resoures/myConfig.yml') prepareDefaultValues script: this , customDefaults: 'myConfig.yml' Example content of 'resources/myConfig.yml' in branch 'master' of the repository denoted by 'foo' : general : changeManagement : changeDocumentLabel : 'ChangeDocument\\s?:' cmClientOpts : '-Djavax.net.ssl.trustStore=<path to truststore>' credentialsId : 'CM' type : 'SOLMAN' endpoint : 'https://example.org/cm' git : from : 'HEAD~1' to : 'HEAD' format : '%b' The properties configured in section 'general/changeManagement' are shared between all change managment related steps. The properties can also be configured on a per-step basis: [ ... ] steps : transportRequestCreate : changeManagement : type : 'SOLMAN' endpoint : 'https://example.org/cm' [ ... ] The parameters can also be provided when the step is invoked. For examples see below. Return value \u00b6 none Exceptions \u00b6 AbortException : If the creation of the transport request fails. IllegalStateException : If the change id is not provided. Example \u00b6 // SOLMAN def transportRequestId = transportRequestCreate script: this , changeDocumentId: '001,' changeManagement: [ type: 'SOLMAN' endpoint: 'https://example.org/cm' ] // CTS def transportRequestId = transportRequestCreate script: this , transportType: 'W' , targetSystem: 'XYZ' , description: 'the description' , changeManagement: [ type: 'CTS' endpoint: 'https://example.org/cm' ]","title":"transportRequestCreate"},{"location":"steps/transportRequestCreate/#transportrequestcreate","text":"","title":"transportRequestCreate"},{"location":"steps/transportRequestCreate/#description","text":"Creates a Transport Request for a Change Document on the Solution Manager (type SOLMAN ) or a Transport Request inside an ABAP system (type CTS ) The id of the transport request is availabe via commonPipelineEnvironment.getTransportRequestId()","title":"Description"},{"location":"steps/transportRequestCreate/#prerequisites","text":"Change Management Client 2.0.0 or compatible version - available for download on Maven Central.","title":"Prerequisites"},{"location":"steps/transportRequestCreate/#parameters","text":"parameter mandatory default possible values script yes changeDocumentId for SOLMAN transportType for CTS no targetSystem for CTS no description for CTS no changeManagement/credentialsId yes changeManagement/endpoint yes changeManagement/clientOpts no changeManagement/git/from no origin/master changeManagement/git/to no HEAD changeManagement/changeDocumentLabel no ChangeDocument\\s?: regex pattern changeManagement/git/format no %b see git log --help changeManagement/type no SOLMAN SOLMAN , CTS script - The common script environment of the Jenkinsfile running. Typically the reference to the script calling the pipeline step is provided with the this parameter, as in script: this . This allows the function to access the commonPipelineEnvironment for retrieving, for example, configuration parameters. changeDocumentId - for SOLMAN only. The id of the change document to that the transport request is bound to. Typically this value is provided via commit message in the commit history. changeManagement/type Where/how the transport request is created (via SAP Solution Manager, ABAP). changeManagement/credentialsId - The credentials to connect to the service endpoint (Solution Manager, ABAP System). changeManagement/endpoint - The service endpoint (Solution Manager, ABAP System). changeManagement/clientOpts - Options forwarded to JVM used by the CM client, like JAVA_OPTS changeManagement/git/from - The starting point for retrieving the change document id changeManagement/git/to - The end point for retrieving the change document id changeManagement/changeDocumentLabel - For type SOLMAN only. A pattern used for identifying lines holding the change document id. changeManagement/git/format - Specifies what part of the commit is scanned. By default the body of the commit message is scanned. description - for CTS only. The description of the transport request. targetSystem - for CTS only. The system receiving the transport request. transportType - for type CTS only. Typically W (workbench) or C customizing.","title":"Parameters"},{"location":"steps/transportRequestCreate/#step-configuration","text":"The step is configured using a customer configuration file provided as resource in an custom shared library. @Library ( 'piper-library-os@master' ) _ // the shared lib containing the additional configuration // needs to be configured in Jenkins @Library ( 'foo@master' ) __ // inside the shared lib denoted by 'foo' the additional configuration file // needs to be located under 'resources' ('resoures/myConfig.yml') prepareDefaultValues script: this , customDefaults: 'myConfig.yml' Example content of 'resources/myConfig.yml' in branch 'master' of the repository denoted by 'foo' : general : changeManagement : changeDocumentLabel : 'ChangeDocument\\s?:' cmClientOpts : '-Djavax.net.ssl.trustStore=<path to truststore>' credentialsId : 'CM' type : 'SOLMAN' endpoint : 'https://example.org/cm' git : from : 'HEAD~1' to : 'HEAD' format : '%b' The properties configured in section 'general/changeManagement' are shared between all change managment related steps. The properties can also be configured on a per-step basis: [ ... ] steps : transportRequestCreate : changeManagement : type : 'SOLMAN' endpoint : 'https://example.org/cm' [ ... ] The parameters can also be provided when the step is invoked. For examples see below.","title":"Step configuration"},{"location":"steps/transportRequestCreate/#return-value","text":"none","title":"Return value"},{"location":"steps/transportRequestCreate/#exceptions","text":"AbortException : If the creation of the transport request fails. IllegalStateException : If the change id is not provided.","title":"Exceptions"},{"location":"steps/transportRequestCreate/#example","text":"// SOLMAN def transportRequestId = transportRequestCreate script: this , changeDocumentId: '001,' changeManagement: [ type: 'SOLMAN' endpoint: 'https://example.org/cm' ] // CTS def transportRequestId = transportRequestCreate script: this , transportType: 'W' , targetSystem: 'XYZ' , description: 'the description' , changeManagement: [ type: 'CTS' endpoint: 'https://example.org/cm' ]","title":"Example"},{"location":"steps/transportRequestRelease/","text":"transportRequestRelease \u00b6 Description \u00b6 Releases a Transport Request. Prerequisites \u00b6 Change Management Client 2.0.0 or compatible version - available for download on Maven Central. Parameters \u00b6 parameter mandatory default possible values script yes changeDocumentId SOLMAN only transportRequestId yes changeManagement/changeDocumentLabel no ChangeDocument\\s?: regex pattern changeManagment/transportRequestLabel no TransportRequest\\s?: regex pattern changeManagement/credentialsId yes changeManagement/endpoint yes changeManagement/git/from no origin/master changeManagement/git/to no HEAD changeManagement/git/format no %b see git log --help changeManagement/type no SOLMAN SOLMAN , CTS script - The common script environment of the Jenkinsfile running. Typically the reference to the script calling the pipeline step is provided with the this parameter, as in script: this . This allows the function to access the commonPipelineEnvironment for retrieving, for example, configuration parameters. changeDocumentId - for SOLMAN only. The id of the change document related to the transport request to release. transportRequestId - The id of the transport request to release. changeManagement/changeDocumentLabel - for SOLMAN only. A pattern used for identifying lines holding the change document id. changeManagment/transportRequestLabel - A pattern used for identifying lines holding the transport request id. changeManagement/credentialsId - The credentials to connect to the service endpoint (Solution Manager, ABAP System). changeManagement/endpoint - The service endpoint (Solution Manager, ABAP System). changeManagement/git/from - The starting point for retrieving the change document id and/or transport request id changeManagement/git/to - The end point for retrieving the change document id and/or transport request id changeManagement/git/format - Specifies what part of the commit is scanned. By default the body of the commit message is scanned. Step configuration \u00b6 The step is configured using a customer configuration file provided as resource in an custom shared library. @Library ( 'piper-library-os@master' ) _ // the shared lib containing the additional configuration // needs to be configured in Jenkins @Library ( 'foo@master' ) __ // inside the shared lib denoted by 'foo' the additional configuration file // needs to be located under 'resources' ('resoures/myConfig.yml') prepareDefaultValues script: this , customDefaults: 'myConfig.yml' Example content of 'resources/myConfig.yml' in branch 'master' of the repository denoted by 'foo' : general : changeManagement : changeDocumentLabel : 'ChangeDocument\\s?:' cmClientOpts : '-Djavax.net.ssl.trustStore=<path to truststore>' credentialsId : 'CM' type : 'SOLMAN' endpoint : 'https://example.org/cm' git : from : 'HEAD~1' to : 'HEAD' format : '%b' The properties configured in section 'general/changeManagement' are shared between all change managment related steps. The properties can also be configured on a per-step basis: [ ... ] steps : transportRequestRelease : changeManagement : type : 'SOLMAN' endpoint : 'https://example.org/cm' [ ... ] The parameters can also be provided when the step is invoked. For examples see below. Return value \u00b6 None. Exceptions \u00b6 IllegalArgumentException : If the change id is not provided ( SOLMAN only) If the transport request id is not provided. AbortException : If the release of the transport request fails. Example \u00b6 // SOLMAN transportRequestRelease script: this , changeDocumentId: '001' , transportRequestId: '001' , changeManagement: [ type: 'SOLMAN' endpoint: 'https://example.org/cm' ] // CTS transportRequestRelease script: this , transportRequestId: '001' , changeManagement: [ type: 'CTS' endpoint: 'https://example.org/cm' ]","title":"transportRequestRelease"},{"location":"steps/transportRequestRelease/#transportrequestrelease","text":"","title":"transportRequestRelease"},{"location":"steps/transportRequestRelease/#description","text":"Releases a Transport Request.","title":"Description"},{"location":"steps/transportRequestRelease/#prerequisites","text":"Change Management Client 2.0.0 or compatible version - available for download on Maven Central.","title":"Prerequisites"},{"location":"steps/transportRequestRelease/#parameters","text":"parameter mandatory default possible values script yes changeDocumentId SOLMAN only transportRequestId yes changeManagement/changeDocumentLabel no ChangeDocument\\s?: regex pattern changeManagment/transportRequestLabel no TransportRequest\\s?: regex pattern changeManagement/credentialsId yes changeManagement/endpoint yes changeManagement/git/from no origin/master changeManagement/git/to no HEAD changeManagement/git/format no %b see git log --help changeManagement/type no SOLMAN SOLMAN , CTS script - The common script environment of the Jenkinsfile running. Typically the reference to the script calling the pipeline step is provided with the this parameter, as in script: this . This allows the function to access the commonPipelineEnvironment for retrieving, for example, configuration parameters. changeDocumentId - for SOLMAN only. The id of the change document related to the transport request to release. transportRequestId - The id of the transport request to release. changeManagement/changeDocumentLabel - for SOLMAN only. A pattern used for identifying lines holding the change document id. changeManagment/transportRequestLabel - A pattern used for identifying lines holding the transport request id. changeManagement/credentialsId - The credentials to connect to the service endpoint (Solution Manager, ABAP System). changeManagement/endpoint - The service endpoint (Solution Manager, ABAP System). changeManagement/git/from - The starting point for retrieving the change document id and/or transport request id changeManagement/git/to - The end point for retrieving the change document id and/or transport request id changeManagement/git/format - Specifies what part of the commit is scanned. By default the body of the commit message is scanned.","title":"Parameters"},{"location":"steps/transportRequestRelease/#step-configuration","text":"The step is configured using a customer configuration file provided as resource in an custom shared library. @Library ( 'piper-library-os@master' ) _ // the shared lib containing the additional configuration // needs to be configured in Jenkins @Library ( 'foo@master' ) __ // inside the shared lib denoted by 'foo' the additional configuration file // needs to be located under 'resources' ('resoures/myConfig.yml') prepareDefaultValues script: this , customDefaults: 'myConfig.yml' Example content of 'resources/myConfig.yml' in branch 'master' of the repository denoted by 'foo' : general : changeManagement : changeDocumentLabel : 'ChangeDocument\\s?:' cmClientOpts : '-Djavax.net.ssl.trustStore=<path to truststore>' credentialsId : 'CM' type : 'SOLMAN' endpoint : 'https://example.org/cm' git : from : 'HEAD~1' to : 'HEAD' format : '%b' The properties configured in section 'general/changeManagement' are shared between all change managment related steps. The properties can also be configured on a per-step basis: [ ... ] steps : transportRequestRelease : changeManagement : type : 'SOLMAN' endpoint : 'https://example.org/cm' [ ... ] The parameters can also be provided when the step is invoked. For examples see below.","title":"Step configuration"},{"location":"steps/transportRequestRelease/#return-value","text":"None.","title":"Return value"},{"location":"steps/transportRequestRelease/#exceptions","text":"IllegalArgumentException : If the change id is not provided ( SOLMAN only) If the transport request id is not provided. AbortException : If the release of the transport request fails.","title":"Exceptions"},{"location":"steps/transportRequestRelease/#example","text":"// SOLMAN transportRequestRelease script: this , changeDocumentId: '001' , transportRequestId: '001' , changeManagement: [ type: 'SOLMAN' endpoint: 'https://example.org/cm' ] // CTS transportRequestRelease script: this , transportRequestId: '001' , changeManagement: [ type: 'CTS' endpoint: 'https://example.org/cm' ]","title":"Example"},{"location":"steps/transportRequestUploadFile/","text":"transportRequestUploadFile \u00b6 Description \u00b6 Uploads a file to a Transport Request. Prerequisites \u00b6 Change Management Client 2.0.0 or compatible version - available for download on Maven Central. Parameters \u00b6 parameter mandatory default possible values script yes changeDocumentId SOLMAN only transportRequestId yes applicationId SOLMAN only filePath yes changeManagement/credentialsId yes changeManagement/endpoint yes changeManagement/git/from no origin/master changeManagement/git/to no HEAD changeManagement/changeDocumentLabel no ChangeDocument\\s?: regex pattern changeManagement/transportRequestLabel no TransportRequest\\s?: regex pattern changeManagement/git/format no %b see git log --help changeManagement/type no SOLMAN SOLMAN , CTS script - The common script environment of the Jenkinsfile running. Typically the reference to the script calling the pipeline step is provided with the this parameter, as in script: this . This allows the function to access the commonPipelineEnvironment for retrieving, for example, configuration parameters. changeDocumentId - For type SOLMAN only. The id of the change document related to the transport request to release. Typically provided via commit history. transportRequestId - The id of the transport request to release. Typically provided via commit history. applicationId - For type SOLMAN only. The id of the application. filePath - The path of the file to upload. changeManagement/credentialsId - The credentials to connect to the service endpoint (Solution Manager, ABAP System). changeManagement/endpoint - The service endpoint (Solution Manager, ABAP System). changeManagement/git/from - The starting point for retrieving the change document id and/or transport request id changeManagement/git/to - The end point for retrieving the change document id and/or transport request id changeManagement/changeDocumentLabel - For type SOLMAN only. A pattern used for identifying lines holding the change document id. changeManagement/transportRequestLabel - A pattern used for identifying lines holding the transport request id. changeManagement/type Where/how the transport request is created (via SAP Solution Manager, ABAP). changeManagement/git/format - Specifies what part of the commit is scanned. By default the body of the commit message is scanned. Step configuration \u00b6 The step is configured using a customer configuration file provided as resource in an custom shared library. @Library ( 'piper-library-os@master' ) _ // the shared lib containing the additional configuration // needs to be configured in Jenkins @Library ( 'foo@master' ) __ // inside the shared lib denoted by 'foo' the additional configuration file // needs to be located under 'resources' ('resoures/myConfig.yml') prepareDefaultValues script: this , customDefaults: 'myConfig.yml' Example content of 'resources/myConfig.yml' in branch 'master' of the repository denoted by 'foo' : general : changeManagement : changeDocumentLabel : 'ChangeDocument\\s?:' cmClientOpts : '-Djavax.net.ssl.trustStore=<path to truststore>' credentialsId : 'CM' type : 'SOLMAN' endpoint : 'https://example.org/cm' git : from : 'HEAD~1' to : 'HEAD' format : '%b' The properties configured in section 'general/changeManagement' are shared between all change managment related steps. The properties can also be configured on a per-step basis: [ ... ] steps : transportRequestUploadFile : applicationId : 'FOO' changeManagement : type : 'SOLMAN' endpoint : 'https://example.org/cm' [ ... ] The parameters can also be provided when the step is invoked. For examples see below. Return value \u00b6 None. Exceptions \u00b6 IllegalArgumentException : If the change id is not provided ( SOLMAN only). If the transport request id is not provided. If the application id is not provided ( SOLMAN only). If the file path is not provided. AbortException : If the upload fails. Example \u00b6 // SOLMAN transportRequestUploadFile script: this , changeDocumentId: '001' , // typically provided via git commit history transportRequestId: '001' , // typically provided via git commit history applicationId: '001' , filePath: '/path' , changeManagement: [ type: 'SOLMAN' endpoint: 'https://example.org/cm' ] // CTS transportRequestUploadFile script: this , transportRequestId: '001' , // typically provided via git commit history filePath: '/path' , changeManagement: [ type: 'CTS' endpoint: 'https://example.org/cm' ]","title":"transportRequestUploadFile"},{"location":"steps/transportRequestUploadFile/#transportrequestuploadfile","text":"","title":"transportRequestUploadFile"},{"location":"steps/transportRequestUploadFile/#description","text":"Uploads a file to a Transport Request.","title":"Description"},{"location":"steps/transportRequestUploadFile/#prerequisites","text":"Change Management Client 2.0.0 or compatible version - available for download on Maven Central.","title":"Prerequisites"},{"location":"steps/transportRequestUploadFile/#parameters","text":"parameter mandatory default possible values script yes changeDocumentId SOLMAN only transportRequestId yes applicationId SOLMAN only filePath yes changeManagement/credentialsId yes changeManagement/endpoint yes changeManagement/git/from no origin/master changeManagement/git/to no HEAD changeManagement/changeDocumentLabel no ChangeDocument\\s?: regex pattern changeManagement/transportRequestLabel no TransportRequest\\s?: regex pattern changeManagement/git/format no %b see git log --help changeManagement/type no SOLMAN SOLMAN , CTS script - The common script environment of the Jenkinsfile running. Typically the reference to the script calling the pipeline step is provided with the this parameter, as in script: this . This allows the function to access the commonPipelineEnvironment for retrieving, for example, configuration parameters. changeDocumentId - For type SOLMAN only. The id of the change document related to the transport request to release. Typically provided via commit history. transportRequestId - The id of the transport request to release. Typically provided via commit history. applicationId - For type SOLMAN only. The id of the application. filePath - The path of the file to upload. changeManagement/credentialsId - The credentials to connect to the service endpoint (Solution Manager, ABAP System). changeManagement/endpoint - The service endpoint (Solution Manager, ABAP System). changeManagement/git/from - The starting point for retrieving the change document id and/or transport request id changeManagement/git/to - The end point for retrieving the change document id and/or transport request id changeManagement/changeDocumentLabel - For type SOLMAN only. A pattern used for identifying lines holding the change document id. changeManagement/transportRequestLabel - A pattern used for identifying lines holding the transport request id. changeManagement/type Where/how the transport request is created (via SAP Solution Manager, ABAP). changeManagement/git/format - Specifies what part of the commit is scanned. By default the body of the commit message is scanned.","title":"Parameters"},{"location":"steps/transportRequestUploadFile/#step-configuration","text":"The step is configured using a customer configuration file provided as resource in an custom shared library. @Library ( 'piper-library-os@master' ) _ // the shared lib containing the additional configuration // needs to be configured in Jenkins @Library ( 'foo@master' ) __ // inside the shared lib denoted by 'foo' the additional configuration file // needs to be located under 'resources' ('resoures/myConfig.yml') prepareDefaultValues script: this , customDefaults: 'myConfig.yml' Example content of 'resources/myConfig.yml' in branch 'master' of the repository denoted by 'foo' : general : changeManagement : changeDocumentLabel : 'ChangeDocument\\s?:' cmClientOpts : '-Djavax.net.ssl.trustStore=<path to truststore>' credentialsId : 'CM' type : 'SOLMAN' endpoint : 'https://example.org/cm' git : from : 'HEAD~1' to : 'HEAD' format : '%b' The properties configured in section 'general/changeManagement' are shared between all change managment related steps. The properties can also be configured on a per-step basis: [ ... ] steps : transportRequestUploadFile : applicationId : 'FOO' changeManagement : type : 'SOLMAN' endpoint : 'https://example.org/cm' [ ... ] The parameters can also be provided when the step is invoked. For examples see below.","title":"Step configuration"},{"location":"steps/transportRequestUploadFile/#return-value","text":"None.","title":"Return value"},{"location":"steps/transportRequestUploadFile/#exceptions","text":"IllegalArgumentException : If the change id is not provided ( SOLMAN only). If the transport request id is not provided. If the application id is not provided ( SOLMAN only). If the file path is not provided. AbortException : If the upload fails.","title":"Exceptions"},{"location":"steps/transportRequestUploadFile/#example","text":"// SOLMAN transportRequestUploadFile script: this , changeDocumentId: '001' , // typically provided via git commit history transportRequestId: '001' , // typically provided via git commit history applicationId: '001' , filePath: '/path' , changeManagement: [ type: 'SOLMAN' endpoint: 'https://example.org/cm' ] // CTS transportRequestUploadFile script: this , transportRequestId: '001' , // typically provided via git commit history filePath: '/path' , changeManagement: [ type: 'CTS' endpoint: 'https://example.org/cm' ]","title":"Example"}]}